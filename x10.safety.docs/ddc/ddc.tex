\documentclass[10pt, conference, compsocconf]{IEEEtran}

\usepackage[cmex10]{amsmath}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{color}
\usepackage%[pdftex]
{graphicx}
%\usepackage{epstopdf}
\usepackage{pstricks, pst-node, pst-blur}

\DeclareUnicodeCharacter{FB00}{ff}
\DeclareUnicodeCharacter{FB01}{fi}
\DeclareUnicodeCharacter{FB02}{fl}
\DeclareUnicodeCharacter{FB03}{ffi}
\DeclareUnicodeCharacter{FB04}{ffl}

\hypersetup{%
  colorlinks=true,    % activates colored references
  linkcolor=red,       % color of internal links
  citecolor=blue,    % color of links to bibliography
  filecolor=magenta, % color of file links
  urlcolor=blue,         % color of external links
  pdftitle={D2C: A Deterministic, Deadlock-free Concurrent Programming Model},
%  pdfauthor={Christian Hammer},
%  pdfsubject={},
%  pdfkeywords={}
}

\begin{document}

\DeclareGraphicsRule{.pdf}{eps}{.bb}{`convert #1 eps:-}%      PDF-graphics
\newcommand{\TODO}[1]{{\bf #1}}

%\conferenceinfo{HIPS '11}{Anchorage, Alaska} 
%\copyrightyear{2011} 
%\copyrightdata{[to be supplied]} 

%\titlebanner{HIPS 2011}        % These are ignored unless
%\preprintfooter{$D^2C$: A Deterministic, Deadlock-Free Concurrent Programming Model}   % 'preprint' option specified.

\title{$D^2C$: A Deterministic, Deadlock-free Concurrent Programming Model}
%\subtitle{Subtitle Text, if any}

\author{

\IEEEauthorblockN{Nalini Vasudevan \ \ \ \ Stephen A. Edwards}
\IEEEauthorblockA{
Columbia University\\
New York, NY\\\
naliniv,sedwards@cs.columbia.edu}
\and
\IEEEauthorblockN{Christian Hammer}
\IEEEauthorblockA{
Purdue University\\
West Lafayette, IN\\
cjhammer@purdue.edu}
\and
\IEEEauthorblockN{Julian Dolby \ \ \ \ Vijay Saraswat}
\IEEEauthorblockA{
IBM Research \\
Hawthorne, NY\\
dolby,vsaraswa@us.ibm.com}
%\and

}

\maketitle

\lstdefinestyle{small}{
  basicstyle={\fontsize{8}{9}\selectfont\rmfamily},
  identifierstyle={\rmfamily\itshape},
  keywordstyle={\rmfamily\itshape\bfseries},
}

\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}
\newcommand{\lemref}[1]{Lemma~\ref{lem:#1}}
\newcommand{\algref}[1]{Algorithm~\ref{alg:#1}}
\newcommand{\lineref}[1]{Line~\ref{line:#1}}

\lstset{
  language=C,
  basicstyle={\fontsize{8.5}{9.5}\selectfont\rmfamily},
  identifierstyle={\rmfamily\itshape},
  keywordstyle={\rmfamily\itshape\bfseries},
  commentstyle={\rmfamily\color{blue}},
  columns=fullflexible,
  escapechar=\#,
  numbers=left,
  numberstyle=\scriptsize,
  numbersep=4pt,
  stepnumber=1,
  escapeinside={/*@}{@*/}
%  alsoletter=0123456789,
}




\begin{abstract}
The advent of multicore processors has made concurrent programming models mandatory. However, most concurrent programming models come with a repertoire of problems.
The two major ones are non-determinism and deadlocks. By determinism, we mean the output behavior of the program is 
independent of the interleaving caused by the schedule and depends only on the 
input behavior. A few concurrent models provide deterministic behavior by providing constructs like barriers and locks that impose additional synchronization, 
but the incorrect usage of these constructs 
leads to problems like deadlocks. 

In this paper, we propose $D^2C$, a new programming model that guarantees
the two desirable properties of concurrency - determinism and deadlock-freedom.
Any program in this model will be deterministic; the output of the program
will solely depend on the input and not on the interleaving of the tasks in the program.
Additionally, the model cannot  introduce deadlocks.
We prove the correctness of our model and evaluate it with a set of examples.

\end{abstract}

\begin{IEEEkeywords}
Determinism, Deadlock-Freedom

\end{IEEEkeywords}

\section{Introduction}

Non-deterministic  behavior  is one of the biggest problems of concurrent programming. The program in~\figref{non-det} is non-deterministic. It uses 
Cilk\cite{blumofe1995cilk} 
like syntax. It creates two tasks $f$ and $g$ in parallel using the \emph{spawn} construct and both take $x$ by reference. Clearly, $x$ is getting 
modiﬁed concurrently by both the tasks, so the value printed by this program is either $3$ or $5$ 
depending on the schedule. 

\begin{figure}[htbp]
\begin{lstlisting}[language=C]
void f(int ref a) { 
    a = 3; 
} 
 
void g(int ref b) { 
    b = 5; 
} 
 
main() { 
    shared int x = 1; 
    spawn f(x) 
    g(x); 
    sync; /* Wait for f and g to finish*/  
    print x; 
} 
\end{lstlisting}
\caption{A non-deterministic concurrent program}
\label{fig:non-det}
\end{figure}

Such non-determinism makes debugging very hard because unwanted behavior is 
rarely reproducible. Re-running a non-deterministic program on the same input usually does not 
produce the same behavior. Debugging then becomes a nightmare. 


By contrast, all sequential programming languages (e.g., C) are deterministic: they produce 
the same output given the same input. This helps programmers by 
making it easy to verify a program because if a program 
produces the desired result for an input during testing, it will do so reliably. 

Concurrent models based on atomic transactions and locks are race free but are not deterministic. 
For instance, protecting x by a lock in~\figref{non-det} will still produce non-deterministic output. Concurrent software languages are generally based on these models and use traditional shared memory, 
locks, and condition variables (e.g., pthreads or Java). They are non-deterministic because 
the output of a program may depend on such things as the operating system’s scheduling policy, 
the relative execution rates of parallel processors, and other things outside the application programmer’s control. Not only does this demand a programmer consider the effects of these things 
when designing the program, it also means testing can only say a program may behave correctly 
on certain inputs, not that it will. 

We agree with Bocchino et al.~\cite{bocchino2009parallel}  that the programming environment should ensure input-output 
determinism. By determinism, we mean that the program’s output should only depend on the input, 
and not on the environment (operating system schedule, processor, cache etc.). There are a number
of models and tools that aid determinism. We discuss them in ~\secref{related}.

While determinism and deterministic concurrent models are interesting, they give rise to a number of problems. 
For example, if the tasks do not synchronize in the right order deﬁned by the synchronization 
protocol, we obtain a deadlock.
A deadlock is a situation when two or more tasks are indeﬁnitely 
 A simple classic example of a deadlock is \emph{lock(p); lock(q);}
by one task and \emph{lock(q); lock(p);} by another. 
waiting for each other to ﬁnish. 
Deadlocks are frustrating and generally hard to manually detect 
during run-time. 

One way to address these problems is to build a deterministic, deadlock-free concurrent programming
model and $D^2C$ is an instance. 
Any application written in this model will guarantee to give 
the same output for a given input
and will never deadlock. 
It is important to note that we do not gaurantee termination.
We do not try to solve the halting problem here - our model is applicable
only for terminating programs.


We start by discussing our model in~\secref{model} and a few examples in~\secref{examples}. 
We then provide a  proof of correcness (determinism and deadlock-freedom) in~\secref{proof}. 
We provide a basic implementation of the runtime in~\secref{implementation}. 
We then evaluate our model by experimenting on a set of examples in~\secref{experimental}. 
We compare our work with related work in~\secref{related} and finally conclude with future work in~\secref{conclusions}.



\section{$D^2C$: Our model}
\label{sec:model}

Non-determinism arises when multiple tasks concurrently modify a shared variable.
Our programming model is a modification to Cilk - we allow multiple tasks to write to
a shared variable concurrently but in a synchronized fashion and we define a commutative, associative
reduction operator that will operate on these
writes.
\begin{figure}
\begin{lstlisting}[language=C]
void f(clocked int ref a) {
    /* x is 1 */
    a <- 3;
    /* x is still 1 */
    next; /* The reduction operator is applied */
    /* x is 8 */
}

void g(clocked int ref b) {
    int local;
    /* x is 1 */
    b <- 5;
    /* x is still 1 */
    local = b; /* local is 1 */
    next; /* The reduction operator is applied */
    /* x is 8 */
    local = b; /* local is 8 */
}

void h (clocked int ref c) {
     /* x is still 1 */
     next;
    /* x is 8 */
}

main() {
     clocked(+) int x = 1; /*@\label{line:declx1}@*/
     /* If there are multiple writers, reduce
      using the + reduction operator */
     spawn clocked(x) f(x) ;
     spawn clocked(x) g(x);
     h(x);
     next;
     /* x is 8 */
}
\end{lstlisting}
\caption{Example of a program written in our model}
\label{fig:ddc}
\end{figure}


The program in~\figref{ddc} creates three tasks
in parallel $f$, $g$ and $h$. $f$ and $g$ are
modifying $x$.
For simplicity, we have used Cilk\cite{blumofe1995cilk}-like syntax.
Even though $f$ and $g$ are modifying
$x$ concurrently, $f$ sees the effect of $g$ only
when it executes $next$. Similarly $g$ sees the effect
of $f$ only when it executes $next$.
When a task executes $next$, it waits for all tasks that
share variables with it, to also execute $next$. The $next$
statement is like a barrier. At this statement, the clocked variables are reduced
using the reduction operator.

In the example in~\figref{ddc}, the reduction operator is $+$ because x
is clocked  with a reduction operator $+$ in~\lineref{declx1}.
$0$ is the initial value applied to the reduction operator while reducing.
 Every task that uses a clocked variable should explicitly share that
variable in the $spawn$ statement. 
 For instance, in~\figref{ddc}, each 
of $f$ and $g$ , because they explicitly have $clocked(x))$ at the
spawn statement. The main task that executes $h$ is clocked on $x$ 
because main declares it.
At the $next$ statement, every clocked variable in the task advances its phase and the values offered
by clocked variables in the previous phase are reduced and used in the current phase. 


The statement $a \leftarrow 3$, does a delayed write to variable $a$, 
which is a reference to $x$, i.e., a value $3$ is offered to the next phase of $a$.
 When the task calls $next$,
the task advances its phase, forcing the value $3$ to be 
seen by other tasks. 

Therefore after the $next$ statement, the values offered by different
tasks are reduced and henceforth the value of $x$ is 3 + 5 which is 8
and it is reflected everywhere.
Function $h$ also rendezvous with $f$ and $g$  by executing $next$
and thus it obtains the new value 8.

 A task may
share multiple variables.
The $next$ statement is a conjunctive barrier on all clocked variables. 
A task holding a clocked variable waits for all other tasks that is also
holding the same clocked variable to either call \emph{next} or to terminate.

A single task may offer multiple values to the same variable in one phase. These values are also reduced using the commutative associative operator along with the values offered by other tasks. For instance, in~\figref{ddc-multiple}, after the execution of 
$next$ in task $f$, the value of x is 4;
\begin{figure}
\begin{lstlisting}[language=C]
void f(clocked int ref a) {
    a <- 1;
    a <- 1;
    a <- 1;
    next;
    /* x is 4 */
}

void g(clocked int ref b) {
    int local;
    a <- 1;
    next; /* The reduction operator is applied */
    /* x is 4 */
}

main() {
    clocked(+) int x = 0; /*@\label{line:declx}@*/
    /* If there are multiple writers, reduce
    using the + reduction operator */
    spawn clocked(x) f(x) ;
    g(x);
  /* x is 4 */
}
\end{lstlisting}
\caption{Muliple offers in a single phase}
\label{fig:ddc-multiple}
\end{figure}





\begin{lstlisting}

\end{lstlisting}

The programming model is deterministic because writes to a particular variable
are made visible only in the next phase. Also, the model is deadlock-free: a task $A$ waiting on another task $B$ at the \emph{next} statement will eventually
proceed, because task $B$ at some point will call \emph{next} or terminate.

A phase is either separted by two $next$s or separated
by the creation of the clocked variable and a $next$.
If no values are offered in a given phase, then the value from
the previous phase is maintained.

{\it If the programmer does not declare a reduction operator, and there are multiple offers in the same phase, then  at the `next' statement following
the phase, the offers are rejected and the value from
the previous phase is maintained. If there is just one offer, then the following $`next'$ statement makes this new value visible to other tasks.}


\section{Examples in $D^2C$}
\label{sec:examples}
We illustrate our model with a few examples: 
histogram, pipeline and mergesort

\subsection{Histogram}
The program in~\figref{histogram} calculated the frequencies
of values from array $a$ 
into array $b$.
Array $b$ is written concurrently. Therefore it should be clocked.
Multiple tasks may write $1$ to the same location in $b$, but these
values are reduced using the $+$ operator in~\lineref{redb}. After the 
$next$ statement, array $b$ will contain the correctly computed histogram
of array $a$. 

\
\begin{figure}
\begin{lstlisting}[language=C]
void f(int value, clocked int ref b[M]) {
    int bucket = value % M;
    b[bucket] = 1; 
}

main() {
    const int a[N] = {...};
    clocked((+) b[M] /*@\label{line:declb}@*/;
    for (i = 0; i < N; i++)
	spawn clocked(b) f(a[i], b); 
    next; /* Reduction happens here */  /*@\label{line:redb}@*/;

}
\end{lstlisting}
\caption{Histogram example in $D^2C$}
\label{fig:histogram}
\end{figure}

\subsection{Pipeline}
The Pipeline example in~\figref{pipeline}
 creates 3 stages in parallel. \emph{Stage1} gets the input,
\emph{stage2} increments it by 1 and passes it to \emph{stage3}. 
\emph{Stage3}, again increments it by one and prints the value. Every task reads the value
from the current phase and offers the value to the next phase. 
For instance \emph{stage2} reads \emph{a} from the current phase 
and offers a new value to \emph{b}. But this new value of \emph{b} is seen by \emph{stage3} only 
in the next phase. The pipeline example does not need a reduction operator
because only one task is writing to a clocked variable at any phase. The pipeline application is a classic example
 of a program that has multiple phases but with single write at each phase.


\begin{figure}[htbp]
\begin{lstlisting}[language=C]
void stage1(clocked  int ref a) {
    int i = 0;
    for (;; i++) {
        a <- i;
        next;
   }
      
}

void stage2 (clocked  int ref a, clocked int ref b) {
    next; /* Skip first clock */
    for (;;) {
        b <- a + 1; /* b is 1, 2, 3...  */ 
        next;
   }


void stage3 (clocked int ref b) {
    int c;
    next; /* Skip first clock */
    next; /* Skip second clock */
    for (;;) {
         c <- b + 1;
         print(c); /* Prints 2, 3, 4, 5 .... */
         next; 
    }

}

main() {
     clocked int a, b, c; 
     spawn clocked(a) stage1 (a);
     spawn clocked(b) stage2 (a, b);
     stage3 (b);
}
\end{lstlisting}
\caption{Pipeline example in $D^2C$}
\label{fig:pipeline}
\end{figure}


\subsection{Merge Sort}
\label{sec:mergesort}

The mergesort (\figref{mergesort}) is a classic example of red-black computation
but in place.
At every stage, the merging task reads from the current phase
and offers the new (merged) values to the next phase, does
eliminating the need of two explicit arrays.

The $sort$ function spawns two $sort$ functions on the subarrays
and waits for them to finish at the $next$ statement.
The $merge$ function takes the two subarrays and merges them.
The merged values are offered to the next phase. 
After executing  the $merge$ function, the executing task
 returns to its parent function ($sort$) and terminates
itself.  The parent $sort$ that spawned this task waits until all its children
terminate at the $next$ statement in~\lineref{next1}. After this statement
the values offered at the previous phase are shifted to the current phase.




\begin{figure}[htbp]
\begin{lstlisting}[language=C]
void sort (clocked int a[], const int start, const int end)  {
    /* first half */
    const int fStart = start;
    const int fEnd  = start + (end - start)/2;
    /* second half */
    const int sStart = fEnd + 1;
    const int sEnd  = end;

    if (start == end)
        return;
        	
    spawn clocked(a) sort (a, fStart, fEnd); /* Sort first half */ 
    sort (a, sStart, sEnd); /* Sort second half */ 
    next; /* Wait for the subarrays to be sorted */ /*@\label{line:next1}@*/;
    merge(a, fStart, fEnd, sStart, sEnd);
}
    
void merge (clocked int ref a[],  const int fStart, 
    const int fEnd,  const int sStart, const int sEnd) {
    	
    int x = fStart;
    int y = sStart;
    int z = fStart;
    const int size = (sEnd - fStart) + 1;
  
    /* Read from a and offer to the next phase of a */   
 
    while(x <= fEnd && y <= sEnd) {
         if(a(x) < a(y))
             a(z++) = a(x++);
         else
             a(z++) = a(y++);
    }
    while(x <= fEnd) {
         a(z++) = a(x++);
    }
    while(y <= sEnd) {
         a(z++) = a(y++);
    }
}

main {
     const int N = 100;
     clocked int a[N]; 
     /* Initialize a below */
     ..
     next; /* Move the intialized values to the next phase */ 
     sort(a, 0, N - 1);
     next; /* Move the offered sorted array to the next phase */
     print (a) ; 
  }
}
\end{lstlisting}
\caption{Merge Sort in $D^2C$}
\label{fig:mergesort}
\end{figure}

\section{Proof of Correctness}
\label{sec:proof}
In this section, we prove that our model guarantees the two desirable properties of concurrency: determinism
and deadlock-freedom.

\subsection{Determinism}
A task always reads from the current phase and writes to the next phase.
Therefore, there are no read-write conflicts.
At the $next$ statement, all concurrent writes are reduced using an associative, commutative
reduction operator. If there is no declared operator but multiple writes,
the writes are ignored. This ensures write-write conflict freedom.

Whenever a clocked variable is created, the creating $task$ owns that
variable. Whenever the clocked variable is used by a \emph{spawn}, the spawned
task also owns the variable. At the $next$ statement, the task waits for all 
threads that share clocked variables with it to either call \emph{next} or terminate.

A task will always synchronize with every task that has already spawned and shares variables with it. Synchronization is deterministic.
We prove this by contradiction. Consider two tasks $t_i$ and $t_j$
that share variable $x$. Suppose a task $t_i$ synchronizes with 
all tasks that share variable $x$  but an already spawned $t_j$.
This implies that $t_i$ is not aware that $t_j$ has been spawned.
Since $t_j$ uses variable $x$, $t_j$'s parent $t_p$ should
also use variable $x$.  $t_p$ is aware that $t_j$ has already spawned because
it is $t_j$'s immediate parent. Therefore $t_p$ will synchornize with $t_j$ thereafter. 
When $t_i$ synchonizes with $t_p$'s $next$, $t_p$ will synchronize with $t_j$, 
forcing $t_i$ to synchronize with $t_j$, and therefore  contradicting the statement that $t_i$ will
miss $t_j$. This follows from the fact that a $spawn$ can happen in serial with a $next$ statement
and not concurrently with it.  Secondly, parent should also clock  $x$ if it spawns a child 
that clocks $x$, unless $x$ is declared in the child.


\subsection{Deadlock-freedom}

The only statement that a task can wait on is the $next$ statement.
Suppose a task $t_i$ waits on the $next$ statement for task $t_j$.
$t_i$ waits until $t_j$ either executes $next$ or terminates.
Every task will either call $next$ or terminate and therefore
$t_i$ can never deadlock.
The $next$ statement  behaves like a conjunctive join on 
all clocked variables.

Suppose $t_i$ waits on $t_j$, $t_j$ waits on $t_k$, and so on
and finally $t_n$ calls $next$ to wait for $t_i$, then there is a cycle in the 
graph. But $t_n$ realizes that $t_i$ has already called $next$, and therefore
it does not wait on $t_i$, breaking the cycle.  

We do not allow Cilk's $sync$ statements in our model because it can cause
deadlocks with $next$ statements.
But a $sync$ statement can be implemented using a $next$ statement as follows:
\begin{lstlisting}[language=c]
clocked void s;
spawn clocked(s) f(s);
spawn clocked(s) g(s);
next;
\end{lstlisting}

The \emph{next} statement forces all tasks that hold $s$ to either
call \emph{next} or terminate. If the body of $f$ and $g$ does not
use $next$, then the role of the $next$ statement is the same as a $sync$
statement.


\section{Implementation}
\label{sec:implementation}

We implemented our model in the X10 programming language \cite{charles2005x10}.
X10 is a parallel, distributed object-oriented language. 
To a Java-like sequential 
quential core it adds constructs for concurrency and distribution through the 
concepts of activities and places. An activity is a unit of work, like a task in 
Cilk; a place is a logical entity that contains both activities and data objects.
X10 uses the Cilk model of task parallelism and a task scheduler similar to that
of Cilk.

In our implementation, we do not support activities at multiple places; we assume all activities run
in a single place - something similar to a shared memory system. 
We force all shared variables to be clocked. This forces
race-freedom. If a shared variable is not clocked, the compiler throws
an error.

During runtime, we maintain two states for each variable, \emph{write state} and \emph{read state}.
If the clocked variable is read, it is simple read from the \emph{read state}. If the clock
variable is written, it is written but atomically to the \emph{write state}.  The value in the \emph{write
state} and the value to be written are reduced using the reduction operator, and this newly obtained
value is written back to the \emph{write state}. This access and modification to the write state is done
atomically to ensure determinism during concurrent writes by different activities.
Whenever, a \emph{next} is called, we swap the references to the \emph{read state} and \emph{write state}.
The \emph{write state} of the previous phase is now the \emph{read state} and vice-versa.
We also clear the \emph{write state} of the current phase.
We ensure determinism by duplicating states. The amount of memory is independent on the number
of tasks or the interleavings, but is only twice of the original program.


Everytime a value is written to a shared variable, a lock is obtained. This was a major
bottleneck in many of our benchmarks. To overcome this problem, we maintained a copy
of the shared variable for every thread created in the program. Since every thread
has its own copy, a lock is not necessary. At the $next$ statement, we reduce
the values offered by the different threads. With this technique, we achieved
about $3-4\%$ improvement in performance. Now, the memory does not depend
on the interleavings but is directly proportional to the number of tasks.




\section{Experimental Results}
\label{sec:experimental}

To test the performance of our model, we ran a number of examples on a 1.6~GHz
Quad-Core Intel Xeon (E5310) server running Linux kernel~2.6.20 with
\textsc{smp} (Fedora Core 6).  The processor ``chip'' actually
consists of two dice, each containing a pair of processor cores.  Each
core has a~32~KB L1 instruction and a~32~KB L1 data cache, and each
die has a 4~MB of shared L2 cache shared between the two cores.

\begin{figure*}[htbp]
\includegraphics[angle=-90]{figures/output.eps}
\caption{Relative performance of the determinized applications on a quad core machine}
\label{fig:output}
\end{figure*}

We tested our model with real applications.
~\figref{output}
shows the results. We measured the determinstic implementation of the applciations
with the original implementation. A bar with value below 1 indicates that the deterministic
verion ran slower than the orignial version. 

Each of the applications created
4 threads.
The AllReduce Example is a parallel tree based implementation of reduction.
The Pipeline example passes data through a number of intermediate stages; at each
stage the data is processed and passed on to the next stage. Convolve is an
application of the Pipeline program.

The N-Queens Problem finds the number of ways in which N queens can be placed
on an N*N chessboard such that none of them attack each other. The MontiPi application
finds the value of $\pi$ using MonteCarlo simulation. The K-Means program partitions
n data points into k clusters concurrently.

The Histogram program sorts an array into buckets based on the elements of the array.
The Merge Sort program sorts an array of integers.
The Prefix example operates on an array and the resulting array is obtained from the sum
of the elements in the original array up to its index.

The SOR, IDEA, RayTrace, LUFact, SparseMatMul and Series programs are JGF benchmarks.
The Raytracer benchmarks renders an image of sixty spheres. It has data dependent
array access.

The SOR example performs Jacobi successive relaxation
on a grid; it continuously updates a location of the grid based on the location's
neighbors.
The Stencil program is the  1-D version of the SOR.

The LUFact application transforms an N*N matrix into upper triangular form. The Series benchmark
computes the first N coefficients of the function $f(x) = (x+1)^x$. The IDEA benchmark
performs International Data Encryption algorithm  (IDEA) encryption and decryption on an array
of bytes.
The SparseMatMul program performs multiplication of two sparse matrices.


The UTS  benchmark~\cite{olivier2006uts}
performing an exhaustive search on an unbalanced tree.
It  counts the number of
nodes in  the implicitly constructed tree that is parameterized in
 shape, depth, size, and imbalance.

For most of the examples, the deterministic version had  performance degradation
 as expected. However, for some examples like SOR and Stencil, the deterministic
version performed better. The original version of these examples had explicit 2-phased 
barriers to differentiate between reads and writes, while the deterministic
version requires just a single phase, because the implementation maintains
 a duplicate copy 
to eliminate read-write conflicts. Hence, the deterministic
version performed better. The Java and C++ versions did about the same.

\begin{figure}[htbp]
\includegraphics[scale=0.5,angle=-90]{figures/next.eps}
\caption{Performance of $next$ statement with varying number of tasks}
\label{fig:next}
\end{figure}


To measure the performance of the synchronizing $next$ statement, which
is the real bottleneck of our model, we created a shared variable.
Each task updates calls $next$ 500 times, and between every two $next$s, it
updates the shared variable. This  forces
each task to synchronize with every other task 500 times.
All tasks do exactly ther same job. \figref{next} shows the output.
The x-axis represents the number of tasks and the y-axis is the time.

In a perfect scalable system, if we add a new task to the system, then we 
expect the  speed to remain the same (assuming the number of tasks is  
 equal to the number of cores). Our system is not perfectly scalable,
as expected, therefore we see a curve rather than a horizantal line for both
the C++ and Java versions in \figref{next}.



\begin{figure*}[htbp]
\includegraphics[scale=0.7,angle=-90]{figures/azul.eps}
\caption{Experimental results with 64 cores }
\label{fig:azul}
\end{figure*}



\section{Related Work}
\label{sec:related}

A number of groups are working on a similar problem.
In this section, we review some of the related work
and compare them with ours.
 
\subsection{Determinizing Tools} 
 
There are a number of tools that provide determinism. For example, 
in the absence of data races, Kendo~\cite{olszewski2009kendo} ensures 
a deterministic order of all lock acquisitions for a given program input. 
However, if we have the sequence \emph{lock(A); lock (B)} by one thread 
and \emph{lock(B); lock(A)} by another thread, the deterministic ordering 
of locks could still lead to a deadlock. 
 
The DMP~\cite{devietti2009dmp} tool uses a deterministic token that is passed around all threads.  It completely works at runtime and there is a considerable performance penalty. 
 
Burmin and Sen~\cite{Burnim2009asserting} provide a framework for checking determinism for 
multithreaded programs. Their tool does not  
 guarantee determinism because it is a testing tool 
that checks the execution trace  with previously executed 
traces to see if the values match. 

\subsection{Programming Models} 
 
SHIM~\cite{edwards2005shim2,tardieu2006scheduling-independent} is also
a deterministic concurrent programming language, but the improper use
of its constructs leads to problems such as deadlocks i.e., a SHIM program
may be susceptible to deadlocks. Any program written in our model is always
deadlock-free.
 
Apart from SHIM, there are  a few programming models and languages 
that provide explicit determinism. StreamIt~\cite{thies2001streamit}, for 
example is a synchronous dataflow language that provides determinism. It 
has simple static verification techniques for deadlock and buffer-overflow. 
However, StreamIt is a strict subset of SHIM and StreamIt's design 
limits it to a small class of streaming applications. 
 
Synchronous programming languages like Esterel are completely deterministic. An Esterel program 
executes in clock steps and the outputs are conceptually synchronous with its inputs. 
It is a finite state language that is easy to verify formally. An Esterel program is susceptible 
to causalities. Causalities are similar to deadlocks, but can be easily detected at compile-time. 
The problem with synchronous models is that they do not perform well. To out knowledge, 
most Esterel compilers generate sequential code and there are hardly any compilers that generate 
concurrent code off Esterel. 
 
\subsection{Type Systems} 
 
Finally, type and effect systems like DPJ~\cite{bocchino2009type} 
 have been designed for deterministic parallel programming to see if
memory locations overlap. Our technique is more explicit. 
In general, type systems require the programmer to manually annotate the program. Our model can also be implemented using annotations in existing
programming languages - we in fact annotated the X10 programming language.
 
\subsection{Software Transactional Memory} 
Software Transactional Memory (STM)~\cite{shavit1995software} 
  is an alternative to locks: a thread completes modifications to  
shared memory without regard for what other threads might be doing. At the end of the transaction, 
it validates and commits if the validation was successful, otherwise it rolls back and re-executes 
the transaction. There are a couple of differences between STM and $D^2C$.
There is no rollback in our runtime system. Secondly,
 STM mechanisms avoid races but do not solve the non-determinism problem. 
 
Tools such as Grace~\cite{berger2009grace} use principles similar to STM, and impose 
an order in which transactions have to be committed. Grace solves the determinism problem 
but it incurs a lot of run-time overhead. Coredet~\cite{bergan2010coredet} is also similar but it does some compiler optimizations. 




\section{Conclusions}
\label{sec:conclusions}
We have presented a concurrent model of computing that addresses the
two major problems of concurrency: non-determinism and deadlocks.
We have proved the correctness of our model. We have evaluated
our model and shown that our model works reasonably well on many
examples.
Our model is merely a simple construct that gives determinism,
flexible enough to write a range of codes, and is implementable with little overhead.

Our current runtime implementation is very basic; 
We wish to optimize
the implementation especially for array data structures.

All clocked variables are created on the stack.
We do not deal with heap data structures because
it introduces aliasing problems.
We wish to improve
our implementation by allowing variables to be created on the heap.
However, we did not find the lack of heap as a huge bottleneck.
We could build all the applications in~\secref{experimental} without
using the heap.

Currently, we force all shared variables to be clocked. We would
like to improve our model by incorporating intelligent
static analysis to reduce runtime overhead.


We have implmented our model as annotations in an existing
programming language, X10.
We also wish to explore the possibilities of implementing such a model
as a library.

Lastly, the challenge still remains to verify if the reduction operator
is commutative and associative especially with user defined operators.
However, this problem is simplified because operators cannot
have concurrent activities in it.
Nate Clarke's work~\cite{aleen2009commutativity} on commutative
analysis for instance uses randomized testing, but does not completely
verify the commutative property.


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
\IEEEtriggeratref{16}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% can use a bibliography generated by BibTeX as a .bbl file
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtranBST/IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEtranBST/IEEEabrv,ddc}


\end{document}

1 AllReduceParallel 5668 6104 0.928571
2 Pipeline 8436 9258 0.911212
3 Convolve 7610 6070 1.25371
4 NQueensPar 37106 37302 0.994746
5 MontyPiParallel 11104 11342 0.979016
6 KMeansScalar 316073 317345 0.995992
7 Histogram 4357 12151 0.358571

An example:
--------------

A Pipeline Example:

import clocked.Clocked;
public class Pipeline {

   public def pipeline() {
   	     finish {
  		val c = Clock.make(); 
   		val op = Int.+;
      		shared var a: int @ Clocked[int] (c, op, 0) = 0;
   		shared var b: int @ Clocked[int] (c, op, 0) = 0;
 	   	 async clocked(c)  {
                        var i: int;
                        for (i = 0; i < 10; i++)  {
                                a <- i;
                                next;  
                        }
                }
        	async clocked (c) {
                        var i: int;
                        for (i = 0; i < 10; i++)  {
                                next; 
                                b <- a + 1;
                        }
                }
      		var i: int;
      		next; 
      		for (i = 0; i < 10; i++)  {
                        next; 
                        val o = b + 1;
                        Console.OUT.println(o);
                }
            }
        }

    public static def main(args:Rail[String]!) {
         val h = new Pipeline();  // final variable
         h.pipeline();
    }
}


The Syntax
-----------

A variable is declared as follows:
val var_name: Type @ Clocked[Type] (clk, op, init);


A method that uses a clocked variable has to have its body annotated as follows
method_desc method_name(arg1: T1, arg2: T2, arg3: T2, ...) : Tn @Clocked(clk) {
	/* Body of the method */

}



Compiler's role:
---------------

Type checking for clocked variables: 
The compiler forces the following:
1. Every async that uses a clocked variable should be clocked on the respective clock.
2. Every method that uses a clocked varibale should have its body annotated by @Clocked

Determinism Check:
At the compiler level,
I do a very conservative analysis to check if a particular variable is being used
by multiple tasks and verify for read-write and write-write conflicts using x10.effects. 
If there is a conflict, the compiler throws an error and 
 the programmer has to declare the variable clocked. This forces
race-freedom. If a variable has a possibility of race and is not declared
clocked, the compiler throws an error.

Runtime:
--------

During runtime, I  maintain two states for each variable, \emph{write state} and \emph{read state}.
If the clocked variable is read, it is simple read from the \emph{read state}. If the clock
variable is written, it is written but atomically to the \emph{write state}.  The value in the \emph{write
state} and the value to be written are reduced using the reduction operator, and this newly obtained
value is written back to the \emph{write state}. This access and modification to the write state is done
atomically to ensure determinism during concurrent writes by different activities.
Whenever a \emph{next} is called, we swap the references to the \emph{read state} and \emph{write state}.
The \emph{write state} of the previous phase is now the \emph{read state} and vice-versa.
We also clear the \emph{write state} of the current phase.
We ensure determinism by duplicating states. The amount of memory is independent on the number
of tasks or the interleavings, but is only twice of the original program.

Optimization:

Everytime a value is written to a shared variable, a lock is obtained. This was a major
bottleneck in many of our benchmarks. To overcome this problem,  I maintained a copy
of the shared variable for every thread created in the program. Since every thread
has its own copy, a lock is not necessary. At the $next$ statement, we reduce
the values offered by the different threads. With this technique, we achieved
about $3-4\%$ improvement in performance.

-----------------------

Determinism Analysis:

We use effect analysis to check if a program is deterministic:
A statement S is labeled by one of the following
1. fun
2. parfun
3. par

The rules of propogation of effects are the same as 
the rules described in lec8 of PPPP class.

Here is the list of modifications to Vijay's code.


Loops
-----

Treating a while loop:

while(cond) {S1}

The effect of this statement is
Effect(cond). Effect(S1). Effect(cond). Effect{S1}
i.e the effect is same as the effect of the loop
unrolled twice. We over-approximate variables within
the loop, i.e for some index i if a(i) is written,
we assume that a(i) is written for all i.
We follow a similar procedure for "for" loops.

Function calls
--------------

We force the effect of every function to be a fun.
i.e a function should not leave any non-terminated process
upon its exit. Also, it should not cause any r-w/w-w conflicts.
The compiler throws an error if either of these conditions
are not met.

Also, the effects of the callee are propagated up to the caller.
For recursive code, this becomes a problem. So we just propogate
to a certain depth "k" in the recursion tree.

Clocked Variables
-----------------

The effect analysis checks for r-w/w-w conflicts. If there is a
a conflict, it checks whether the particular variable causing
the conflict is clocked. If the variable is clocked, then it
is not a conflict. In summary, the effect analysis procedure
filters out clocked variables from its conflict set. If the
resultant conflict set is null, then the program is deterministic
otherwise the compiler flags an error.

Implementation details
---------------------

The Effects set originally was composed of read, write and
atomic sets. Now, it has an additional clocked set.

To run the effect analysis code, use the SAFE_PARALLELIZATION_CHECK
=true

On 8 way

1 AllReduceParallel 5816 5914 0.983429
2 Pipeline 6821 6159 1.10748
3 Convolve 6704 6177 1.08532
4 NQueensPar 13672 17504 0.781079
5 MontyPiParallel 7098 8377 0.84732
6 KMeansScalar 312029 314043 0.993587
7 Histogram 4426 11722 0.377581
8 MergeSort 6249 15521 0.402616
9 Stream 12045 10130 1.18904
10 Prefix 4408 6811 0.647188
11 UTS 35705 70404 0.507144
12 IDEA 4621 9804 0.471338
13 SOR 1826683 138171 13.2205
14 Stencil 732713 421931 1.73657
15 Series 21434 54896 0.390447
16 RayTrace 17911 22173 0.807784
17 LUFact 69940 66743 1.0479
18 SparseMatMul 5981 13398 0.44641







