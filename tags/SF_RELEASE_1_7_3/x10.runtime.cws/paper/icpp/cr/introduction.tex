\section{Introduction}
\label{s:intr}\label{sec:intro}

Obtaining practical efficient implementations for large, irregular
graph problems is challenging. Current software systems and commodity
multiprocessors do not support fine-grained, irregular parallelism
well. Implementing a custom framework for fine-grained parallelism for
each new graph algorithm is impractical.

We present \XWS{}, the \Xten{} Work Stealing framework.  \XWS{} is
intended as an open-source runtime for the programming language
\Xten{} \cite{x10}, a partitioned global address space language supporting
fine-grained concurrency.  \XWS{} is also intended as a library to be
used directly by application writers. \XWS{} extends Cilk
work stealing \cite{BJKLRZ95,frigo98implementation} with several
features necessary to efficiently implement graph algorithms, viz.,
support for improperly nested procedures, worker-specific
data-structures, global termination detection, and phased computation.

We present simple elegant programs using \XWS{} for different spanning
tree algorithms using (pseudo-)depth-first search and breadth-first
search.  We evaluate these programs on a 32-way Niagara (moxie), and
an 8-way Opteron server (altair) and on three different bounded-degree
graphs: (i) graphs with randomly selected edges and (a) no degree
restrictions (b) fixed degree, and (ii) planar torus graphs.

We show the performance of BFS and pseudo-DFS search depends crucially
on the granularity of parallel tasks. We show that the granularity
natural to the algorithms -- the examination of a single edge ---
leads to poor performance at scale. Instead, sets of of vertices must
be grouped into {\em batches}. We show that a fixed-size batching
scheme does not perform well. For instance, batches of size $1$ yield
a peak performance of 20 MEPS (Million Edges Per Second) on Niagara.
Instead we develop an adaptive batching scheme in which the batch
size is sensitive to the instantaneous size of the work queue.  With
this scheme, pseudo-DFS shows linear scaling on Niagara and Opteron,
achieving peak performance of over 220 MEPS on moxie and substantially
outperforming C and Cilk implementations.

\subsection{Challenges in solving large, irregular graph problems}
The last few years have seen an explosion of mainstream
architectural innovation --- multi-cores, symmetric multiprocessors,
clusters, and accelerators (such as the Cell processor and GPGPUs) ---
that now requires application programmers to confront varied
concurrency and distribution issues. This raises the fundamental
question: what programming model can application programmers use to
productively utilize such diverse machines and systems?

Consider for instance the problem faced by designers of graph
algorithms.  Graph problems arise in traditional and
emerging scientific disciplines such as VLSI design, optimization,
databases, computational biology, social network analysis, and
transportation networks.

Large-scale graph problems are challenging to solve in parallel --
even on shared memory symmetric multiprocessor (SMP) or on a multicore
system -- because of their irregular and combinatorial nature.
Irregular graphs arise in many important real world settings. For
random and ``scale-free'' graphs \cite{CZF04} no known efficient
static partitioning techniques exist, and hence the load must be
balanced dynamically.  

Consider the spanning tree problem. Finding a spanning tree of a graph
is an important building block for many graph algorithms such as those
for biconnected components, ear decomposition \cite{MR86} and graph
planarity testing \cite{KR88}.  Spanning tree represents a wide range
of graph problems that have fast theoretic parallel algorithms but no
known efficient parallel implementations that achieve speedup without
serious restrictive assumptions about the inputs.

Bader and Cong \cite{BC04a} presented the first fast parallel spanning
tree algorithm that achieved good speedups on SMPs. Their algorithm is
based on a graph traversal approach, and is similar to DFS or BFS.
There are two steps in the algorithm. First a small stub tree of size
$O(p^2)$ is generated by one of the $p$ worker through a random walk of the
graph. The vertices of this tree are then evenly distributed to each
worker.  Each worker then traverses the graph in a manner similar to
sequential DFS or BFS, using efficient atomic operations (e.g.{}
Compare-and-Swap) to update the state of each node (e.g.{} update the
{\tt parent} pointer). The set of nodes being worked on is kept in a
local queue.  When a worker is finished with its portion (its queue is
empty), it checks randomly for any other worker with a non-empty
queue, and ``steals'' a portion of the victim's work for itself.

For efficient execution, it is very important that the queue be
managed carefully. For instance, the operation of adding work (a node)
to the local queue should be efficient (i.e.{} should not require
locking) since it will be performed frequently. Stealing is however
relatively infrequent and it is preferable to shift the cost of
stealing from the victim to the thief since the thief has no work to
do (the ``work first'' principle). The graph algorithm designer now
faces a choice. The designer may note \cite{BC04a} that correctness is
not compromised by permitting a thief to {\em copy} the set of nodes
that the victim is working on. Here the victim is permitted to write
to the queue without acquiring a lock. Now the price to be paid is
that the thief and the victim may end up working on the same node
(possibly at the same time).  While work may thus be duplicated,
correctness is not affected since the second worker to visit a node
will detect that the node has been visited (e.g.{} because its atomic
operation Compare-and-Swap will fail) and do nothing. Alternatively,
the designer may use a modified version of the Dekker protocol
\cite{BJKLRZ95} to resolve the race condition.  This guarantees that
no work will be duplicated, but the mechanism used is very easy to get
wrong, leading to subtle concurrency errors.

The above illustrates that the design of such high-performance
concurrent data-structures is difficult and error-prone. 
%Those concerns that are of interest to the graph algorithm designer (e.g.{} expressing breadth-first vs depth-first search) are mixed in with the concerns for efficient parallel representation.  
This suggests packaging the required components in a library or a framework and
exposing a higher-level interface to programmers.

\subsection{\Xten{}}
The \Xten{} programming language \cite{x10} has been designed to
address the challenges of ``productivity with performance'' on 
diverse architectures.  \Xten{} augments a core
sequential modern object-oriented language (very similar to Java or Scala) with constructs for distribution ({\em places}) and
concurrency ({\em asyncs}, {\em finish}, {\em atomic} and {\em
clocks}).\footnote{Discussion of the distribution constructs of
\Xten{} and advanced concurrency constructs ({\tt when}) is out of
scope of this paper.}  The statement {\tt async S} spawns a new task
or activity to execute the statement {\tt S}. The statement {\tt
finish S} executes {\tt S} and waits for all activities spawned during
its execution to terminate. The statement {\tt atomic S} causes the
statement {\tt S} to be executed as if in a single indivisible step
(with no other activity executing during this step). An \Xten{} clock
is a data-structure that represents a dynamic barrier. The activity
creating a clock is said to be registered on that clock. An activity
that is registered on a clock {\tt c} may create new activities
registered on {\tt c} by executing {\tt async clocked(c) S}. An
activity registered on a clock may execute a {\tt next} operation to
suspend until such time as all activities registered on the clock have
executed a {\tt next} operation (barrier behavior).

\Xten{} may be used to implement spanning tree computations in a very straighforward way:
\begin{example}[Psuedo-DFS in \Xten] \label{example:dfs}
The parallel exploration of a graph may be implemented thus:
{\footnotesize
\begin{verbatim}
1. class V  {
2.   V [] neighbors;
3.   V parent;
4.   V(int i){super(i);}
5.   boolean tryColor(V n) {
6.     atomic if (parent ==null) parent=n;
7.     return parent==n;
8.   }
9.   void compute() {
10.     for (V e : neighbors) 
11.       if (e.tryColor(this)) 
12.             async e.compute();
13.   }
14.   void dfs() {
15.     parent = this; // root is visited.
16.     finish compute();
17.   }}
\end{verbatim}}
Computation is initiated by invoking {\tt r.dfs()} on the root vertex
{\tt r}. This visits {\tt r} within the scope of a {\tt finish}. When
a vertex is visited (its {\tt compute()} method invoked), each of its
outgoing edges is examined in sequence. If the vertex reached from the
edge does not have its parent set, its parent is set atomically, and
the vertex is (recursively) visited asynchronously. Thus an activity
is spawned for each vertex in the connected component containing {\tt
r}. (This code does not implement batching, a batched version is
discussed later.)
\end{example}

We note that this program cannot be written in Cilk without excessive
synchronization. Cilk enforces a ``fully strict'' condition: in
\Xten{} terminology this condition requires that if the body of a
procedure spawns an {\tt async}, then it must contain a {\tt finish}
enclosing that {\tt async}.  Thus to write this program in Cilk an
extra {\tt finish} must be wrapped around the {\tt for} loop on Line
10. This introduces needless extra synchronization which reduces
performance.

\begin{example}[BFS in \Xten] \label{example:bfs}
The breadth-first parallel exploration of a graph may be implemented
as follows:
{\footnotesize
\begin{verbatim}
1. class V  extends VertexFrame {
2.   V [] neighbors;
3.   V parent;
4.   V(int i){super(i);}
5.   boolean tryColor(V n) { ... }
6.   void compute(clock c) {
7.     for (V e : neighbors) 
8.       if (e.tryColor(this)) 
9.         async clocked(c) { 
10.            next; 
11.            e.compute(c);
12.         }}
13.   void bfs() {
14.     parent=this;
15.     finish async {
16.      clock c = new clock();
17.      compute(c);
18.     }}}
\end{verbatim}}
The code differs from DFS in that all asyncs are clocked. 
A new node is visited only after the clock advances (Line 10-11).
Hence all vertices are visited in breadth-first order.
\end{example}
\subsection{\XWS}
A central challenge in implementing \Xten{} is to efficiently
load-balance multiple activities, while respecting dependencies
introduced by clocks and {\tt finish}. This paper presents the design,
implementation and evaluation of a portion of the \Xten{} runtime
system for multicore and SMPs, \XWS{}. \XWS{} implements fine-grained
concurrency through an extension of Cilk Work Stealing (\CWS)
\cite{BJKLRZ95,frigo98implementation}.  
%Work stealing is a powerful technique organized
%around a collection of workers (threads) that each maintains a
%double-ended queue (deque) of {\em frames} (or tasks). A worker pops
%and pushes frames from the bottom of the deque. When its deque is
%empty, it randomly selects another worker and attempts to steal a
%frame from the top of its deque. CWS is carefully organized to
%streamline parallel overhead so that execution of the code with a
%single worker incurs a small constant factor overhead over execution
%of the sequential code. The overhead associated with stealing is
%deferred to the worker performing the steal (the {\em thief}) as
%opposed to the worker being mugged (the {\em victim}). The CWS
%algorithm is known to have nice properties in theory, and can be
%efficiently implemented in practice.
%
\XWS{} extends \CWS{} to better support the programming of
applications with irregular concurrency. It removes the link between
recursion and concurrency introduced by fully strict Cilk
model. Crucial to this removal is a method in \XWS{} for detecting
termination of a computation without counting all the frames created
during the computation.  Further, \XWS{} integrates barriers --
essential for phased computations such as breadth-first search -- with
work stealing. Finally, \XWS{} support the implementation of {\em
adaptive batching} schemes by the programmer. Batching is a technique
for increasing the granularity of parallel tasks by batching together
several small tasks. Thieves steal a batch at a time. Depending on the
algorithm, the batching size may have a dramatic impact on the
performance of work stealing.  \XWS{} permits the
(\Xten{} or \XWS{}) programmer to sense key metrics of the current
execution and use these to adjust batching size dynamically.

\subsection{Rest of this paper}

The rest of this paper is as follows. In Section~\ref{sec:XWS}, we
present the details of the design of \XWS. In Section~\ref{sec:Graph}
we examine comparable programs written using an application-specific
framework (Simple, \cite{BC04a}), as well as Cilk, and compare
performance on three different graph inputs.  Our graph generators
include several employed in previous experimental studies of parallel
graph algorithms for related problems. For instance, we include the
torus topologies, random graphs and geometric graphs, used in \cite{Gre94}, \cite{HRD97} and others.
%Krishnamurthy \emph{et al.} \cite{KLC97}, and others.

\begin{itemize}
\itemsep0pt
\item \textbf{2D Torus} The vertices of the graph are placed on a 2D
  mesh, with each vertex connected to its four neighbors.  

\item \textbf{Random Graph} We create a random graph of $n$ vertices
  and $m$ edges by randomly adding $m$ unique edges to the vertex
  set. Several software packages generate random graphs this way.
%  including LEDA \cite{MN99}.
  
\item \textbf{Geometric Graph} In these $k$-regular graphs,
  %$n$ points are chosen uniformly and at random in a unit square in
  %the Cartesian plane, and 
  each vertex is connected to its $k$ %nearest
  neighbors.  
%Moret and Shapiro \cite{MS94} use these
%  in their empirical study of sequential MST algorithms. \textbf{AD3}
%  is a geometric graph with $k=3$.  
\end{itemize}

%%% SUMMARIZE RESULTS.

We show that the performance of these programs in \XWS{} can be
substantially improved with batching. We present schemes for
adaptively determining the size of the batch based upon an estimate of
the current stealing pressure.

Finally we conclude with a section on related work and
acknowledgements. Due to the page limit, we can not provide more
examples on how to use the \XWS{} framework. Readers please refer to
the Java implementation of \XWS{} for details, available at {\tt
http://x10.sf.net}, in the module {\tt x10.runtime.xws}.
