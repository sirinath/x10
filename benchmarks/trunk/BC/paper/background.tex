\section{Background}
\label{sec:background}
% Define BC
Betweenness Centrality (BC) is a graph theoretic metric of a node's importance
in a graph.~\footnote{In this paper, we focus exclusively on the unweighted,
directed graphs.}
%
Informally, a node's BC gives an indication of the ratio of the total number 
of shortest paths between all other nodes taken pair-wise.
%
That is, a node with a high centrality score can reach other nodes with 
relatively fewer hops than another node with a lower centrality score.
%
More formally, let $G(V,E)$ be a graph with the vertex set $V$ and edge set
$E$; let the number of vertices ($\lvert{}V\rvert{})$ be $n$ and the number of 
edges ($\lvert{}E\rvert{}$) be $m$.
%
The BC of a node (vertex) $v\in{V})$ is given by the equation:
%
\begin{equation}
BC_{v} = \sum_{s\ne{}v\ne{}t\in{}V}{\frac{\sigma{}_{st}(v)}{\sigma{}_{st}}}
\label{eq:bc}
\end{equation}
%
Where $\sigma{}_{st}$ is the number of shortest paths from node $s$ to node $t$ 
and $\sigma{}_{st}(v)$ is the number of those shortest paths that go through 
node $v$.
% Get into the simple means of computing BC
There are multiple ways of computing the BC scores of nodes in a graph; in the
following subsections, we highlight a few of these techniques.
%
Please note that regardless of the approach that is used in computing BC, the 
central kernel is enumerating all the shortest paths.

\subsection{Algebraic Approach}
\label{subsec:algebraic}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\begin{minipage}{0.20\textwidth}
\begin{center}
\begin{displaymath}
\xymatrix{
d \ar[r] & a \ar[dl] \\
b \ar[u] \ar[r] & c \ar[u]
}
\end{displaymath}
\end{center}
\end{minipage}
\hspace{10pt}
\begin{minipage}{0.18\textwidth}
\begin{center}
\begin{displaymath}
A = \left[ \begin{array}{cccc}
  0 & 1 & 0 & 0 \\
  0 & 0 & 1 & 1 \\
  1 & 0 & 0 & 0 \\
  1 & 0 & 0 & 0 \\
\end{array} \right]
\end{displaymath}
\end{center}
\end{minipage}
\caption{A sample directed, unweighted graph and its resulting adjacency
matrix. A $1$ in position $a_{ij}$ indicates an edge from node $i$ to node
$j$.}
\label{fig:sample}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Every graph $G(V,E)$ can be represented as an adjacency matrix $A$, where an 
entry $a_{ij}$ is marked as $1$ \textit{iff} $(i,j)\in{}E$.
%
Figure~\ref{fig:sample} shows a sample four node graph and its related 
adjacency matrix; we will be using this graph as a running example throughout
this section.
%
By inspection, we can tell that the diameter of the graph is $3$; therefore, 
all the possible paths in the matrix (including the number of paths) can be 
enumerated by simply taking the powers of the adjacency matrix $A$; in other 
words, we find the transitive closure of the graph $G$.
%
However, computing the transitive closure is an over-kill; what we want in 
order to compute BC are just the shortest paths between all pair of vertices,
not all paths of length diameter or less.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\begin{minipage}{0.2\textwidth}
\begin{center}
\begin{displaymath}
A^2 = \left[ \begin{array}{cccc}
   0 & 0 & 1 & 1 \\
   2 & 0 & 0 & 0 \\
   0 & 1 & 0 & 0 \\
   0 & 1 & 0 & 0 \\
\end{array} \right]
\end{displaymath}
\end{center}
\end{minipage}
\hspace{10pt}
\begin{minipage}{0.2\textwidth}
\begin{center}
\begin{displaymath}
A^3 = \left[ \begin{array}{cccc}
   2 & 0 & 0 & 0 \\
   0 & 2 & 0 & 0 \\
   0 & 0 & 1 & 1 \\
   0 & 0 & 1 & 1 \\
\end{array} \right]
\end{displaymath}
\end{center}
\end{minipage}
\caption{The $2^{nd}$ and $3^{rd}$ powers of the adjacency matrix, $A$, shown
in Figure~\ref{fig:sample}. $A^2$ shows the paths of path length $2$, and $A^3$
shows the paths in the sample graph of path length $3$. The entries in $A$,
$A^2$, and $A^3$ indicate the number of paths. For example, $A^2_{2,1}=2$ as
there are two paths of length 2 between $b$ and $a$ in Figure~\ref{fig:sample}.}
\label{fig:sample_powers}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Floyd-Warshall's Algorithm
\begin{algorithm}
\SetKwFunction{edgeCost}{edgeCost}
\SetKwFunction{minimum}{min}

\caption{FloydWarshall}
\label{alg:floyd_warshall}
\KwIn{$A$: Adjacency matrix of graph $G(V,E)$}

\For{$i=1:n$}{
  \For{$j=1:n$}{
    $path_{ij}$ = \edgeCost{$i$,$j$};\
  }
}

\For{$k=1:n$}{
  \For{$i=1:n$}{
    \For{$j=1:n$}{
      $path_{ij}$ = \minimum{$path_{ij}$, $path_{ik}+path_{kj}$};\
    }
  }
}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Another possible solution was proposed by Batagelj~\cite{Batagelj-1994} and 
involved modifying Floyd/Warshall's algorithm~\cite{floyd62,warshall62} for 
all pairs shortest paths.
%
Briefly, Floyd/Warshall's algorithm computes all pairs shortest paths given an
adjacency matrix representation of a weighted graph in $O(n^3)$ time; the
algorithm is outlined in
Algorithm~\ref{alg:floyd_warshall}.~\footnote{Floyd/Warshall's does not handle
negative cycles, although it can be used to detect such cycles in a graph.}
%
In his solution, Batagelj avoided unnecessary work by using \textit{geodetic
semiring}, an instance of the closed semiring generalization for shortest
paths~\cite{Aho-1974}.
%
We briefly sketch the solution here for the sample graph shown in
Figure~\ref{fig:sample}.
%
First, from the adjacency matrix $A$, we create a new relation matrix
$R=[(d_{u,v}, n_{u,v})]$, where $d$ is the geodesic between $(u,v)\in{}E$ and
$n_{u,v}$ is the number of geodesics between $u$ and $v$; initially
%
\begin{equation}
(d,n)_{u,v} = \left\{ \begin{array}{rcl} 
  (1,1) & \mbox{for} & (u,v)\in{} A \\
  (\infty{},0) & \mbox{for} & (u,v)\notin{} A \\
\end{array}\right.
\label{eq:dnuv}
\end{equation}

Using this transformation on the adjacency matrix shown in
Figure~\ref{fig:sample}, we get the following matrix:
%
\begin{displaymath}
R = \left[ \begin{array}{cccc}
  (\infty{},0) & (1,1) & (\infty{},0) & (\infty{},0) \\
  (\infty{},0) & (\infty{},0) & (1,1) & (1,1) \\
  (1,1) & (\infty{},0) & (\infty{},0) & (\infty{},0) \\
  (1,1) & (\infty{},0) & (\infty{},0) & (\infty{},0) \\
\end{array} \right]
\end{displaymath}
%
From this matrix $R$, we compute the geodesic closure $R^+$ using the semiring
$(R, \oplus{}, \odot{}, (\infty,0), (0,1))$, where:

\begin{equation}
(a,i)\oplus{}(b,j) = (min (a,b), \left\{ \begin{array}{rr} 
  i & a<b \\
  i+j & a=b \\
  j & a>b \\
\end{array}\right.)
\label{eq:oplus}
\end{equation}

\begin{equation}
(a,i)\odot{}(b,j) = (a+b, i\times{}j)
\label{eq:odot}
\end{equation}
%
The key intuition here is that knowing the distance ($d_{u,v}$), and the number
of shortest paths ($n_{u,v}$), it is easy to compute the number of shortest
paths between $(u,v)$ using the following equation:
%
\begin{equation}
n_{u,v}(t) = \left\{ \begin{array}{rr} 
  n_{u,t}\times{}n_{t,v} & d_{u,t}+d_{t,v} \\
  0 & otherwise \\
\end{array}\right.
\label{eq:bcoft}
\end{equation}
%
For a complete proof of $(R, \oplus{}, \odot{}, (\infty,0), (0,1))$ being a 
geodesic semiring, please refer to Batagelj~\cite{Batagelj-1994}.~\footnote{
$(R, \oplus{}, \odot{}, (\infty,0), (0,1))$ is a semiring \textit{iff} all 
distances ($d_{i,j}$) are positive.}
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Modified Batagelj's Algorithm
\begin{algorithm}
\SetKwFunction{minimum}{min}

\caption{computeGeodeticSemiRing}
\label{alg:batagelj}
\KwIn{$R$: Relational matrix of graph $G(V,E)$}

\For{$k=1:n$}{
  \For{$i=1:n$}{
    \For{$j=1:n$}{
      $distance$ = \minimum{$\infty{},d_{i,k}+d_{k,j}$}\;
      \If{$d_{i,j}\ge{}distance$}{
        $count = n_{i,k}\times{}n_{k,j}$\;
        \If{$d_{i,j}==distance$}{
          $n_{i,j} = n_{i,j} + count$\;
        }\Else{
          $n_{i,j} = count$\;
          $d_{i,j} = distance$\;
        }
      }
    }
  }
}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
The modified Floyd-Warshall's algorithm that computes $R^+$ is given in
Algorithm~\ref{alg:batagelj}; after application of this algorithm to the 
matrix $R$, we get:
%
\begin{displaymath}
R^+ = \left[ \begin{array}{cccc}
  (3,2) & (1,1) & (2,1) & (2,1) \\
  (2,2) & (3,2) & (1,1) & (1,1) \\
  (1,1) & (2,1) & (3,1) & (3,1) \\
  (1,1) & (2,1) & (3,1) & (3,1) \\
\end{array} \right]
\end{displaymath}
%
From $R^+$, it is simple to compute the BC of any node using
equations~\ref{eq:bcoft} and~\ref{eq:bc}.
%
For example, BC($d$) (and BC($c$)) in $G$ (from Figure~\ref{fig:sample}) is
$1/2$ as it lies on the only shortest path between $b$ and $a$.
%
In this method, computing $R^+$ takes $O(n^3)$ operations, and then computing
BC of any node requires considering all pair-wise entries, which takes a
further $O(n^2)$ computations.
%
Therefore, the overall computational complexity of the algorithm is $O(n^3)$;
space complexity is $O(n^2)$.
%
This is too steep a price to pay in real-world graphs, which are sparse and 
large.

%
% Brandes' approach
%
\subsection{Graph Traversal Approach}
\label{subsec:graph_traversal}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Brandes' Algorithm
\begin{algorithm}
\SetKwFunction{enqueue}{enqueue}
\SetKwFunction{dequeue}{dequeue}
\SetKwFunction{push}{push}
\SetKwFunction{pop}{pop}
\SetKwFunction{new}{new}
\SetKwFunction{append}{append}
\SetKwFunction{neighbor}{neighbor}

\caption{brandesBC}
\label{alg:brandes}
\KwIn{$G(V,E)$: A graph}
$BC_v = 0$, $v\in{}V$\;

\For{$s\in{}V$}{
  $S \leftarrow{}$ \new{$stack$}\;
  $P_w \leftarrow{} \emptyset{}, w\in{}V$\;
  $\sigma{}_w \leftarrow{} 0, w\in{}V; \sigma{}_s \leftarrow{} 1$\;
  $D_w \leftarrow{} -1, w\in{}V; D_s \leftarrow{} 0$\;
  $Q \leftarrow{}$ \new{$queue$}\;
  \enqueue{$Q$,$s$}\;

  \While{$Q\ne\emptyset{}$}{
    $v \leftarrow{}$ \dequeue{$Q$}\;
    \push{$S$,$v$}\;
    \ForEach{$w\leftarrow{}$\neighbor{$v$}}{
      \If{$D_w<0$}{
        \enqueue{$Q$,$w$}\;
        $D_w = D_v + 1$\;
      }
      \If{$D_w=D_v+1$}{
        \append{$P_w$,$v$}\;
        $\sigma{}_w = \sigma{}_w + \sigma{}_v$\;
      }
    }
  }
  $\delta{}_v \leftarrow{} 0, v\in{}V$\;
  \While{$S\ne\emptyset{}$}{
    $w \leftarrow{}$ \pop{$S$}\;
    \ForEach{$v\in{}P_w$}{
      $\delta{}_v\leftarrow{}\delta{}_v+\frac{\sigma{}_v}{\sigma{}_w}\times{}(1+\delta{}_w)$\;
    }
    \If{$w\ne{}s$}{
      $BC_w \leftarrow{} BC_w + \delta{}_w$\;
    }
  }
}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
The key to computing efficiently is to exploit the sparsity of the graph 
structure (as opposed to its adjacency matrix).
%
This was the central theme on which Brandes~\cite{brandes01:_mathsoc} algorithm
is based.
%
Brandes recognized the recursive nature of BC computations that were exploited
in equation~\ref{eq:bcoft}.
%
For unweighted graphs, the basic idea is to perform breadth-first searches
(BFS) from all nodes.
%
At each step, the closest set of vertices are added, and during this step, 
cumulative betweenness scores for all vertices are computed by using the 
predecessor relationship.
%
Formally, let us define the \textit{predecessors} of a vertex $v$ on shortest
paths from $s$ to be
%
\begin{equation}
P_v(s)=\{u\in{}V:{u,v}\in{}E,d_G(s,v)=d_G(s,u)+(u,v)\}
\end{equation}
%
Where $d_G(s,v)$ is the shortest path from $s$ to $v$.
%
Now, the number of shortest paths from $s$ to $v$ ($\sigma{}_{sv}$) is exactly
$1$ more than the sum of the number of shortest paths from $s$ to each vertex
$u\in{}P_v(s)$. 
%
\begin{equation}
\sigma{}_{sv} = 1 + \sum_{u\in{}P_v(s)}\sigma{}_{su}
\end{equation}
%
Therefore, in $O(m)$ time, we can compute all the shortest paths and number of
shortest paths from a vertex $s$ to every other vertex; that is, in $O(mn)$,
we can compute all pairs and number of shortest paths.
%
Furthermore, this solution only requires $O(m+n)$ space.
%
The only thing left to do is to determine the contributions to the BC of each
vertex from every other vertex.
%
This information is already embedded in $P_v(s)$; there are
$\lvert{}P_v(s)\rvert{}$ distinct predecessors in the shortest paths from $s$
to $v$, and the number of shortest paths that each predecessor $P_{u,s}(i)$
is on is $\sigma{}_{su}(i)$. 
%
Now, using this information, we can compute the contribution of $(s,v)$ to 
the BC scores of each of the predecessors in $P_v(s)$.
%
\begin{equation}
BC(u) = BC(u) + \frac{\sigma{}_{su}}{\sigma{}_{sv}}, u\in{}P_v(s)
\end{equation}
%
In actual computation, the total number of shortest paths that go from $s$ to 
any vertex $v\in{}V$ through vertex $t\ne{}s,v$ is accumulated for every vertex
$s,v,t$ and finally, the BC scores are computed.
%
This final algorithm is given in Algorithm~\ref{alg:brandes}.

%
Let us consider the execution of Brandes's algorithm~\ref{alg:brandes} on our
sample graph $G$ from Figure~\ref{fig:sample}; let the start vertex be $a$.

\begin{align*}
Initialize\ BCs\ (line\ 1)\\
BC_a\leftarrow{}BC_b\leftarrow{}BC_c\leftarrow{}BC_d\leftarrow{}0\\
Initialize\ for\ BFS\ from\ a\ (lines\ 3\ to\ 8) \\ 
Q \leftarrow{} a, S \leftarrow{} \emptyset{} \\
P_a\leftarrow{}P_b\leftarrow{}P_c\leftarrow{}P_d\leftarrow{}\emptyset{}\\
\sigma{}_b\leftarrow{}\sigma{}_c\leftarrow{}\sigma{}_d\leftarrow{}0, \sigma{}_a\leftarrow{}1\\
D_b\leftarrow{}D_c\leftarrow{}D_d\leftarrow{}-1, D_a\leftarrow{}0
\end{align*}

\begin{align*}
BFS\ from\ a\ (lines\ 9\ to\ 18)\\
Q \leftarrow{} \emptyset{}, S \leftarrow{} (c,d,b,a) \\
P_a\leftarrow{}\emptyset{},P_b\leftarrow{}a, P_c\leftarrow{}b,P_d\leftarrow{}b\\
\sigma{}_b\leftarrow{}\sigma{}_c\leftarrow{}\sigma{}_d\leftarrow{}\sigma{}_a\leftarrow{}1\\
D_a\leftarrow{}0,D_b\leftarrow{}1,D_c\leftarrow{}2,D_d\leftarrow{}2
\end{align*}

\begin{align*}
Compute\ BC\ contributions\ (lines\ 19\ to\ 25)\\
S\leftarrow{}\emptyset{}\\
\delta{}_a\leftarrow{}1,\delta{}_b\leftarrow{}2,\delta{}_c\leftarrow{}\delta{}_d\leftarrow{}0\\
BC_a\leftarrow{}BC_c\leftarrow{}BC_d\leftarrow{}0,BC_b\leftarrow{}2
\end{align*}
%
This change in the $BC_b$ denotes that $d$ is on \textit{all} shortest paths
from $a$ to $c$ and $d$.
%
Similarly, when shortest paths from $b$ are computed, the BC's are changed 
to:
%
\begin{align*}
BC_a\leftarrow{}0,BC_b\leftarrow{}2,BC_c\leftarrow{}BC_d\leftarrow{}1/2
\end{align*}
%
These changes in $BC_c$ and $BC_d$ indicate that there are two shortest paths 
from $b$ to $a$, one each going through $c$ and $d$ respectively.
%
Similarly, when shortest paths from $c$ are computed, the BC's are changed 
to:
%
\begin{align*}
BC_a\leftarrow{}2,BC_b\leftarrow{}3,BC_c\leftarrow{}BC_d\leftarrow{}1/2
\end{align*}
%
These changes in $BC_a$ and $BC_b$ are because $a$ lies on two shortest paths 
from $c$ (to $b$ and $d$) and $b$ on the shortest path from $c$ to $d$.
%
Finally, when shortest paths from $d$ are computed, BC's are changed to:
%
\begin{align*}
BC_a\leftarrow{}4,BC_b\leftarrow{}4,BC_c\leftarrow{}BC_d\leftarrow{}1/2
\end{align*}
%
These changes in $BC_a$ and $BC_b$ are because $a$ lies on two shortest paths 
from $d$ (to $b$ and $c$) and $b$ on the shortest path from $d$ to $c$.

%
% Discuss some parallelization issues
%
\subsubsection{Parallelization}
%
\todoanju{Add information about parallelization.}

%
% Vector formulation of the problem
%
\subsection{Hybrid Formulation}
\label{subsec:hybrid}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Hybrid Algorithm
\begin{algorithm}
\SetKwFunction{minimum}{min}
\SetKwFunction{matrixy}{matrix}
\SetKwFunction{nnzExists}{nnzExists}
\SetKwFunction{matMult}{matMult}
\SetKwFunction{eltWiseAdd}{eltWiseAdd}
\SetKwFunction{eltWiseMult}{eltWiseMult}
\SetKwFunction{eltWiseAssign}{eltWiseAssign}
\SetKwFunction{eltWiseInvert}{eltWiseInvert}
\SetKwFunction{eltWiseNot}{not}
\SetKwFunction{extractRows}{extractRows}

\caption{hybridBC}
\label{alg:hybrid}
\KwIn{$A$: Adjacency matrix of an unweighted graph $G$}
\KwIn{$n$: Dimension of $A$ ($n\times{}n$)}
\KwIn{$nVerts$: Number of BFS' to perform at once}
\tcp{We assume that nVerts is divisible by $n$}
$nPasses = \frac{nVerts}{n}$\;
$BC\leftarrow{}$\matrixy{$1$,$n$,$0$}\;

\ForEach{$p\in{}(1:nPasses)$}{
  $BFS \leftarrow{} \emptyset{}$\;
  $batch = ((p-1)\times{}nVerts+1):$\minimum{$p\times{}nVerts,N$}\;
  $nsp\leftarrow{}$\matrixy{$nVerts$,$n$, $0$}\;
  \ForEach{$row\in{}(1:nVerts)$}{$nsp(row,batch(row))\leftarrow{}1$\;}
  $depth\leftarrow{}0$\;
  \eltWiseAssign{$fringe$,\extractRows{$A$,$batch$}}\;

  \tcp{BFS search for all vertices in current batch}
  \While{\nnzExists{$fringe$}}{
    $depth\leftarrow{}depth+1$\;
    $nsp\leftarrow{}$\eltWiseAdd{$nsp$,$fringe$}\;
    $BFS(depth)\leftarrow{}fringe$\;
    $fringe\leftarrow{}$\matMult{$fringe$,$A$}\;
    \tcp{Reset entries for already visited vertices}
    $fringe\leftarrow{}$\eltWiseMult{$fringe$,\eltWiseNot{$nsp$}}\;
  }

  \tcp{Pre-compute BC updates for all but source vertices}
  $BC_{updt}\leftarrow{}$\matrixy{$nVerts$,$n$,$1$}\;
  $nsp^{inv}\leftarrow{}$\eltWiseInvert{$nsp$}\;

  \tcp{Compute BC updates for all but source vertices}
  \For{$d\in{}(depth,depth-1,...,2)$}{
    \tcp{Compute child weights}
    $weights_1\leftarrow{}$\eltWiseMult{$BFS(d)$,$nsp^{inv}$}\;
    $weights\leftarrow{}$\eltWiseMult{$weights_1$,$BC_{updt}$}\;
    \tcp{Apply child weights}
    $temp_1\leftarrow{}$\matMult{$A$,$weights^T$}$^T$\;
    \tcp{Sum them up over parents}
    $temp_2\leftarrow{}$\eltWiseMult{$BFS(d-1)$,$nsp$}\;
    \tcp{Apply weights based on parents values}
    $temp_3\leftarrow{}$\eltWiseMult{$temp_1$,$temp_2$}\;
    $BC_{updt}\leftarrow{}$\eltWiseAdd{$BC_{updt}$,$temp_3$}\;
  }

  \tcp{Update BC scores from each source vertex's BFS}
  \For{$row\in{}(1:nVerts)$}{
    $BC\leftarrow{}$\eltWiseAdd{$BC$,$BC_{updt}(row,:)$}\;
  }
}
\tcp{Subtract additional values added by precomputation}
$BC\leftarrow{}$\eltWiseAdd{$BC$,\matrixy{$1$,$n$,$-nPasses$}}\;
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Both from a computational and spatial standpoint, there are several
inefficiencies in Algorithm~\ref{alg:brandes}.
%
First, notice that the queue $Q$ and the stack $S$ can be combined into one
array structure that when accessed from one end acts as a queue, and from the
other end, acts as a stack.
%
Second, the distance array $D$ is redundant in a BFS computation as
\textit{all} the shortest paths to a node must be reached during the same
\textit{BFS fringe's} expansion.
%
Any paths that reach a node during a later fringe are \textit{not} shortest
paths; hence, we can replace $D$ with a bit-array with one bit per node to
denote that its shortest path was reached during a particular fringe
expansion.
%
Third, consider the set of predecessors $P_v$ of a particular vertex $v$; since
$G$ is unweighted, these predecessors must have themselves been discovered 
during the previous fringe expansion.
%
That is, to compute the set of predecessors for a particular vertex $v$, we can
look at all its incoming \textit{discovered} edges; we need not store $P$ 
explicitly.
%
These three optimizations decrease the amount of space needed to execute
Brandes' algorithm by a significant constant factor.
%
The final optimization tries to perform BFS fringe expansions in terms of
sparse matrix multiplication with the \textit{fringe} vector (BLAS-2 kernel);
this allows exploration of one full fringe in one operation rather than
looping over all the neighbors of a particular node (lines $12$ to $18$ in
Algorithm~\ref{alg:brandes}).
%
The initial fringe vector contains just one entry, which corresponds to the 
start/source vertex.
%
Furthermore, shortest paths from multiple sources can be determined together by
replacing the sparse matrix vector product operation with a sparse matrix
matrix operation (BLAS-3 kernel); this is a standard trick in linear algebra
called \textit{blocking}.
%
However, note that the space requirements increase linearly with the block 
size.
%
After performing all these optimizations, we end up with
algorithm~\ref{alg:hybrid}, which is given as the sample MATLAB implementation
for the SSCA benchmarks~\cite{ssca_matlab}.
%
For example, in our sample graph $G$ from Figure~\ref{fig:sample}, the first 
fringe expansion starting out from vertices $a$ and $c$ can be expressed as
follows:

%
\begin{minipage}{0.18\textwidth}
\begin{center}
\begin{displaymath}
\left[ \begin{array}{cccc}
  a^T & b^T & c^T & d^T \\
  0 & 0 & 1 & 1 \\
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
\end{array} \right]
\end{displaymath}
\end{center}
\end{minipage}
\hspace{5pt}
\begin{minipage}{0.12\textwidth}
\begin{center}
\begin{displaymath}
\left[ \begin{array}{cc}
   a^T & c^T \\
   0 & 1 \\ 
   1 & 0 \\ 
   0 & 0 \\ 
   0 & 0 \\
\end{array} \right]
\end{displaymath}
\end{center}
\end{minipage}
\begin{minipage}{0.12\textwidth}
\begin{center}
\begin{displaymath}
 = \left[ \begin{array}{cc}
  b^T & a^T\\
  0 & 0\\
  0 & 1\\ 
  1 & 0\\
  1 & 0\\
\end{array} \right]
\end{displaymath}
\end{center}
\end{minipage}
\\ % Making sure we start on a new line. Minipage is awful!

%
For formatting purposes, we have shown right multiplication by the starting
vectors, which requires transposing the original adjacency matrix.
%
As can be seen, by starting out with BFS expansion from $a$ and $c$, we end 
up with the new fringe $b$ for $a$, and $a$ for $c$, respectively.
%
In fact, since this is always the case, we can forgo this computation and
simply consider the first fringe to be the starting vertex's adjacency row.
%
Please note that in an actual, high performance implementation, all matrices
are sparsely represented to save space.
%
Let us now work through Algorithm~\ref{alg:hybrid} for the sample graph $G$ 
from Figure~\ref{fig:sample}; for simplicity, we process only one vertex at a
time.
%
As before, let us start processing from vertex $a$.
%
\begin{align*}
(Initialization:\ lines\ 1\ to\ 10) \\
nPasses\leftarrow{}4,
BC\leftarrow{}\left[\begin{array}{cccc}0 & 0 & 0 & 0 \\\end{array} \right]\\
BFS\leftarrow{}\emptyset{}, batch\leftarrow{}1:1, depth\leftarrow{}0\\
nsp\leftarrow{}\left[\begin{array}{cccc}1 & 0 & 0 & 0 \\\end{array} \right],
fringe\leftarrow{}\left[\begin{array}{cccc}0 & 1 & 0 & 0 \\\end{array} \right]
\end{align*}
%
Here, we have initialized $nsp(a)$ to be $1$ and set the fringe to be $a$'s
adjacency ($b$).
%
\begin{align*}
(BFS\ search\ from\ a:\ lines\ 11\ to\ 16,\ first\ pass) \\
depth\leftarrow{}1,
BFS(1)\leftarrow{}\left[\begin{array}{cccc}0 & 1 & 0 & 0 \\\end{array} \right]\\
nsp\leftarrow{}\left[\begin{array}{cccc}1 & 1 & 0 & 0 \\\end{array} \right],
fringe\leftarrow{}\left[\begin{array}{cccc}0 & 0 & 1 & 1 \\\end{array} \right]
\end{align*}
%
At the end of the first pass, we discover vertex $b$; $nsp$ now indicates that
we have found one shortest path $(a,b)$.
%
\begin{align*}
(BFS\ search\ from\ a:\ lines\ 11\ to\ 16,\ second\ pass) \\
depth\leftarrow{}2,
BFS(2)\leftarrow{}\left[\begin{array}{cccc}0 & 0 & 1 & 1 \\\end{array} \right]\\
nsp\leftarrow{}\left[\begin{array}{cccc}1 & 1 & 1 & 1 \\\end{array} \right],
fringe\leftarrow{}\left[\begin{array}{cccc}0 & 0 & 0 & 0 \\\end{array} \right]
\end{align*}
%
At the end of the second pass, we have discovered $c$ and $d$; at this point
there are no more undiscovered vertices and the BFS search from $a$ ceases.
%
$nsp$ indicates that we discovered 3 shortest paths, $(a,b)$, $(a,c)$, and
$(a,d)$; the path $(a,a)$ is not useful in computing BC scores when the start
vertex is $a$ itself.
%
Now, we move on to updating the BC scores for all vertices based on our
exploration from vertex $a$.
%
\begin{align*}
(Initialize\ for\ BC\ updates:\ lines\ 17\ and \ 18) \\
BC_{updt}\leftarrow{}\left[\begin{array}{cccc}1 & 1 & 1 & 1 \\\end{array} \right],
nsp^{inv}\leftarrow{}\left[\begin{array}{cccc}1 & 1 & 1 & 1 \\\end{array} \right]
\end{align*}
%
As in Brandes~\ref{alg:brandes}), we move back from the final fringe to the 
first; in our example, this is $depth=2$.
%
\begin{align*}
(Compute\ and\ update\ child\ weights:\ lines\ 20\ to\ 25)\\
weights_1\leftarrow{}\left[\begin{array}{cccc}0 & 0 & 1 & 1 \\\end{array} \right],
weights\leftarrow{}\left[\begin{array}{cccc}0 & 0 & 1 & 1 \\\end{array} \right]\\
temp_1\leftarrow{}\left[\begin{array}{cccc}0 & 2 & 0 & 0 \\\end{array} \right],
temp_2\leftarrow{}\left[\begin{array}{cccc}0 & 1 & 0 & 0 \\\end{array} \right]\\
temp_3\leftarrow{}\left[\begin{array}{cccc}0 & 2 & 0 & 0 \\\end{array} \right],
BC_{updt}\leftarrow{}\left[\begin{array}{cccc}1 & 3 & 1 & 1 \\\end{array} \right]
\end{align*}
%
Next, we update the BC scores for each vertex with $BC_{updt}$.
%
\begin{align*}
(Update\ the\ BC\ scores:\ lines\ 26\ and\ 27)\\
BC\leftarrow{}\left[\begin{array}{cccc}1 & 3 & 1 & 1 \\\end{array} \right]
\end{align*}
%
Notice that in Algorithm~\ref{alg:hybrid}, we pre-compute $BC_{updt}$ to be $1$
for all vertices; hence, even though at the end of the BFS exploration for 
vertex $a$, BC scores of all vertices show a value $>0$, the only true change
is that of vertex $b$, which lies on the path from $a$ to both $c$ and $d$.
%
For brevity, we will now simply list the values of BC after the end of BFS 
exploration for each of the remaining vertices:
%
\begin{align*}
After\ BFS\ from\ b,\ 
BC\leftarrow{}\left[\begin{array}{cccc}2 & 4 & 2.5 & 2.5 \\\end{array} \right]\\
After\ BFS\ from\ c,\ 
BC\leftarrow{}\left[\begin{array}{cccc}5 & 6 & 3.5 & 3.5 \\\end{array} \right]\\
After\ BFS\ from\ d,\ 
BC\leftarrow{}\left[\begin{array}{cccc}8 & 8 & 4.5 & 4.5 \\\end{array} \right]
\end{align*}
%
The final trick to get the accurate BC scores is to subtract the number of
passes as each pass adds $1$ to each vertex's BC score
(Algorithm~\ref{alg:hybrid}, line $28$).
%
Note that without initializing $BC_{updt}$ with ones, line $24$ in
Algorithm~\ref{alg:hybrid} would not have computed $weights$ accurately;
therefore, it is critical to initialize $BC_{updt}$ with ones.
%
\begin{align*}
Subtract\ 4\ from\ BC\ (line\ 28):\ 
BC\leftarrow{}\left[\begin{array}{cccc}4 & 4 & 0.5 & 0.5 \\\end{array} \right]
\end{align*}
%
This gives the final and accurate BC scores for all the vertices.
%

%
% Discuss some parallelization issues
%
\subsubsection{Parallelization}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Variable & Shape & Space Requirement & Representation & Sparsity (RMAT-specific) \\ \hline
$BC$ & Row-vector & $n$ & \code{double}[] & Dense \\ \hline
$A$ & Square-matrix & $O(m)$ & CSR/CSC(\code{<int,int>}) & Hyper-sparse \\ \hline
$fringe$ & Rectangular-matrix & $O(nVerts\times{}m)$ & CSR/CSC(\code{<int,int>}) & Sparse \\ \hline
$BFS$ & Rectangular-matrix & $O(m)$ & CSR/CSC\code{<int,int>}[] & Sparse \\ \hline
$nsp$ & Rectangular-matrix & $nVerts\times{}n$ & \code{int}[] & Dense \\ \hline
$nsp^{inv}$ & Rectangular-matrix & Computed on the fly & \code{int}[] & Dense \\ \hline
$BC_{updt}$ & Rectangular-matrix & $nVerts\times{}n$ & \code{double}[] & Dense \\ \hline
$weights_1$ & Rectangular-matrix & Computed on the fly & --- & --- \\ \hline
$weights$ & Rectangular-matrix & $O(nVerts\times{}n)$ & \code{double}[] & Sparse to Hyper-sparse \\ \hline
$temp_1,temp_2,temp_3$ & Rectangular-matrix & Computed on the fly & --- & Sparse \\ \hline
\end{tabular}
\caption{Table depicting the space requirements of each variable in
Algorithm~\ref{alg:hybrid}. Sparsity is specific to RMAT graphs (see
Section~\ref{sec:rmat}. Hyper-sparse means that there are less non-zeros than 
the number of columns/rows. CSR refers to the compressed sparse row format and 
CSC to the compressed sparse column format.}
\label{tbl:hybrid}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}
\centering
\begin{tabular}{|c|c|c|} \hline
Kernel & Description & Type \\ \hline
\functhree{matrix}{m}{n}{init} & Create an $(m\times{}n)$ matrix initialized to \textit{init} & local \\ \hline
\functwo{extractRows}{A}{rowList} & Extract a submatrix formed by the rows in $rowList$ & global \\ \hline
\functwo{eltWiseAssign}{A}{B} & Assign the matrix $A$ to the matrix $B$ & local \\ \hline
\functwo{eltWiseAdd}{A}{B} & Element-wise addition of two (sparse) matrices & local \\ \hline
\functwo{eltWiseMult}{A}{B} & Element-wise multiplication of two (sparse) matrices & local \\ \hline
\funcone{nnzExists}{A} & Check for the presence of a non-zero in the matrix $A$ & global \\ \hline
\functwo{matMult}{A}{B} & Multiply two (sparse) matrices & global \\ \hline
\end{tabular}
\caption{A Table depicting the different matrix kernels in Algorithm~\ref{alg:hybrid}
and their features.}
\label{tbl:complexity}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Algorithm~\ref{alg:hybrid} is expressed mostly in terms of computations on 
sparse matrices with sparse or dense vectors, which makes it especially 
suitable for parallelization.
%
In this segment, we discuss some of the factors that influence parallelization
of Algorithm~\ref{alg:hybrid}.
%
First, let us recap that we are interested mostly in social network analysis
and the real-world graphs in this particular domain are similar to RMAT
graphs (see Section~\ref{sec:rmat}).
%
Briefly, these graphs follow the power-law, have low average connectivity and
low diameter.
%
First, let us recap that Algorithm~\ref{alg:hybrid} implements BFS as a
matrix-matrix multiplication (line $15$).
%

%
% Space analysis
%
\paragraph{Space Analysis}
Table~\ref{tbl:hybrid} lists the space requirements of each of the variables
used in Algorithm~\ref{alg:hybrid}; we will now analyze each variable
separately.
% BC and BC_updt
BC is computed for each vertex $v\in{}V$, and hence, the row-vector $BC$ is
represented as a dense array of \code{double}'s; for simplicity, we assume that
this is the case for $BC_{updt}$ as well; 
%
% Dropping the value from A, fringe and BFS since these are unweighted.
%
The adjacency matrix of the graph $A$ for RMAT graphs is sparse; therefore, 
it can be represented in either CSR, CSC, DCSR, or DCSC~\cite{buluc-2010} 
formats.
%
However, as we are exclusively dealing with \textit{unweighted} graphs in this 
paper, we can drop the edge weights completely; this saves us $m$ entries in 
$A$ alone.
%
For similar reasons, we can drop the edge weights from $fringe$ and $BFS$.
%
Notice that $\sum{}_{i=1}^{diameter(G)}BFS(i)=m$; that is, the total number of 
non-zero entries in $BFS$ is exactly the same as the number of edges in $G$.
%
Therefore, by not storing edge weights, we save an additional $m$ entries for a
total saving of $2m$ (potentially integers).
%
% nsp has to be stored dense since in a power-law graph, most people are 
% connected and hence, there is no that much saving happening if you think
% about it.
%
A stark contrast is detected when it comes to the variable $nsp$, an array that
stores the number of shortest paths from each source vertex to all other
vertices.
%
In an RMAT graph, there exists a connection between any two vertices with a 
high probability; that is, $nsp$ will be dense. 
%
Therefore, storing $nsp$ in regular dense format, which requires
$nVerts\times{}n$ \code{integer}s is beneficial over storing them in compressed
formats that require explicit representation of the row and the column numbers,
thereby saving us This saves us $\approx{}2n$ space.

\paragraph{Kernel Operations}
Table~\ref{tbl:complexity} depicts the different kernels used in
Algorithm~\ref{alg:hybrid}; As can be seen, other than \func{extractRows},
\func{nnzExists}, and \func{matMult}, all the other operations are completely
\textit{local}.
%
That is, these operations do not require and communication in the case of a 
distributed-memory implementation.
%
One way to parallelize on both shared- and distributed-memory machines is to 
parallelize the operations that are depicted in Table~\ref{tbl:complexity}; 
Buluc~\cite{buluc-2010} parallelized these operations for distributed memory 
and for shared memory~\cite{buluc-2011}, but not for a multi-level architecture
consisting of both shared- and disributed-memory machines.

\subsubsection{Shortcomings}
%
Algorithm~\ref{alg:hybrid} works \textit{only} for unweighted graphs.
%
When the edges are weighted, the ``shortest path'' from one node to another 
is determined not only by the number of hops that need to be taken, but also
the cost/weight of each hop.
%
Consequently, many changes are needed to algorithm~\ref{alg:hybrid} in order 
to make it work for weighted graphs.
%
For example, line $16$, which ensures that each vertex is visited only once
is incorrect for weighted graphs as a path with greater number of hops, but 
lower cost can still be found.
%
In essence, we degenerate to Floyd-Warshall's $O(n^3)$ algorithm; therefore, 
Algorithm~\ref{alg:hybrid} is unsuitable for weighted graphs.
%
However, for benchmarking purposes, Algorithm~\ref{alg:hybrid} is sufficient;
for example, kernel 4 of HPC Graph Analysis Benchmarks~\cite{ssca_matlab} only
require computation of unweighted betweenness centrality.

%
% Discuss Approximate solutions 
%
\subsection{Approximating BC}
\label{subsec:approximate_bc}
%
Real-world graph sizes are ever increasing and BC computations are expensive;
consequently, approximate measures for BC have been explored.
%
There are three prominent works in this regard.
%
First, Eppstein and Wang~\cite{Eppstein-2004} describe a randomized
approximation algorithm for estimation of \textit{closeness centrality} in
weighted graphs.~\footnote{Closeness centrality (CC) of a vertex is a global
metric that measures the the distance of a vertex to all other vertices in the
graph.
%
Formally $CC_v=\frac{1}{\sum_{u\in{}V}{d(v,u)}}$.}
%
Their method (RAND) randomly chooses $k$ vertices one by one and use these 
vertices as start vertices for the single source shortest paths; that is, 
instead of solving all sources shortest paths problem, $k$ sources shortest
paths problem is solved.
%
They further proved that for $k=\Theta{(\frac{log(n)}{\epsilon{}^2})}$, their
algorithm approximates closeness centrality with an additive error of
$\epsilon{}\Delta{}_{G}$, where $\Delta{}_G$ is the diameter of the graph, and
$\epsilon{}$ is a small constant.
%
Brandes and Pich~\cite{brandes-2007}, after experimenting with various 
deterministic strategies for selecting source vertices for approximating BC,
concluded that a random sampling strategy was superior.
%
Bader et al.~\cite{Bader07:ApproxBC} presented an adaptive sampling technique
that approximates the BC of \textit{a given vertex}.
%
They conclude that for $0<\epsilon{}<\frac{1}{2}$, if the centrality of a vertex
$v$ is $\frac{n^2}{t}$ for some constant factor $t\ge{}1$, then with a 
probability $\ge{}(1-2\epsilon{})$, its centrality can be estimated within a 
factor of $\frac{1}{\epsilon{}}$ by using only $\epsilon{}t$ samples of 
source vertices.
