\section{Background}
\label{sec:background}
% Define BC
Betweenness Centrality (BC) is a graph theoretic metric of a node's importance
in a graph.~\footnote{In this paper, we focus exclusively on the unweighted,
directed graphs.}
%
Informally, a node's BC gives an indication of the ratio of the total number 
of shortest paths between all other nodes taken pair-wise.
%
That is, a node with a high centrality score can reach other nodes with 
relatively fewer hops than another node with a lower centrality score.
%
More formally, let $G(V,E)$ be a graph with the vertex set $V$ and edge set
$E$; let the number of vertices ($\lvert{}V\rvert{})$ be $n$ and the number of 
edges ($\lvert{}E\rvert{}$) be $m$.
%
The BC of a node (vertex) $v\in{V})$ is given by the equation:
%
\begin{equation}
BC_{v} = \sum_{s\ne{}v\ne{}t\in{}V}{\frac{\sigma{}_{st}(v)}{\sigma{}_{st}}}
\label{eq:bc}
\end{equation}
%
Where $\sigma{}_{st}$ is the number of shortest paths from node $s$ to node $t$ 
and $\sigma{}_{st}(v)$ is the number of those shortest paths that go through 
node $v$.
% Get into the simple means of computing BC
There are multiple ways of computing the BC scores of nodes in a graph; in the
following subsections, we highlight a few of these techniques.
%
Please note that regardless of the approach that is used in computing BC, the 
central kernel is enumerating all the shortest paths.

\subsection{Algebraic Approach}
\label{subsec:algebraic}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\begin{minipage}{0.20\textwidth}
\begin{center}
\begin{displaymath}
\xymatrix{
A \ar[r] & D \ar[d] \\
B \ar[u] \ar[ur] \ar[r] & C \ar[ul]
}
\end{displaymath}
\end{center}
\end{minipage}
\hspace{10pt}
\begin{minipage}{0.18\textwidth}
\begin{center}
\begin{displaymath}
A = \left[ \begin{array}{cccc}
  0 & 0 & 0 & 1 \\
  1 & 0 & 1 & 1 \\
  1 & 0 & 0 & 0 \\
  0 & 0 & 1 & 0 \\
\end{array} \right]
\end{displaymath}
\end{center}
\end{minipage}
\caption{A sample directed, unweighted graph and its resulting adjacency
matrix. A $1$ in position $a_{ij}$ indicates an edge from node $i$ to node
$j$.}
\label{fig:sample}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Every graph $G(V,E)$ can be represented as an adjacency matrix $A$, where an 
entry $a_{ij}$ is marked as $1$ \textit{iff} $(i,j)\in{}E$.
%
Figure~\ref{fig:sample} shows a sample four node graph and its related 
adjacency matrix; we will be using this graph as a running example throughout
this section.
%
By inspection, we can tell that the diameter of the graph is $3$; therefore, 
all the possible paths in the matrix (including the number of paths) can be 
enumerated by simply taking the powers of the adjacency matrix $A$; in other 
words, we find the transitive closure of the graph $G$.
%
However, computing the transitive closure is an over-kill; what we want in 
order to compute BC are just the shortest paths between all pair of vertices,
not all paths of length diameter or less.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\begin{minipage}{0.2\textwidth}
\begin{center}
\begin{displaymath}
A^2 = \left[ \begin{array}{cccc}
   0 & 0 & 1 & 0 \\
   1 & 0 & 1 & 1 \\
   0 & 0 & 0 & 1 \\
   1 & 0 & 0 & 0 \\
\end{array} \right]
\end{displaymath}
\end{center}
\end{minipage}
\hspace{10pt}
\begin{minipage}{0.2\textwidth}
\begin{center}
\begin{displaymath}
A^3 = \left[ \begin{array}{cccc}
   1 & 0 & 0 & 0 \\
   1 & 0 & 1 & 1 \\
   0 & 0 & 1 & 0 \\
   0 & 0 & 0 & 1 \\
\end{array} \right]
\end{displaymath}
\end{center}
\end{minipage}
\caption{The $2^{nd}$ and $3^{rd}$ powers of the adjacency matrix, $A$, shown
in Figure~\ref{fig:sample}. $A^2$ shows the paths of path length $2$, and $A^3$
shows the paths in the sample graph of path length $3$.}
\label{fig:sample_powers}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Floyd-Warshall's Algorithm
\begin{algorithm}
\SetKwFunction{edgeCost}{edgeCost}
\SetKwFunction{minimum}{minimum}

\caption{FloydWarshall}
\label{alg:floyd_warshall}
\KwIn{$A$: Adjacency matrix of graph $G(V,E)$}

\For{$i=1:n$}{
  \For{$j=1:n$}{
    $path_{ij}$ = \edgeCost{$i$,$j$};\
  }
}

\For{$k=1:n$}{
  \For{$i=1:n$}{
    \For{$j=1:n$}{
      $path_{ij}$ = \minimum{$path_{ij}$, $path_{ik}+path_{kj}$};\
    }
  }
}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Another possible solution was proposed by Batagelj~\cite{Batagelj-1994} and 
involved modifying Floyd/Warshall's algorithm~\cite{floyd62,warshall62} for 
all pairs shortest paths.
%
Briefly, Floyd/Warshall's algorithm computes all pairs shortest paths given an
adjacency matrix representation of a weighted graph in $O(n^3)$ time; the
algorithm is outlined in
Algorithm~\ref{alg:floyd_warshall}.~\footnote{Floyd/Warshall's does not handle
negative cycles, although it can be used to detect such cycles in a graph.}
%
In his solution, Batagelj avoided unnecessary work by using \textit{geodetic
semiring}, an instance of the closed semiring generalization for shortest
paths~\cite{Aho-1974}.
%
We briefly sketch the solution here for the sample graph shown in
Figure~\ref{fig:sample}.
%
First, from the adjacency matrix $A$, we create a new relation matrix
$R=[(d_{u,v}, n_{u,v})]$, where $d$ is the geodesic between $(u,v)\in{}E$ and
$n_{u,v}$ is the number of geodesics between $u$ and $v$; initially
%
\begin{equation}
(d,n)_{u,v} = \left\{ \begin{array}{rcl} 
  (1,1) & \mbox{for} & (u,v)\in{} A \\
  (\infty{},0) & \mbox{for} & (u,v)\notin{} A \\
\end{array}\right.
\label{eq:dnuv}
\end{equation}

Using this transformation on the adjacency matrix shown in
Figure~\ref{fig:sample}, we get the following matrix:
%
\begin{displaymath}
R = \left[ \begin{array}{cccc}
  (\infty{},0) & (\infty{},0) & (\infty{},0) & (1,1) \\
  (1,1) & (\infty{},0) & (1,1) & (1,1) \\
  (1,1) & (\infty{},0) & (\infty{},0) & (\infty{},0) \\
  (\infty{},0) & (\infty{},0) & (1,1) & (\infty{},0) \\
\end{array} \right]
\end{displaymath}
%
From this matrix $R$, we compute the geodesic closure $R^+$ using the semiring
$(R, \oplus{}, \odot{}, (\infty,0), (0,1))$, where:

\begin{equation}
(a,i)\oplus{}(b,j) = (min (a,b), \left\{ \begin{array}{rr} 
  i & a<b \\
  i+j & a=b \\
  j & a>b \\
\end{array}\right.)
\label{eq:oplus}
\end{equation}

\begin{equation}
(a,i)\odot{}(b,j) = (a+b, i\times{}j)
\label{eq:odot}
\end{equation}
%
The key intuition here is that knowing the distance ($d_{u,v}$), and the number
of shortest paths ($n_{u,v}$, it is easy to compute the number of shortest
paths between $(u,v)$ using the following equation:
%
\begin{equation}
n_{u,v}(t) = \left\{ \begin{array}{rr} 
  n_{u,t}\times{}n_{t,v} & d_{u,t}+d_{t,v} \\
  0 & otherwise \\
\end{array}\right.
\label{eq:bcoft}
\end{equation}
%
For a complete proof of $(R, \oplus{}, \odot{}, (\infty,0), (0,1))$ being a 
geodesic semiring, please refer to Batagelj~\cite{Batagelj-1994}.~\footnote{
$(R, \oplus{}, \odot{}, (\infty,0), (0,1))$ is a semiring \textit{iff} all 
distances ($d_{i,j}$) are positive.}
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Modified Batagelj's Algorithm
\begin{algorithm}
\SetKwFunction{minimum}{minimum}

\caption{computeGeodeticSemiRing}
\label{alg:batagelj}
\KwIn{$R$: Relational matrix of graph $G(V,E)$}

\For{$k=1:n$}{
  \For{$i=1:n$}{
    \For{$j=1:n$}{
      $distance$ = \minimum{$\infty{},d_{i,k}+d_{k,j}$}\;
      \If{$d_{i,j}\ge{}distance$}{
        $count = n_{i,k}\times{}n_{k,j}$\;
        \If{$d_{i,j}==distance$}{
          $n_{i,j} = n_{i,j} + count$\;
        }\Else{
          $n_{i,j} = count$\;
          $d_{i,j} = distance$\;
        }
      }
    }
  }
}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
The modified Floyd-Warshall's algorithm that computes $R^+$ is given in
Algorithm~\ref{alg:batagelj}; after application of this algorithm to the 
matrix $R$, we get:
%
\begin{displaymath}
R^+ = \left[ \begin{array}{cccc}
  (3,1) & (\infty{},0) & (2,1) & (1,1) \\
  (1,1) & (\infty{},0) & (1,1) & (1,1) \\
  (1,1) & (\infty{},0) & (3,1) & (2,1) \\
  (2,1) & (\infty{},0) & (1,1) & (3,1) \\
\end{array} \right]
\end{displaymath}
%
From $R^+$, it is simple to compute the BC of any node using
equations~\ref{eq:bcoft} and~\ref{eq:bc}.
%
For example, BC($D$) in $G$ (from Figure~\ref{fig:sample}) is $1$ as it lies 
on the only shortest path between $C$ and $A$.
%
In this method, computing $R^+$ takes $O(n^3)$ operations, and then computing
BC of any node requires considering all pair-wise entries, which takes a
further $O(n^2)$ computations.
%
Therefore, the overall computational complexity of the algorithm is $O(n^3)$;
space complexity is $O(n^2)$.
%
This is too steep a price to pay in real-world graphs, which are sparse and 
large.

%
% Brandes' approach
%
\subsection{Graph Traversal Approach}
\label{subsec:graph_traversal}
The key to computing efficiently is to exploit the sparsity of the graph 
structure (as opposed to its adjacency matrix).
%
This was the central theme on which Brandes~\cite{brandes01:_mathsoc} algorithm
is based.
%
Brandes recognized the recursive nature of BC computations that were exploited
in equation~\ref{eq:bcoft}.
%
For unweighted graphs, the basic idea is to perform breadth-first searches
(BFS) from all nodes.
%
At each step, the closest set of vertices are added, and during this step, 
cumulative betweenness scores for all vertices are computed by using the 
predecessor relationship.
%
Formally, let us define the \textit{predecessors} of a vertex $v$ on shortest
paths from $s$ to be
%
\begin{equation}
P_v(s)=\{u\in{}V:{u,v}\in{}E,d_G(s,v)=d_G(s,u)+(u,v)\}
\end{equation}
%
Where $d_G(s,v)$ is the shortest path from $s$ to $v$.
%
Now, the number of shortest paths from $s$ to $v$ ($\sigma{}_{sv}$) is exactly
$1$ more than the sum of the number of shortest paths from $s$ to each vertex
$u\in{}P_v(s)$. 
%
\begin{equation}
\sigma{}_{sv} = 1 + \sum_{u\in{}P_v(s)}\sigma{}_{su}
\end{equation}
%
Therefore, in $O(m)$ time, we can compute all the shortest paths and number of
shortest paths from a vertex $s$ to every other vertex; that is, in $O(mn)$,
we can compute all pairs and number of shortest paths.
%
Furthermore, this solution only requires $O(m+n)$ space.
%
The only thing left to do is to determine the contributions to the BC of each
vertex from every other vertex.
%
This information is already embedded in $P_v(s)$; there are
$\lvert{}P_v(s)\rvert{}$ distinct predecessors in the shortest paths from $s$
to $v$, and the number of shortest paths that each predecessor $P_{u,s}(i)$
is on is $\sigma{}_{su}(i)$. 
%
Now, using this information, we can compute the contribution of $(s,v)$ to 
the BC scores of each of the predecessors in $P_v(s)$.
%
\begin{equation}
BC(u) = BC(u) + \frac{\sigma{}_{su}}{\sigma{}_{sv}}, u\in{}P_v(s)
\end{equation}
%
In actual computation, the total number of shortest paths that go from $s$ to 
any vertex $v\in{}V$ through vertex $t\ne{}s,v$ is accumulated for every vertex
$s,v,t$ and finally, the BC scores are computed.
%
This final algorithm is given in Algorithm~\ref{alg:brandes}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Brandes' Algorithm
\begin{algorithm}
\SetKwFunction{enqueue}{enqueue}
\SetKwFunction{dequeue}{dequeue}
\SetKwFunction{push}{push}
\SetKwFunction{pop}{pop}
\SetKwFunction{new}{new}
\SetKwFunction{append}{append}
\SetKwFunction{neighbor}{neighbor}

\caption{brandesBC}
\label{alg:brandes}
\KwIn{$G(V,E)$: A graph}
$BC_v = 0$, $v\in{}V$\;

\For{$s\in{}V$}{
  $S \leftarrow{}$ \new{$stack$}\;
  $P_w \leftarrow{} \emptyset{}, w\in{}V$\;
  $\sigma{}_w \leftarrow{} 0, w\in{}V; \sigma{}_s \leftarrow{} 1$\;
  $D_w \leftarrow{} -1, w\in{}V; D_s \leftarrow{} 0$\;
  $Q \leftarrow{}$ \new{$queue$}\;
  \enqueue{$Q$,$s$}\;

  \While{$Q\ne\emptyset{}$}{
    $v \leftarrow{}$ \dequeue{$Q$}\;
    \push{$S$,$v$}\;
    \ForEach{$w\leftarrow{}$\neighbor{$v$}}{
      \If{$D_w<0$}{
        \enqueue{$Q$,$w$}\;
        $D_w = D_v + 1$\;
      }
      \If{$D_w=D_v+1$}{
        \append{$P_w$,$v$}\;
        $\sigma{}_w = \sigma{}_w + \sigma{}_v$\;
      }
    }
  }
  $\delta{}_v \leftarrow{} 0, v\in{}V$\;
  \While{$S\ne\emptyset{}$}{
    $w \leftarrow{}$ \pop{$S$}\;
    \ForEach{$v\in{}P_w$}{
      $\delta{}_v\leftarrow{}\delta{}_v+\frac{\sigma{}_v}{\sigma{}_w}\times{}(1+\delta{}_w)$\;
    }
    \If{$w\ne{}s$}{
      $BC_w \leftarrow{} BC_w + \delta{}_w$\;
    }
  }
}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
