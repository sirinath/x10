\chapter{Dealing With Concurrency}\label{chap:concurrency}

In our discussion of the Monte-Carlo computation of $\pi$, we introduced four
of the five main constructs that \Xten{} uses to describe 
concurrency:
\begin{itemize}
\item {\tt at} to do something at a different place;
\item {\tt async} to start a parallel computation, or in \Xten{} terms, to
spawn an activity;
\item {\tt finish} to wait for a bunch of concurrent computations to be done;
\item {\tt atomic} to keep things from happening at
      times when they shouldn't---or if you prefer, to keep access to shared
      resources orderly.
\end{itemize}
The missing construct is {\tt when}, which is related to {\tt atomic}, 
in that it is used to have one activity wait for another one to do something.
We'll give some examples in a moment---see
section \ref{ssec:whenstmt}.

There are a few other \Xten{} constructs, and some useful concurrent data
structures, but if you know the five, the rest are easy.  For
example, the {\tt AtomicInteger} class (section \ref{ssec:efficient}) gives you a
way to work with an integer variable atomically at lower cost than
the generic {\tt atomic} statement would incur.  Bottom line: what remains
to be learned has more do with efficiency and grace than much fundamentally new.

\section{Here, There, and Everywhere}

Every bit of running \Xten{} has a {\em place} where it is running.  We've already
seen places in use in Section \ref{sec:distwork}.  In most other
parallel programming languages, each place has a unique integer id,
and that is how you work with it in the running program.
\Xten{} also provides a unique id, but as part of a larger object whose
type is {\tt x10.lang.Place}.  Places are thus objects, so
you can talk about them and compute with them as you would with any
other object.  

Why not just an integer?  One reason is that \Xten{}
assumes that different places might be running on different kinds of 
processors.
Some places may be implemented on a subset
of the cores in a graphics processing unit (a ``GPU''),
others might be more conventional CPU's,
and still others on special purpose chips perhaps associated with sensors of
some kind. How nice, then, if you can ask a place whether it is a GPU by
evaluating ``{\tt place.isCUDA()}''---CUDA is one of several architectures
for GPUs, and {\tt isCUDA}, as it happens, really is a public method
for {\tt Places}.  

The concept of {\tt Place} has, therefore, been carefully kept abstract:
an \Xten{} program's places partition its address space.  That's all they do.
Places can correspond to processors, but they don't have to. If you are
implementing \Xten{} on a multiprocessor, you would probably
set it up to have one {\tt Place}
per CPU. That would give your users precise control over where their data and
computation goes.  

But you needn't stop there.
In principle, you could also implement a ``uniprocessor mode'' for the multiprocessor.
You could then have a runtime flag that turns the mode on, with the effect that
the multiprocessor
and its memory are treated as a single place and the operating system's
thread manager distributes the work among the available processors.
From the point of view of the \Xten{} language, this is a perfectly reasonable
thing to do.  

You can also go the other direction and 
arrange to have many {\tt Places} per processor, whether or not the
physical processor is a multiprocessor.  For example,
you might want to test your supercomputer code on a laptop by putting eight or
sixteen {\tt Places} on its single core.  This may not run very fast, but it will
run---or at least hobble---and will give you a chance to find some bugs.  


\subsubsection{Getting Work Done At Other Places}

The set of {\tt Place}s available to a program is fixed when the program starts.
\footnote{
\Xten{} isn't built for dealing with changes in the available hardware in 
mid-application.  It can't handle what
amounts to plugging new computers in in mid-run.  While there {\em are}
applications for which plugging new computers in mid-run makes sense,
handling a dynamic set of {\tt Place}s across all applications would slow
down those computations that do not require that flexibility, and it is these
more limited application's performance that is \Xten{}'s primary concern.
}

The constant {\tt Places.ALL\_PLACES} tells you how many places there are.
The following program displays it on the console:
%%START X10: PlaceCounter.x10 placecounter
\fromfile{PlaceCounter.x10}
\begin{xtennum}[]
public class PlaceCounter {
  public static def main(argv:Array[String](1)) {
     Console.OUT.println("This is running on " + 
        Place.ALL_PLACES + " places.");
  }
}

\end{xtennum}
%%END X10: PlaceCounter.x10 placecounter


\bard{WE NEED MORE HERE ABOUT THE COMMAND LINE AND ENVIRONMENT.
Run this with a couple different sets of parameters, showing the
parameters on a couple of settings and how you control the number of places.
Evidently multi-place is broken on Macs just now, alas.}

If an activity needs to get a computation {\tt S} done at another place {\tt p},
it executes the statement {\tt at(p) S}.  The initiating activity's thread that 
is requesting the {\tt at} is suspended, all necessary data from the initiating 
place is copied to {\tt p}, and then {\tt S} is executed at {\tt p}.  For
example, the {\tt for} loop here displays ``Hello...' from every available
place exactly once:
%%START X10: src/concurrency/HelloFromEveryPlace.x10 hfep
\fromfile{HelloFromEveryPlace.x10}
\begin{xtennum}[]
public class HelloFromEveryPlace {
  public static def main(argv:Array[String](1)) {
    for(p in Place.places()) { 
      at(p) {
        Console.OUT.println("Hello from " + here); 
        assert here == p;
      }
    }
  }
}
\end{xtennum}
%%END X10: src/concurrency/HelloFromEveryPlace.x10 hfep

\begin{description}
\item[line \xlref{hfep-for}{3}] 
Here we loop over a collection.
We've already seen looping over a range like \xcd`1..N`.   
In fact, X10 allows loops over anything that looks like a
collection of things -- we'll see the details in Section \ref{sec:forinloops}.

{\tt Place.places()} implements the
interface {\tt Sequence}, which is the interface that C and Java arrays
present: a fixed size and indexed by a single integer.  The entries in the
{\tt Sequence} are all the {\tt Place}s available to the program, indexed
by their {\tt ids}. 
The {\tt for...in} loop i iterates over all the entries of the {\tt Sequence}
in order.  In this example, that means each {\tt Place} should be visited in order
by {\tt id}.  

\item[line \xlref{hfep-at}{4}]
The body of the loop is this single {\tt at(p)} statement.  Let's look carefully
at what happens as the loop is executed.

    {\em Startup:}  When
    execution reaches the {\tt for} loop, the action is at a
    {\tt Place} that we'll call {\tt pMain}.\footnote{
    Normally, {\tt main} is called because the class is being invoked to do its thing
    from the command line: ``{\tt x10 HelloFromEveryPlace}''.  In that case,
    {\tt pMain} by convention is the {\tt Place} with {\tt id==0}.  But for our
    purposes right now, it does not matter at what {\tt Place} {\tt main} is executing.
    }
    
     {\em The top of the loop:}  {\tt p} gets assigned the next entry in the
     {\tt Sequence} {\tt Place.places()}, unless none
     remain, in which case we exit the loop.  
     
     {\em The transfer:} The thread executing the {\tt for} is suspended.
     A new thread is spawned at {\tt p}.
     
     {\em The body:} The thread at {\tt p} executes the body of the loop.
     
     {\em The bottom of the loop:}. Once the thread executing the body
     completes its work, it dies, and  the thread for the
     {\tt for} loop is resumed at {\tt pMain}.
     The effect is to go back to the top of the loop.

 The important thing to understand here is that {\em there is no concurrency}.
 Even though every {\tt Place} participates, and even though
 there is one more thread than
 there are {\tt Places}, {\em there is still only one thread active at a time.}
 From the point of view of the \Xten{} language, what we are looking at here
 is a single activity: it happens to involve a lot of {\tt Places} and threads,
 but still: it is a single, serial set of operations.  
 
\item[line \xlref{hfep-out}{5}] The identifier {\tt here} is reserved by \Xten.
It always refers to the {\tt Place} where the computation is happening at
the moment the occurrence of {\tt here} is accessed. On line \xlref{hfep-out}{5},
that is the
{\tt Place} {\tt p} to which the {\tt at} on line  \xlref{hfep-at}{4} sent us.  

\item[line \xlref{hfep-assert}{6}] Just for the sake of the example,
we check that {\tt p} really is the same as {\tt here}.  
This is an assumption that we expect always to be true, and would be very
upset if it turned out to be false.  The \xcd`assert` statement is designed
for just that purpose.  

(In this case, the rules of the language guarantee that the two really are the
same.  We wouldn't check on it with an \xcd`assert` in most real programs --
unless we suspected that the compiler was broken.)


   {\tt assert E} checks that an expression {\tt E} whose value is a 
   boolean (that is, whose value is either {\tt true} or {\tt false}) is {\tt true}, and, if it's not,
   it reports the error to the standard error stream and {\em aborts the activity
   that failed the assertion.}   Thus, {\tt assert} both documents and enforces
   your assumptions in a nice, compact way.
   Another virtue of using {\tt assert} over a plain
   old {\tt if} is that there is a compiler switch ``{\tt -noassert}'' that turns off all
   the {\tt assert} statements in your code.  


By the way, something subtle has happened here that is easy to overlook.
In order to compare {\tt p} with {\tt here}, {\em the thread at {\tt p} has to know
the value of the variable {\tt p}}: the value of {\tt p} got
sent from one place, {\tt pMain}, to another, {\tt p}.  We say that
the {\tt at} statement ``captured the variable {\tt p}''.  
We'll learn much more about what {\tt at} statements have to capture shortly.       
\end{description}

In {\tt HelloFromEveryPlace}, {\tt at(p)} is, in effect, being used as a command: 
``go to {\tt p} and execute the statement that follows.''   It can also be used just to 
compute an expression.  Again, there is no concurrency: ``{\tt x = at(p) e}'' suspends
the thread computing {\tt x}, computes the expression {\tt e} at {\tt p}, copies the
value back to {\tt x}'s {\tt Place}, and resumes {\tt x}'s thread, which
stores the value into {\tt x}.  We saw this in action in our multi-place version
of the Monte Carlo computation of $\pi$.  Here's another example:
%%START X10: src/concurrency/AtExpr.x10 atexpr
\fromfile{AtExpr.x10}
\begin{xtennum}[]
public class AtExpr {
   public static def main(argv:Array[String](1)) {
      val pMain = here;                      
      val pNext = pMain.next();              
      val nextNext = at(pNext) here.next();  
      Console.OUT.println("The next next is "+nextNext.id);     
   }
}
\end{xtennum}
%%END X10: src/concurrency/AtExpr.x10 atexpr

\begin{description}
\item[Line \xlref{atexpr-pmain}{3}:] 
{\tt main} starts at somewhere that we'll call {\tt pMain} again.    
\item[line \xlref{atexpr-pnext}{4}] 
Whenever {\tt p} is a {\tt Place}, 
{\tt p.next()} is the next one: the one whose id is 1 more than {\tt p}'s.  If
{\tt p} is the {\em last} {\tt Place}, then {\tt p.next()} is the {\tt Place}
with id 0: {\tt next()} wraps around.

\item[Line \xlref{atexpr-nextnext}{5}] At {\tt pNext} we ask what the next {\tt Place}
is and assign that value, back at {\tt pMain}, to the variable {\tt nextNext}.
\end{description}

\section{Concurrency: Walking {\em and} Chewing Gum}

For those too young to remember, the gibe ``He wasn't smart enough to walk
and chew gum at the same time'' was famously (and very unfairly) aimed at
President Gerald Ford.  You don't want people saying that about your code.
One of the main reasons for one processor sending a computation---particularly
a big one---off to another processor is that while the second processor handles
the computation, the first processor can continue working on other things.
 
Now, {\tt at} gets you to other processors, but, as we have been
emphasizing, by itself, {\tt at}  does not lead to a parallel, concurrent thread.
For example, if the two calls to {\tt bigComputation} in
\begin{xten}
val big1 = at(Place.places(1)) bigComputation(100,200);
val big2 = at(Place.places(2)) bigComputation(200,100);
\end{xten}
are independent of one another, there is no reason not to do them 
concurrently.  But the code, as we have written it above, will wait for the
assignment to {\tt big1} to complete before initiating the second, because the
first {\tt at} will suspend its own thread until its remote thread finishes.  

Getting  {\tt Places} 1 and 2 do most of the work using this straight-line
code yields no
speedup {\em per se}, and it won't do so no
matter what {\tt bigComputation} is actually doing.  The two
constructs that we need to get the two {\tt Places} working simultaneously are:
\begin{quote}
\begin{description}
\item[ {\tt async} {\tt S}:] to start a new
activity that runs in parallel with its originator.  The new activity executes the
statement {\tt S} and then dies.
\item[{\tt finish} {\tt S}:] to execute the statement {\tt S} and on reaching the
end of {\tt S}, suspends its own activity until every activity spawned by
executing {\tt S} has completed.  
\end{description}
\end{quote}
If you come from the C world, this is an extension of the familiar ``{\tt fork/join}''
duo, except that {\tt finish} allows you to wait for a whole set of activities,
whereas {\tt wait} is a per-thread operation.
Here's how \Xten's two work together to make our example hum:
%%START X10: src/concurrency/WalkAndChew.x10 walkandchew
\fromfile{WalkAndChew.x10}
\begin{xtennum}[]
val n = 3;
val big1: GlobalRef[Cell[String]] = GlobalRef[Cell[String]](new Cell[String]("!"));
val big2: GlobalRef[Cell[String]] = GlobalRef[Cell[String]](new Cell[String]("2"));
val pMain = here;   
finish { 
   async at(Place.place(0)) { 
      val bc1 = bigComputation(n,n);
      at(big1.home) big1()() = bc1; 
   }
   async at(Place.place(0)) { 
      val bc2 = bigComputation(n,n);
      at(big2.home) big2()() = bc2; 
   }
} 
assert big1()().equals(big2()()); 
\end{xtennum}
%%END X10: src/concurrency/WalkAndChew.x10 walkandchew

\bard{Picture!}
\bard{Explain that funky \xcd`big2()()`!}
\begin{description}
\item[Line \xlref{walkandchew-pMain}{4}: ]
We capture the {\tt Place} we are coming from, so we can let the {\tt Places}
that are doing the dirty work know where to send their results.
\item[Line \xlref{walkandchew-finish}{5}: ]
This {\tt finish} guards two {\tt asyncs}.  When control reaches here, the two {\tt asyncs}
will get executed.  Control will then reach the end of the {\tt finish}'s block
at line \xlref{walkandchew-finishend}{14}. The {\tt finish}'s  activity will be stopped until the 
two {\tt asyncs} have both finished.  Only then will line  \xlref{walkandchew-assert}{15}
be reached.
\item[Lines \xlref{walkandchew-async1}{6} and \xlref{walkandchew-async2}{10}: ]
Each {\tt async} creates an activity that runs in parallel with the {\tt finish}.
That activity computes its value at the remote {\tt Place}
and then ``goes home'' to slam the result into memory there.
\item[Lines \xlref{walkandchew-big1}{8} and \xlref{walkandchew-big2}{12}: ]
Here we see the `` {\tt at}''s capturing {\tt bc1} at {\tt Place} 1 and {\tt bc2} at 
{\tt Place} 2.
and assigning their values back home, which is wherever the original activity was executing.
\item[Line \xlref{walkandchew-assert}{15}: ]
When we get here, we {\em know} that both {\tt big1} and {\tt big2} have been
set to their new values, so the comparison is safe.
\end{description}

If you want to play with this code, we have provided the serial version in
\href{src/concurrency/WalkThenChew.x10}{WalkThenChew.x10}, and the
parallel version in \href{src/concurrency/WalkAndChew.x10}{WalkAndChew.x10}

You might also want to experiment by changing line 
\xlref{walkandchew-async2}{10} with ``{\tt at(Place.places(2))} {\tt async}'', in
which the new activity is spawned at the {\em remote} {\tt Place}, rather than at
the current one. 

Finally, a more realistic example of this sort of program is a shared-memory
(single {\tt Place}) version of the {\em Quicksort} algorithm, a working version
of which you can find in
\href{chapter-concurrency/QSortInts.x10}{QSortInts.x10}.:
%%START X10: src/concurrency/QSortInts.x10 qsortints
\fromfile{QSortInts.x10}
\begin{xtennum}[]
public class QSortInts {   
   /**
    * top-level call: sorts the input array in place using the
    * quicksort algorithm.
    * @param data the array of Ints to be sorted.
    */
   public static def sort(data: Array[Int](1)) {
      val r = data.region;
      val first = r.min(0), last = r.max(0);
      sort(data, first, last); 
   }
   public static def sort(data:Array[Int](1), 
             left:Int, right:Int) {
      var i: Int = left, j: Int = right;
      val pivot = data(left + (right-left)/2);
      while (i <= j) {
         while (data(i) < pivot) i++;
         while (data(j) > pivot) j--;
         if (i <= j) {
            val tmp = data(i);
            data(i++) = data(j);
            data(j--) = tmp;
         }
      }
      finish { // when you are here i > j 
         if (left < j) async sort(data, left, j);
         if (i < right) async sort(data, i, right);
      }
   }
\end{xtennum}
%%END X10: src/concurrency/QSortInts.x10 qsortints
\begin{description}
\item[Line \xlref{qsortints-sort}{10}] 
The algorithm is recursive: the call {\tt sort(data,left,right)} sorts the slice 
of the array between the indices {\tt left} and {\tt right}.  We assume that
{\tt left} {\tt <=} {\tt right}.

\item[Line \xlref{qsortints-pivot}{15}]
This is a naive choice of ``pivot'' point: we hope that the element in the middle
of the slice is close to the median for the slice.
This loop reorganizes the slice into
two subarrays: (1) those elements less than the pivot, and (2) those greater.

\begin{finepoint}
If you're wondering why we wrote {\tt left} {\tt +} {\tt (right-left)/2} rather than
the more natural {\tt (left+right)/2}, the answer is {\em integer overflow}: the intermediate sum
{\tt left+right} may overflow, but if {\tt left} and {\tt right} are non-negative
{\tt Ints} and {\tt left} {\tt <=} {\tt right}, then the difference {\tt right-left} is not
going to overflow, and since {\tt left} {\tt +} {\tt (right-left)/2}
{\tt <=} {\tt right}, our computation of the middle index is safe.
\end{finepoint}

\item[Lines \xlref{qsortints-while}{16}-\xlref{qsortints-endwhile}{24}] 
This loop partitions the input slice of {\tt data} into two parts,
and reorganizes {\tt data} so that the left part consists of the entries that
should precede the pivot, and the right part consists of those that should
follow it.  This is straightforward serial code. 

\item[Lines \xlref{qsortints-finish}{25}-\xlref{qsortints-endfinish}{28}] 
We can use two independent activities to sort the left- and right-hand
portions of the slice.   The {\tt finish} guarantees that when control reaches
the end of the method, the whole slice specified by its arguments has been sorted.  
\end{description}

{\bf Exercise:} If you stare at this code for a while, you'll probably realize several things.
First, we don't really need {\em two} {\tt asyncs}: we only need an extra {\tt async}
when we have
to sort {\em two} subarrays.  If there's only one (which can happen if by some evil
chance, the pivot turns out to be the largest or smallest element in the slice),
there's no gain to spawning a new acitivity: the current activity can handle it.
Also, we really don't have to put the ``{\tt finish}'' in the
recursive call, {\em if we make that method {\tt private}, so no one will mistakenly call it
directly}.  The {\tt finish} gets moved into the public, top-level {\tt sort} method, because
we don't really care in what order the spawned activities finish: all we care is that {\em all}
of them finish.  This brings up an important point: it does not matter how, when, or where the
activities inside a {\tt finish} get spawned---all must be complete
before the {\tt finish} will allow its own activity to resume.  So the activities spawned by the
recursive calls are all monitored by the one {\tt finish} at the top level when
you use it to guard the call to {\tt sort} in line \xlref{qsortints-sort}{10}.

If you want to see our version of this improved code, you can find it in
\href{chapter-concurrency/QSortInts2.x10}{QSortInts2.x10}.  In
\href{chapter-concurrency/QSortInts3.x10}{QSortInts3.x10}, we push it one step
farther with a less naive choice of pivot.

\begin{finepoint}
Finally, a different question, why just sort {\tt Ints}? 
Why not sort data of any type whatever, so long as we know how to compare
two things of that type using the  binary operators ``{\tt <}'' and ``{\tt >}''?   
To do that, 
we need syntax that allows us to declare the operators ``{\tt <} '' and ``{\tt >}'',
and  {\tt x10.util.Ordered[T]} that is exactly what we need.
It requires its implementors to provide four ``methods'':
\begin{xten}
operator this <   (that: T): Boolean;
operator this >   (that: T): Boolean;
operator this <=  (that: T): Boolean;
operator this >=  (that: T): Boolean;
\end{xten}
These declarations are just like the abstract method declarations we've seen in interfaces
before, except that the keyword phrase ``{\tt operator this}'' 
is used rather than ``{\tt def}''.  Of course, there is nothing special about comparison
operators.  You can also implement any of the arithmetic and bitwise operators for
any class or struct you create.
\footnote{
For examples, take a look at {\tt x10.lang.Arithmetic.x10}, where all
of the numeric operators are declared, and {\tt x10.lang.Bit\-wise.x10}, where you'll
find the shifts, and's, and or's. 
}

If a class or struct {\tt T} implements {\tt Ordered[T]}, and if {\tt t1} and {\tt t2}
are two instances of {\tt T}, then these declarations say we can
execute  {\tt t1} {\tt <} {\tt t2} to determine whether {\tt t1} is less than {\tt t2},
and of course the same for the other three operators..

How do you declare concrete versions of these operators in your own classes
and structs so that they can implement this interface?  It's as easy as you would think:
add the keyword ``{\tt public}'' and provide the body of code that implements the 
operator:
\begin{xten}
   public operator this  < (that: T): Boolean {
      // your implementation comparing "this" to "that"
      // goes here
   }
\end{xten}
The only difference, once again, from a method declaration is that  
\xcd`operator this`
replaces \xcd`def`.

There is one final problem: how does one say ``my class {\tt T} implements
{\tt Ordered[T]}'' when you go to use {\tt T} in a declaration?
The operator ``{\tt <:}'' that we introduced on page
\pageref{subsub:type:anything} does the trick:

\begin{xten}
public class QSort[T]{ T <: Ordered[T] }  { ... }
\end{xten}

You can find our version of the code with these modifications in 
\href{chapter-concurrency/QSort.x10}{chapter-concur\-ren\-cy/QSort.x10}.

It is a good exercise to run this code on some big examples in order to
understand the cost for writing  generic code versus type-specific code.
We provided some minimal wrappers
for {\tt Int} and {\tt String} as examples to get you started using our generic code. 
Strings are an interesting variant, because if they have an average length much
greater than 8 or 10, then---unlike the story for Int---significantly more effort may be
required to do the comparisons than the
overhead of the array operations and {\tt async} management.  This should reduce
the relative penalty for going generic (which should be independent of the item
size and comparison cost).   We also 
have provided a ``native'' Quicksort for strings, 
\href{chapter-concurrency/QSortStrings.x10}{QSortStrings.x10},
that you can use to compare with the generic.
\end{finepoint}

\subsection{More About Asyncs}\label{ssec:moreasyncs}

When control reaches \xcd`async S`, a new
activity in parallel with the executing activity is spawned to execute {\tt S}.
The two activities are both running in the same {\tt Place}  and have access
to the same variables, but otherwise they are
independent of one another, except for such synchronization as the originating
activity imposes---for example, executing the {\tt async} within a {\tt finish}.

The best way to think about multiple activities happening concurrently at a
single {\tt Place} is to imagine there is one master activity, ``{\em the scheduler}'', 
that runs with special privileges.  The
scheduler alone gets to decide, at each point in time, which activity is going to run.
It will generally limit the time the activity can be run before it interrupts
the activity on behalf of the other waiting activities.  It may also choose to interrupt an
activity part-way into its time slot, even in mid-statement, if it perceives a need, like
an I/O operation that must be serviced promptly.  Finally, an activity can ask the
scheduler to suspend it until some event occurs: ``As soon as the value of
the variable {\tt x} is greater than 12, let me resume, please.'' 

Having the scheduler around is a little like having a partner helping you by
writing some of the code.  Sequential code that you write all by 
yourself can be pretty bad, hard enough to get right.  Concurrent code
can be much, much worse: it has all the problems of sequential 
code plus a few really bad ones of its own because your co-author,
the scheduler, neither knows nor cares about your intentions.

We are going to look carefully at three problems that you don't find in
sequential code:

\begin{description}
\item[1. Data races: ]
As we saw when we went to implement the multi-cluster Monte-Carlo computation
of $pi$ (page \pageref{verb:montepierror}), the scheduler can create situations
where several activities sharing a resource (like a storage location or an I/O stream)
can run into trouble because they failed to coordinate their actions.  We'll expand
on that example in a moment.
\item[2. Deadlock: ]
Deadlock happens when some family of activities can make no progress because
each one has asked the scheduler to  suspend it because it is waiting for one of
the other activities to update some variable. We'll see an example of how this
might happen in \Sref{ssec:queues}.
\item[3. Error handling: ]
Errors occur in serial code, too, of course.  But parallelism adds a level of 
complexity.  Suppose an activity {\tt A} spawns an activity {\tt B} to do some
job---store a row in a relational database, perhaps.  {\tt A} goes on doing
whatever it has to do while {\tt B} does its thing.  
Suppose that {\tt A} doesn't need any {\em result} returned by {\tt B},
it just needs {\tt B} to do what it was asked to do.  What happens
if  {\tt B}
fails?  How does {\tt A} find out?  When?  It may or may not be okay for
{\tt A} to continue when {\tt B} fails.  Somehow we've got to get 
help from the scheduler, who is responsible for the overall management
of our activities. We'll discuss all
this in Section \ref{sec.???}
\end{description} 

None of these problems is \Xten{}'s fault.  In fact, \Xten{} has features that make
writing parallel code safer and more readable than many high-performance languages.
(There are languages that give even
safer concurrency, but they pay for it by having programs run substantially
more slowly.)  Our aim is that by the time you finish this book, you will be comfortable
using \Xten{} to write serious concurrent code that's visibly---and in many
cases, even {\em provably}---safe.

\subsection{Finish Really Means ``Finish Everything Everywhere''}\label{ssec:finishmeans}

The {\tt finish S} construct executes the statement {\tt S} and then
waits until {\em all} activities started by {\tt S}
are finished.  We've seen examples that show it waiting for the activities started
by {\tt asyncs}
appearing explicitly in {\tt S},  but you should be aware that it also waits
for any activities spawned by those {\tt asyncs}, and even activities
spawned by methods called while executing {\tt S}.  If you worked through all of
the Quicksort examples, you will have seen this at work.  Here is a much simpler program,
with a more direct example:


%%START X10: NestAsyncs.x10 nestasyncs 
\fromfile{NestAsyncs.x10}
\begin{xtennum}[]
 var a:Boolean = false, b:Boolean = false, c:Boolean = false;
 def nestAsyncs() {
  finish { 
      async {
        async { a = true; } 
        async { b = true; }
      }
      async { c = true; }
   }
   assert a && b && c;
}

\end{xtennum}
%%END X10:   NestAsyncs.x10 nestasyncs 

When {\tt nestAsyncs()} is invoked, four activities get started: one at 
line \xlref{nestasyncs-async1}{4},
which in turn starts the activities at lines \xlref{nestasyncs-async2}{5}.
and \xlref{nestasyncs-async3}{6},
and finally the original activity spawns another at line \xlref{nestasyncs-async5}{8}.
The {\tt finish} beginning at line
\xlref{nestasyncs-finish}{3} and ending at line \xlref{nestasyncs-endfinish}{11}
waits for all four of these to finish before control
proceeds to line \xlref{nestasyncs-assert}{10}.  
So, the {\tt assert} there is always going to be succeed.

We can rewrite the program so that some of the activities are spawned by
a different method.  Even this does not fool {\tt finish}, which still waits
for all of the activities to finish.
%%START X10: src/concurrency/IndirectAsyncs.x10 indasyncs 
\fromfile{IndirectAsyncs.x10}
\begin{xtennum}[]
var a:Boolean = false, b:Boolean = false, c:Boolean = false;
def spawnAsyncs() { 
   async {a = true;}
   async {b = true;} 
}
def nestAsyncs() {
  finish { 
     async  spawnAsyncs();
    async { c = true; }
  }
  assert a && b && c;
}
\end{xtennum}
%%END X10: src/concurrency/IndirectAsyncs.x10 indasyncs 
The activity started in line \xlref{indasyncs-async3}{8} calls {\tt spawnAsyncs},
which spawns two {\tt asyncs}.  At the point where we reach the end of the
{\tt finish} at line \xlref{indasyncs-endfinish}{10}, as many as 4 {\tt asyncs} may be
alive.  The main activity is blocked until all 4 finish.

\subsection{When To Use {\tt finish}}\label{ssec:whenusefinish}

Most {\tt async}s should have a {\tt finish} controlling them.
The usual idiom is: 
\begin{xten}
finish {
   /*  start some async computations here */
}
/* use the results here */
\end{xxten}

If you don't have a {\tt finish} around the {\tt asyncs}, then you have no way of
dependably knowing when all of its results are available.
Sometimes this may be exactly what you want---some examples:

\begin{itemize}
\item Perhaps you have just finished some big part of the computation and are
      spawning off an activity to write the answer to a file. If the current activity
      doesn't need the file, there's no need
      to wait for the child activity to finish writing.

\item Perhaps at some point there are several ways to get the result you need.
	You might want to fire off one {\tt async} per way.  For example, ``compute from
	scratch'', ``look it up in the database'', and ``search the Web''.
	Here all you want is one  answer, so all you care about is when the {\em first}
	computation in the set succeeds.  You will probably want to have some way
       to kill the others then, rather than letting them run and
       waste time and resources.   We'll look at this sort of pattern in section
      \ref{sec.firstcomefirstserve}.

\item Perhaps the activity is some kind of background chore, like listening
	for incoming network traffic and servicing it.  It doesn't particularly have
	a final answer.  On the contrary, it's supposed to run quietly doing its
	thing forever.  Waiting for it to finish would be ridiculous.  It's not supposed to.

\item Perhaps the newly spawned activity will run for a long time, but the main 
      activity, although it {\em does} need the result, has to proceed with some
      other work: you care  about the answer, you just don't want to be idle
      while you wait for it.  Some other communication mechanism is needed
      for the child activity to tell the
      main activity when its result is available.  We'll worry about
      this pattern in section \ref{sec:clocks}.
\end{itemize}

While none of these is particularly outr\'{e}, none of them is terribly common in \Xten{}
programming, either.   Most activities should have a {\tt finish}.

{\bf Basic Question:} Is there a point in the currently executing program that
you {\em might} need to be sure a set of {\tt asyncs} has completed?  

If the
answer is ``Yes!'',   you obviously need put the {\tt asyncs} inside a {\tt finish}.
If you're not sure the answer to that question is ``No!'', err on the side of caution:
you probably want the {\tt finish}.   Remember that the scheduler will eventually
cause you problems if it possibly can.

{\bf A Less Obvious Question:} Can one or
more of the {\tt asyncs} have terminated only because some unrecoverable
error cause them to be aborted, not because they actually completed the task
they were set to do?  

From the point of view of the {\tt finish}, done is
done: it does not ask {\em how} an {\tt async} terminated, it just wants to know
that it {\em has} terminated.  Is there hope that if you knew why the abnormal
termination (an ``{\em abend}'') occurred, could you recover? 
Or the flip side: if an abend occurred, dare you continue?  We'll discuss ways
to handle these questions in section \ref{sec:error-recovery}.  For the moment
we just wanted to make sure you were aware of what {\tt finish} does {\em not} do for you.
