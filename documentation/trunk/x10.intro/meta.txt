Sat Jul 28 04:27:59 2012

Authors: Vijay, Olivier, Dave G, Dave C, Mikio, Ben, Bard, 

This document is to be the guide to programming efficiently in X10 for
the HPC programmer. 

The programmer is supposed to be familiar with large scale computing
issues, including execution on multiple places, message based
communication (MPI), SPMD  computations, collective operations. The
programmer is familiar with C, C++, but not necessarily Java. Hence
not used to the idea that the language is garbage collected.

Our agenda is to open the minds of these programmers. Get them to
realize that there is a viable way of looking at the world that goes
beyond MPI. And there are some really beautiful things you can do
here. 

We want to explain to them all X10 concepts and concrete idioms
(including annotations) that they need to use to write efficient,
idiomatic code. Where certain "hacks" are needed to get performance,
we should explain those but hopefully in a way where we can explain
what the idiomatic version is and what we expect to do to the tool
chain in the future to support the same performance as the hacked
version for the idiomatic code.

This is intended to be the definitive guide to high performance
programming in X10 v 2.2.3, the PERCS milestone version. This document 
is going to be focused on the C++ backend. We shall focus on
multi-processing, not so much on multi-threading. 

The pedagogic strategy is to use the PERCS benchmarks to concretely
illustrate all ideas. They will serve as the running guide.

Everywhere (after the introduction) use concrete pieces of running
code to explain concepts. 


Target page count: 100-120 pp

Order of chapters

 I Introduction

 II Basics of X10 
    basic oo structure
    structs
    functions
    types -- constrained types, inner types, type-defs, type
    inference, <: 

    arrays -- indexedmemorychunk

    interface to C/C++

    annotations, pragmas

III APGAS Model

  a. places
    at, object model (copying of values), 

  b. async

  c. finish

  d. clocks 

   (How exensively are clocks used in these benchmarks?)

  e. atomic, when (They may not play a major role in this document.)
   
IV.  APGAS  model as realized in X10

    GlobalRef

    PlaceLocalHandle

    async scheduling -- fork join scheduler.

    SPMD computation -- place groups, teams.
      x10.util.Team -- interface to low level collective operations
      all reduce

    Array .. array copy.


 IV Stream
    Explain the basic computational problem.

    So what do we need? We need to be able to allocate arrays of
    doubles, zeroed. Use huge pages to reduce size of look-aside
    buffer? 

    IndexedMemoryChunk
    GlobalRef

    Team -- broadcast flat. 

    Discuss how this is implemented as just
    public def broadcastFlat(cl:()=>void) {
      @Pragma(Pragma.FINISH_SPMD) finish for (p in this) {
       at (p) async cl();
     }
   }

V   Using Teams

   KMeans -- embarassingly parallel code.

    Team.allReduce
    Team.barrier

    Perf discussions

    FT -- allToAll, barrier

    Perf discussions

VI Using Distributed Arrays
    LU 
      blocked array, views on arrays.

      Async and back idiom: how to get a remote row, how to swap memory rows.

      Perf discussions

    Random Access
       Congruent arrays
       RemoteIndexedMemoryChunk

VII Using multiple threads

      SSCA2
        priorityqueue
        allreduce
      Performance

VIII Putting it all together
    UTS
      all the idioms

    Performance

TBD: Do we have space and time for SSCA1?

Appendix x10doc for key library routines. 

This chapter is intended to have the normative discussion, referenced
to by the rest of the book for details.

  IndexedMemoryChunk

  GlobalRef

  PlaceLocalHandle

  PlaceGroup

  Array
    including array.copy

  Options

Appendix: Building X10

Choosing which transports to work with. When to use MPI.

optimized build.

