/************************************************************************
This program solves a system of linear equations using basic Gaussian
elimination.

The computation is controlled by LAPI task 0 (also called VM 0).  It
first creates a Solver object and uses LAPI to tell all of the other
VMs about the object.  The idea is that it uses NewGlobalObject to
create a persistent key which will be the name by which the Solver will
be known globally.  All subsequent LAPI calls will refer to the solver
by this name and each VM must map this name to a local version.  (The
key is a "fat pointer.")

The next step is the generation of the coefficients of the matrix A
and the vector b.  The idea is that the columns of A are distributed
to VMs in a round robin fashion.  The vector b (and the solution vectors
x and y) are located on VM 0.  Identical copies of the code to generate
the coefficients is run on each VM.  Each VM, of course, only updates
the coefficients that it owns.

Not much care is taken in generating the matrix.  (The first bug is this:
there is no check to ensure that the various processors have, in fact,
executed the setup.  I should add a LAPI_Fence.)  Certain sized matrices
generated by setup will be singular (i.e., the system does not have
a unique solution.)  Other matrices will result in solutions with
unpleasantly large residuals.  If you encounter one of these, try a
slightly different problem size.

The first step where we care about performance is in the LU decomposition
of A.  VM 0 iterates over the columns from left to right.  Each column
is processed by the VM that owns it.  The LU decomposition process is
serial.  For each column, a simple message is dispatched to the VM
owning the column telling it what column to process and where (VM 0) to
send the result.  (The result is a boolean indicating whether or not
the process has discovered a singularity in the system.  For singular
systems, the LU factorization process stops abruptly.)  Of course,
no message is actually sent to VM 0 (from VM 0) for columns held on VM 0.

LU factorization has a step (describe below) that can be done in parallel.
For this reason, and because the process can take a while, the steps are:

   VM 0 sends a message to VM n and does a busy wait waiting for a
   flag to change.

   VM n receives the message and creates a new thread to run the local
   processing.  At this point, the LAPI threads on VM n are ready for
   traffic.

   Once VM n has completed its work, it does a LAPI_PUT_XFER to update
   the flag that VM 0 is waiting for.  After that, the worker thread dies.

The LU factorization routine computes two pieces of information:
   1. The row with which the current row must be exchanged
   2. The set of multipliers used to scale the rows underneath
      the current one.

Once those two pieces of information are available, every column (other
than the current one) can be processed in parallel.  The "processing"
is called AdjustBelowDiagonal.  (Okay, not a terrific name.) The LU
factorizer sends a message to every other processor.  The idea is to
tell each processor to start performing whatever transformations are
necessary for all of the columns owned by the processor.  This is the
place where we can get some parallelism, so we want all this to be
fast.  The steps are:

   VM n (the one performing the current LU factorization step) sends a
   message to every other processor.  The message consists of a header
   and data where the data is the set of multipliers.  VM n waits for
   the send completion routine to be called before advancing to send
   the message to the next processor.  Perhaps this could be done better?
   Maybe using fences?  At any rate, once the message has been sent to
   all processors (other than VM n itself) VM n proceeds to perform
   the required steps (AdjustBelowDiagonal) on its own columns

   VMs other than VM n receive the AdjustBelowDiagonal message and the
   header handler arranges to copy the data part of the method directly
   into a Java array.  The header handler returns and allows the completion
   handler to take over.  It requests that the completion handler run
   on the same thread as the header handler.  Is this a good idea?
   At any rate, the completion handler causes Java code to run on the
   same thread as the completion handler.  This code performs the
   required work on all columns owned by the VM.  There could be a
   bit of work required here.

   When VM n has finished performing work on its local columns, it waits
   for all the other processors to finish.  It simply performs a busy wait.

   When VMs other than n finish their processing, they each perform
   a LAPI_PUT_XFER to update the busy wait flag on VM n. When all of the
   processors have performed such a PUT, VM n finishes its busy wait.

When the factorization process completes, the matrix is now L (which
are the cumulative set of multipliers below the diagonal plus I, the
identity matrix) and U (which is everything above and to the right of
the main diagonal.)  At the same time, y has been computed by VM 0.
The following identity now holds Ux = y.  So what remains to be done is
to back substitute to solve for x.

As with LU factorization, the process is controlled by VM 0 which iterates
over all the rows starting at the bottom and working up.  Processing
row i results in computing xi (the i'th component of the x vector).
Processing of row i is performed on the processor that owns column i
Similar to LU factorization, for each row, the following steps are taken:

   VM 0 sends a message to VM n and does a busy wait waiting for a
   flag to change.

   VM n receives the message and creates a new thread to run the local
   processing.  At this point, the LAPI threads on VM n are ready for
   traffic.

   Once VM n has completed it's work, it does a LAPI_PUT_XFER to update
   the flag that VM 0 is waiting for.  After that, the worker thread dies.

One of the steps in Uxeqy (the back substitution routine) involves summing
all of the coefficients in row i of the matrix U.  (Since our matrix
is really a combination of L and U, that really means summing the
coefficients to the right of the main diagonal.)  This summation can
also be done in parallel.  The Uxeqy process sends a message to every
other processor asking it to compute its partial sum of the coefficients
of row i in the columns that it owns.  The steps are:

   VM n (the one performing the current Uxeqy step sends a message to
   every other processor.  It follows the same pattern that the LU
   factorization step uses. (And, therefore, may have similar problems.)
   Again, once the message has been sent to all processors other than
   VM n, VM n performs the required computation on its own columns.

   VMs other VM n receive the rowSum message and the header handler returns
   almost immediately requesting that the completion handler run on the
   same thread.  The completion handler causes Java code to run on
   its own thread.  The work here is a small loop summing appropriate
   coefficients.

   When VM n has finished computing its partial sum, it waits for all
   other processors to finish.  Again, it uses a simple busy wait.

   When VMs other than VM n finish computing their partial sums they
   respond to VM n. This is a bit ugly and is probably not done correctly.
   The idea is to use a single LAPI_PUT_XFER to send both an indication
   that the work is complete and the result.  (The result is a double
   and every possible bit pattern is a valid value.)  The memory where
   the result in PUT (on VM 0) consists of a volatile flag on either
   side of a double.  VM 0 waits for both flags to be updated and assumes
   that if the flags on both sides are updated, the value in the middle
   is probably valid.  Something doesn't feel right about this.

Once the Uxeqy loop has finished, the part of the problem where we care
about performance is over.  Nothing else is timed.  However, the remaining
steps are: re-compute the original coefficients of A and b and compute
the residual (|Ax - b|).  For this step, I got lazy and fetch one
coefficient at a time as needed.  (Again, a LAPI active message and
a PUT reply.)  Needless to say, this part is quite slow.  I might
work on a better implementation.
*************************************************************************/

import java.io.ByteArrayInputStream;
import java.io.FileInputStream;
import java.io.IOException;
import java.lang.reflect.Field;
import java.lang.reflect.Array;
import java.util.Iterator;
import java.util.Properties;
import java.util.StringTokenizer;

class JAxeqb {
    public static void main(String[] args) {
        int N = 3;
        boolean debug = false;
        boolean computeResidual = true;
        for (int i = 0; i < args.length; ++i) {
            if (args[i].charAt(0) == '-') {
                if (args[i].equals("-debug")) {
                    debug = true;
                }
                if (args[i].equals("-vm")) {
                    ++i;
                    VMInfo.THIS_IS_VM = Integer.parseInt(args[i]);
                    if (VMInfo.THIS_IS_VM >= NUMBER_OF_VMS) {
                        System.err.println("vm # " + VMInfo.THIS_IS_VM +
                                           " >= " + NUMBER_OF_VMS);
                        throw new Error();
                    }
                }
                if (args[i].equals("-nr")) {
                    computeResidual = false;
                }
            } else {
                N = java.lang.Integer.parseInt(args[i]);
            }
        }
        for (int vm_no = 0; vm_no < VM_.length; ++vm_no) {
            VM_[vm_no].lapiTarget = vm_no;
        }
        System.loadLibrary("LAPISupport");
        VMInfo.init(VM_);
        // Main VM controls things
        if (VMInfo.THIS_IS_VM == VMInfo.MasterVM) {
            JAxeqb solver = new JAxeqb(N, debug);
            solver.setGlobalKey(solver.getGlobalKey());
            for (int vm_no = 1; vm_no < VM_.length; ++vm_no) {
                VM_[vm_no].newSolver(N, debug, solver.solverKey);
                VM_[vm_no].setUpSolver(solver.solverKey);
            }
            solver.setup();
            // Need a check to make sure that everyone is done
            if (debug) solver.print();
            long start_time = System.currentTimeMillis();
            try { solver.LUFactorize();
            } catch (Error e) {
                System.err.println(e);
            }
            if (debug) solver.print();
            if (!JAxeqb.computeYDirectly) solver.LyeqPb();
            if (debug) {
                solver.print();
                solver.checkLy();
            }
            solver.Uxeqy();
            long finish_time = System.currentTimeMillis();
            System.out.println("Number of milliseconds: " + (finish_time-start_time));
            // Maybe we should speed up the rest of this.
            if (debug) solver.print();
            solver.setup();
            for (int vm_no = 1; vm_no < VM_.length; ++vm_no) {
                VM_[vm_no].setUpSolver(solver.solverKey);
            }
            if (debug) solver.print();
            if (computeResidual) {
                double d = solver.check();
                System.out.println("residual = " + d);
            }
            //System.out.println("Number of milliseconds: " + (finish_time-start_time));
            for (int vm_no = 1; vm_no < VM_.length; ++vm_no) {
                VM_[vm_no].deleteSolver(solver.solverKey);
            }
            solver.releaseGlobalKey(solver.solverKey);
            solver = null;
            for (int vm_no = 1; vm_no < VM_.length; ++vm_no) {
                VM_[vm_no].shutdown();
            }
        } else {
            // VMs other than the main one
            while (!VMInfo.shallIShutdown) {
                try {
                    Thread.sleep(50);
                } catch (InterruptedException ie) {
                }
            }
        }
        VMInfo.term();
    }

    void setGlobalKey(long solverKey) {
        this.solverKey = solverKey;
    }
    
    void setup() {
        int myNextCol = VMInfo.THIS_IS_VM;
        int myActualCol = 0;
        int init = 1325;
        for (int col = 0; col < N; ++col) {
            double bv = 0;
            for (int row = 0; row < N; ++row) {
                init = 3125 * init % 65535;
                final double value = (init - 32768.0)/16384.0;
                if (col == myNextCol) {
                    A[myActualCol][row] = value + row - col;
                }
                bv += value;
            }
            if (col == myNextCol) {
                myNextCol += VM_.length;
                ++myActualCol;
            }
            if (VMInfo.THIS_IS_VM == VMInfo.MasterVM) {
                b[col] = bv;
                if (JAxeqb.computeYDirectly) {
                    y[col] = b[col];
                }
            }
        }
    }

    JAxeqb(int N_, boolean debug_) {
        N = N_;
        // Assume we have three VMs handling this.    Then, VM 0
        // handles columns 0,    3,    6,    9,       etc,  VM 1
        // handles columns   1,    4,    7,    10,    etc,  VM 2
        // handles columns     2,    5,    8,     11, etc
        A = new double[(N/VM_.length)+1][N];
        if (VMInfo.THIS_IS_VM == VMInfo.MasterVM) {
            b = new double[N];
            y = new double[N];
            x = new double[N];
        } else {
            b = y = x = null;
        }
        debug = debug_;
    }
    final int N;
    final double[][] A;      // A is an NxN matrix
    final double[] b;        // b is a column vector of length N
    final double[] y;
    final double[] x;
    final boolean debug;
    long solverKey;             // "Global" name of this solver
    static final boolean computeYDirectly = true;

    void LUFactorize() {
        // This is done by VM 0
        for (int diag_index = 0; diag_index < N; ++diag_index) {
            boolean b = true;
            if ((diag_index % VM_.length) == VMInfo.MasterVM) {
                b = LUFactorize(diag_index);
            } else {
                b = VM_[diag_index % VM_.length].LUFactorize(solverKey, diag_index);
            }
            if (!b) throw new Error("A is singular");
        }
    }

    // When VM 0 tells this VM to do an LUFactorization of a
    // column, that work should be done on a separate thread to
    // allow the LAPI threads to get back to business.
    void LUFactorizeOnAnotherThread(int diag_index, int lapi_task_expecting_reply, long address_of_reply) {
        final JAxeqb solver = this;
        final int d_i = diag_index;
        final int lt = lapi_task_expecting_reply;
        final long la = address_of_reply;
        new Thread() {
            public void run() {
                boolean b = solver.LUFactorize(d_i);
                VM_[lt].returnLUFactorizeValue(b, la);
            }
        }.start();
    }

    boolean LUFactorize(int diag_index) {          // do this column
        // This is done by the VM containing column "diag_index"
        int my_diag_index = diag_index / VM_.length;
        double largest_val = A[my_diag_index][diag_index];
        double swp = largest_val;
        int largest_row = diag_index;
        for (int row = diag_index; row < N; ++row) {
            if (Math.abs(A[my_diag_index][row]) > Math.abs(largest_val)) {
                largest_val = A[my_diag_index][row];
                largest_row = row;
            }
        }

        // we now need to swap rows "diag_index" and "largest_row"
        // in the entire matrix A and also in the vector b
        // we'll do our own column first
        if (largest_row != diag_index) {
            A[my_diag_index][largest_row] = swp;
            A[my_diag_index][diag_index] = largest_val;
            if (!JAxeqb.computeYDirectly && VMInfo.THIS_IS_VM == VMInfo.MasterVM) {
                // while we're here, we'll swap rows of b
                double swpb = b[diag_index];
                b[diag_index] = b[largest_row];
                b[largest_row] = swpb;
            }
        }
        
        //if (A[my_diag_index][diag_index] == VMInfo.MasterVM) {
        if (Math.abs(A[my_diag_index][diag_index]) < 1e-32) {
            // This matrix is Singular...Gaussian elimination will not work
            return false;
        } else {
            // now for each row, compute the multipliers
            final double[] multipliers = new double[N - diag_index - 1];
            for (int row = diag_index+1; row < N; ++row) {
                // don't really need to set A[][] ... we always use
                // a temporary "multipliers"
                A[my_diag_index][row] /= A[my_diag_index][diag_index];
                multipliers[row - diag_index - 1] = A[my_diag_index][row];
            }

            // We're going to send the same message to all of the
            // processors.
            for (int vm_no = 0; vm_no < VM_.length; ++vm_no) {
                if (vm_no != VMInfo.THIS_IS_VM) {
                    VM_[vm_no].adjustDone = false;
                } else {
                    VM_[vm_no].adjustDone = true;
                }
            }
            VMInfo.AdjustBelowDiagonal(solverKey, diag_index, largest_row, multipliers);
            // Now we do the scaling, etc. for the columns on this
            // processor.
            AdjustBelowDiagonal(diag_index, largest_row, multipliers);
            
            // And wait until all are done
            boolean allDone;
            do {
                allDone = true;
                for (int vm_no = 0; vm_no < VM_.length; ++vm_no) {
                    if (!VM_[vm_no].adjustDone) {
                        allDone = false;
                        break;
                    }
                }
            } while (!allDone);
        }
        return true;
    }

    // we may need a version of this that happens on an other thread
    void AdjustBelowDiagonal(int diag_index, int largest_row, double[] multipliers) {
        if (largest_row != diag_index) {
            // first we swap all the rows in columns to the left or
            // right of out current column
            for (int col = VMInfo.THIS_IS_VM, xcol = 0; col < N; col += VM_.length, ++xcol) {
                if (col != diag_index) {
                    double swpb = A[xcol /*col / VM_.length*/][diag_index];
                    A[xcol /*col / VM_.length*/][diag_index] = A[xcol /*col / VM_.length*/][largest_row];
                    A[xcol /*col / VM_.length*/][largest_row] = swpb;
                }
            }
            if (JAxeqb.computeYDirectly && VMInfo.THIS_IS_VM == VMInfo.MasterVM) {
                double swpb = y[diag_index];
                y[diag_index] = y[largest_row];
                y[largest_row] = swpb;
            }
        }
        // then we subtract the scaled row for all columns to the
        // right (+ the y vector if we want to compute it directly.)
        if (JAxeqb.computeYDirectly && VMInfo.THIS_IS_VM == VMInfo.MasterVM) {
            for (int row = diag_index+1; row < N; ++row) {
                double m = multipliers[row - diag_index - 1];
                y[row] = y[row] - m * y[diag_index];
            }
        }
        int low_col = diag_index + 1;
        if (VMInfo.THIS_IS_VM < low_col % VM_.length) {
            low_col += VM_.length - ((low_col % VM_.length) - VMInfo.THIS_IS_VM);
        } else {
            low_col += VMInfo.THIS_IS_VM - (low_col % VM_.length);
        }
        for (int col = low_col, xcol = low_col / VM_.length; col < N; col += VM_.length, ++xcol) {
            for (int row = diag_index+1; row < N; ++row) {
                double m = multipliers[row - diag_index - 1];
                A[xcol /*col / VM_.length*/][row] = A[xcol /*col / VM_.length*/][row] - m * A[xcol /*col / VM_.length*/][diag_index];
            }
        }
    }

    void LyeqPb() {
        // We have L (which is I + the multipliers below the diagonal)
        // and Pb, so we can proceed to compute y
        for (int diag_index = 0; diag_index < N; ++diag_index) {
            double yi;
            if ((diag_index % VM_.length) == VMInfo.MasterVM) {
                yi = LyeqPb(diag_index, b[diag_index]);
            } else {
                System.out.println("you can only run this with a single VM");
                assert false;   // this is not supported in LAPI mode
                yi = VM_[diag_index % VM_.length].LyeqPb(solverKey, diag_index, b[diag_index]);
            }
            y[diag_index] = yi;
        }
    }
    
    double LyeqPb(int diag_index, double bi) {
        // We're going to compute yi at the place holding
        // column i.  Once we have it, we'll multiply all
        // of the multipliers (below the main diagonal) by
        // yi.  This could potentially be vectorized which
        // is one reason we do it.  The other is to avoid
        // having to re-fetch the yi value in subsequent steps.
        double yi = bi;

        // We're going to send the same message to all of the processors
        for (int vm_no = 0; vm_no < VM_.length; ++vm_no) {
            if (vm_no != VMInfo.THIS_IS_VM) {
                VM_[vm_no].rowSumIsDone = false;
            } else {
                VM_[vm_no].rowSumIsDone = true;
            }
        }
        VMInfo.rowSum(solverKey, false, diag_index);
        VM_[VMInfo.THIS_IS_VM].thisRowSum = computeRowSum(false, diag_index);
        // And wait until all are done
        boolean allDone;
        do {
            allDone = true;
            for (int vm_no = 0; vm_no < VM_.length; ++vm_no) {
                if (!VM_[vm_no].rowSumIsDone) {
                    allDone = false;
                    break;
                }
            }
        } while (!allDone);
        
        for (int vm_no = 0; vm_no < VM_.length; ++vm_no) {
            yi -= VM_[vm_no].thisRowSum;
        }
        
        for (int row = diag_index+1; row < N; ++row) {
            A[diag_index / VM_.length][row] *= yi;
        }
        return yi;
    }

    void Uxeqy() {
        // We have U (which is the main diagonal and above) and
        // y which was computed in LyeqPb.  Now compute x
        for (int diag_index = N-1; diag_index >= 0; --diag_index) {
            double xi;
            if ((diag_index % VM_.length) == VMInfo.MasterVM) {
                xi = Uxeqy(diag_index, y[diag_index]);
            } else {
                xi = VM_[diag_index % VM_.length].Uxeqy(solverKey, diag_index, y[diag_index]);
            }
            x[diag_index] = xi;
        }
    }

    // When VM 0 tells this VM to do an LUFactorization of a
    // column, that work should be done on a separate thread to
    // allow the LAPI threads to get back to business.
    void UxeqyOnAnotherThread(int diag_index, double yi, int lapi_task_expecting_reply, long address_of_reply) {
        final JAxeqb solver = this;
        final int d_i = diag_index;
        final double y = yi;
        final int lt = lapi_task_expecting_reply;
        final long la = address_of_reply;
        new Thread() {
            public void run() {
                double d = solver.Uxeqy(d_i, y);
                VM_[lt].returnUxeqyValue(d, la);
            }
        }.start();
    }
    
    double Uxeqy(int diag_index, double yi) {
        // We're on the processor holding column i and we're going
        // to compute xi.  Once we have it, we'll multiply all
        // of the values in column i by xi. This could potentially
        // be vectorized which is one reason we do it.  The other is
        // to avoid having to re-fetch the xi value in subsequent steps.
        double xi = yi;
        
        // We're going to send the same message to all of the processors
        for (int vm_no = 0; vm_no < VM_.length; ++vm_no) {
            if (vm_no != VMInfo.THIS_IS_VM) {
                VM_[vm_no].rowSumIsDone = false;
            } else {
                VM_[vm_no].rowSumIsDone = true;
            }
        }
        VMInfo.rowSum(solverKey, true, diag_index);
        VM_[VMInfo.THIS_IS_VM].thisRowSum = computeRowSum(true, diag_index);
        // And wait until all are done
        boolean allDone;
        do {
            allDone = true;
            for (int vm_no = 0; vm_no < VM_.length; ++vm_no) {
                if (!VM_[vm_no].rowSumIsDone) {
                    allDone = false;
                    break;
                }
            }
        } while (!allDone);
        for (int vm_no = 0; vm_no < VM_.length; ++vm_no) {
            xi -= VM_[vm_no].thisRowSum;
        }
        
        xi /= A[diag_index / VM_.length][diag_index];

        for (int row = 0; row < diag_index; ++row) {
            A[diag_index / VM_.length][row] *= xi;
        }
        return xi;
    }

    // compute the (partial) sum of all of the elements I own of
    // row diag_index either to the right of diag_index or to the left
    // We may need a version of this that runs on another thread.
    double computeRowSum(boolean to_the_right, int diag_index) {
        double sum = 0;
        int low_col, high_col;
        if (to_the_right) {
            low_col = diag_index + 1;
            if (VMInfo.THIS_IS_VM < low_col % VM_.length) {
                low_col += VM_.length - ((low_col % VM_.length) - VMInfo.THIS_IS_VM);
            } else {
                low_col += VMInfo.THIS_IS_VM - (low_col % VM_.length);
            }
            high_col = N;
        } else {
            low_col = 0;
            high_col = diag_index;
        }
        for (int col = low_col, xcol = low_col / VM_.length; col < high_col; col += VM_.length, ++xcol) {
            // here is where we would have to multiply by xi or yi
            // if we hadn't done so earlier
            sum += A[xcol /*col / VM_.length*/][diag_index];
        }
        return sum;
    }
    
    double getAat(int col, int row) {
        if ((col % VM_.length) == VMInfo.THIS_IS_VM) {
            return A[col / VM_.length][row];
        } else {
            return VM_[col % VM_.length].getAat(solverKey, col, row);
        }
    }
    
    double check() {
        double rc = 0;
        for (int row = 0; row < N; ++row) {
            double s = 0;
            for (int col = 0; col < N; ++col) {
                double Av = getAat(col,row);
                double xv = x[col];
                s += (Av * xv);
            }
            double bv = b[row];
//            if (debug || Math.abs(s - bv) > 0.00000000001) {
//                System.err.println("s[" + row + "] = " + s +
//                                   " b[" + row + "] = " + bv);
//                if (Math.abs(s - bv) > 0.00000000001) rc = false;
//            }
            rc += Math.abs(s - bv);
        }
        return rc;
    }
    
    void checkLy() {
    }
    
    void print() {
        for (int row = 0; row < N; ++row) {
            for (int col = 0; col < N; ++col) {
                double v = getAat(col,row);
                System.out.print(v + " ");
            }
            double bv = b[row];
            System.out.print(": " + bv);
            double yv = y[row];
            System.out.print(": " + yv);
            double xv = x[row];
            System.out.println(": " + xv);
        }
        System.out.println("--------------------");
    }

    public native long getGlobalKey();
    public native void releaseGlobalKey(long key);

    public static int NUMBER_OF_VMS = 1;
    public static VMInfo[] VM_;
    private static String getConfigurationFileName_() {
        String rc = System.getProperty("lapi.conf");
        if (rc == null) rc = "myConf";
        return rc;
    }
    static {        
        String cfg = getConfigurationFileName_();
        if (cfg != null) {
            try {
                Properties props = new Properties();
                FileInputStream fis = new FileInputStream(cfg);
                byte[] data = new byte[fis.available()];
                if (data.length != fis.read(data))
                    throw new Error();
                String s = new String(data).replace('\\','/');
                props.load(new ByteArrayInputStream(s.getBytes()));
                // arrg... the Iterator isn't always in order of text
                Iterator i = props.keySet().iterator();
                while (i.hasNext()) {
                    String key = (String) i.next();
                    if (key.equals("NUMBER_OF_VMS")) {
                        String val = props.getProperty(key);
                        set(key, val);
                        VM_ = new VMInfo[NUMBER_OF_VMS];
                    }
                }
                
                i = props.keySet().iterator();
                while (i.hasNext()) {
                    String key = (String) i.next();
                    String val = props.getProperty(key);
                    set(key, val);
                } // end of 'for each configuration directive'
            } catch (IOException io) {
                System.err.println("Failed to read configuration file " + cfg + ": " + io);
                throw new Error(io);
            }
        } // end of 'have configuration file'       
    }

    // Borrowed from X10 Runtime.... basically reading configuration file
    // to set up LAPI tasks
    private static void set(String key, String val) {
        Class c = JAxeqb.class;
        int idx=0;
        String fld = null;
        try {
            if (key.indexOf('[') > 0) {
                idx = Integer.parseInt(key.substring(key.indexOf('[')+1,key.indexOf(']')));
                fld = key.substring(key.indexOf('.')+1);
                key = key.substring(0, key.indexOf('['));
            }
            Field f = c.getField(key);
            Class t = f.getType();
            Object o = null;
            if (fld != null) {
                if (t.isArray()) {
                    if (t.getComponentType().isPrimitive()) {
                    } else {
                        o = Array.get(f.get(null), idx);
                        if (o == null) {
                            Array.set(f.get(null), idx, o = t.getComponentType().newInstance());
                        }
                        f = o.getClass().getField(fld);
                        t = f.getType();
                    }
                } else {
                    System.err.println(key + " is not an array");
                }
            }
            if (t == String.class) {
                f.set(o, val);
            } else if (t == Integer.TYPE) {
                f.setInt(o, new Integer(val).intValue());
            } else if (t == Float.TYPE) {
                f.setFloat(o, new Float(val).floatValue());
            } else if (t == Double.TYPE) {
                f.setDouble(o, new Double(val).doubleValue());
            } else if (t == Long.TYPE) {
                f.setLong(o, new Long(val).longValue());
            } else if (t == Short.TYPE) {
                f.setShort(o, new Short(val).shortValue());
            } else if (t == Byte.TYPE) {
                f.setByte(o, new Byte(val).byteValue());
            } else if (t == Character.TYPE) {
                if (val.length() != 1)
                    System.err.println("Parameter" + key + " only takes on character,"+
                                       " using only the first character of configuration"+
                                       " value >>" + val + "<<");
                f.setChar(o, new Character(val.charAt('0')).charValue());
            } else if (t == Boolean.TYPE) {
                if (val.equalsIgnoreCase("true")) {
                    f.setBoolean(null, true);
                } else if (val.equalsIgnoreCase("false")) {
                    f.setBoolean(null, false);
                } else {
                    System.err.println("Parameter |" + key + "| expects a boolean, not |" 
                                       + val + "|. Ignored.");
                }
            }
        } catch (NoSuchFieldException nsfe) {
            System.err.println("Parameter " + key + " not found, configuration directive ignored.");
        } catch (InstantiationException ie) {
            System.err.println("Failed to create object for " + key);
            throw new Error(ie);
        } catch (IllegalAccessException iae) {
            System.err.println("Wrong permissions for field " + key + ": " + iae);
            throw new Error(iae);
        } catch (NumberFormatException z) {
            System.err.println("Parameter |" + key + "| expects a number, not |" + val + "|. Ignored.");
        }
    }
}

final class VMInfo {
    public static final int MasterVM = 0;
    public static native void init(VMInfo[] VM_);
    public static native void term();
    public static VMInfo[] VM_;         // copied from JAxeqb.VM_
    public static int THIS_IS_VM = 0;
    public static boolean shallIShutdown = false;
    public native void newSolver(int N, boolean debug, long key);
    public native void deleteSolver(long key);
    public native void setUpSolver(long key);
    public native boolean LUFactorize(long key, int diag_index);
    public static native void AdjustBelowDiagonal(long key, int diag_index, int largest_row, double[] multipliers);
    public volatile boolean adjustDone;   // this VM has finished AdjustBelow..
    public static native void rowSum(long key, boolean to_the_right, int diag_index);
    public double thisRowSum;             // value returned by rowSum();
    public volatile boolean rowSumIsDone; // set when rowSum() is done
    public native void shutdown();
    public native void returnLUFactorizeValue(boolean b, long address_of_reply);
    public native double Uxeqy(long key, int diag_index, double yi);
    public native double LyeqPb(long key, int diag_index, double yi);
    public native void returnUxeqyValue(double d, long address_of_reply);
    public native double getAat(long key, int col, int row);
    public String          hostName;
    public int             portNumber;
    public int lapiTarget;   // must == index into VM_ table
}
