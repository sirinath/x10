\section{Runtime Support for X10 programs}
\label{s:runtime}

We have implemented a runtime system that supports co-existence of
multiple parallel programming, as envisioned by the X10 programming
model. As an initial implementation, we have developed a Java-based
runtime system to support shared-memory parallelization. It is
distributed as a Java package, {\texttt x10.runtime.cws}, under an
open-source license~\cite{x10-webpage}. In this section, we discuss the
implementation key features of the runtime system.

The computation is organized as collection of {\em tasks}. A task is a
sequence of instructions that can spawn other tasks and wait for
completion of the spawned tasks. The computation begins with a single
task and is considered complete when there are no more tasks executing
in the system. 

Cilk is a popular programming model for shared-memory parallelization
that such computations. Cilk requires {\em fully-strict} programs
in which a task waits for all its descendents to complete before
returning. Such tasks are also called properly nested tasks. 

The X10 runtime system is designed to leverage the Cilk design while
supporting a larger class of programs. X10 provides support for {\em
strict} computations, in which a ancestor task need not wait for its
descendent tasks to be completed. Such tasks are said to be improperly
nested. {\bf (also talk about algorithmic properties such as
deadlock-freedom in this context)}

The execution model consists of a pool of {\em workers} that
co-operatively execute the tasks until termination is detected. A task
is created as an instance of a sub-class of {\texttt
x10.runtime.cws.Frame}. The program begins execution when a driver
thread submits a task a global task queue shared by the workers. 

One of the workers retrieves the task from the global queue and begins
executing it. When a worker does not have tasks to execute it steals
tasks available at other workers. Assuming the computation contains
sufficient parallelism stealing happens infrequently. The design
ensures that there are few overheads during normal execution, referred
to as the fast path. The additional overheads incurred to load-balance
the computation are proportional to the number of steals in the
execution. The design of the worker for the the task types supported
is discussed in subsequent sections.

%The normal execution corresponds to the depth-first
%sequential execution of the tasks spawned. Thus the execution
%corresponds to a sequential execution when there is only one worker. 

\subsection{Task Execution}

Every async in the X10 program is compiled into a sub-class of
Frame. An async is said to consist of threads, where a thread is a
non-blocking sequence of instructions terminating in the spawning of or
waiting on an async. 

Each class created for an async contains as member variables the local
variables used in the async body, a counter ({\em PC}) that specifies
the next instruction in the body of the async after the current
thread.

\begin{figure*}
\begin{minipage}{0.25\textwidth}
\begin{verbatim}
int fib(int n) {
  int x, y;
  if(n<2) return n;
  finish {
    async x=fib(n-1);
    async y=fib(n-2);
  }
  return x+y;
}
    (a)
\end{verbatim}
\end{minipage}%
\begin{minipage}{0.4\textwidth}
\begin{verbatim}
int fib(Worker w, int n) 
 throws StealAbort { 
  int x, y;
  if (n < 2) return n;

  FibFrame frame = new FibFrame(n);
  frame.PC=LABEL_1;
  w.pushFrame(frame);

  x = fib(w, n-1);
  w.abortOnSteal(x);

  frame.x=x;
  frame.PC=LABEL_2;

  y=fib(w, n-2);
  w.abortOnSteal(y);

  w.popFrame();
  return frame.x+y;
}

      (b)
\end{verbatim}
\end{minipage}%
\begin{minipage}{0.45\textwidth}
\begin{verbatim}
void compute(Worker w, 
 Frame frm) throws StealAbort {
  int x, y;
  FibFrame f=(FibFrame)frm;
  int n = f.n;
  switch (f.PC) {
  case ENTRY: 
    if (n < 2) {
      result = n;
      setupReturn();
      return;
    }
    f.PC=LABEL_1;
    x = fib(w, n-1);
    w.abortOnSteal(x);
    f.x=x;
  case LABEL_1: 
    f.PC=LABEL_2;
    int y=fib(w,n-2);
    w.abortOnSteal(y);
    f.y=y;
  case LABEL_2: 
    f.PC=LABEL_3;
    if (sync(w)) return;
  case LABEL_3:
    result=f.x+f.y;
    setupReturn();
  }
}
       (c)
\end{verbatim}
\end{minipage}%
\caption{(a) X10 program for Fibonacci. (b) Fast version. (c) Slow version}%
\label{fig:fib-ill}
\end{figure*}

Two versions of the async body are created -- the fast and the slow
versions. The fast version of the program is executed during the
normal course of the program. The slow version is invoked by the
worker to initiate processing of a task. Fig.~\ref{fig:fib-ill} shows
the fast and slow versions for the Fibonacci method shown in
Fig.~\ref{fig:fib-ill}(a).

Each worker maintains a deque consisting of stack of Frames. On
entering a fast version, a frame object is created and pushed onto the
stack (method: {\texttt pushFrame}). This frame contains the
up-to-date values of variables whenever any other worker might steal
this task. This frame is popped from the stack (method: {\texttt
popFrame}) when this method returns.

Whenever a task is spawned, the worker immediately proceeds to execute
the spawned task like a sequential method call. When the spawned task
finishes execution, the worker checks whether the current frame was
stolen. If so, the result computed by the child task is stored to be
passed onto its parent and the execution of the task aborts (method:
{\texttt abortOnSteal}). The method call stack is unwound by throwing
an exception ({\texttt StealAbort}) that is caught in the main
routine executed by the worker. The values in the frame of the parent
stack are updated before proceeding to execute a spawned child, as the
worker now has a non-executing task and hence is target of a steal. 

All the descendents are guaranteed to be completed in the fast version
when a {\texttt finish} is encountered. Hence the finish statements
are ignored.

The slow version restores any local variables and uses the {\em PC} to
start execution of the task past the execution point at which it might
have been stolen. A task might have been stolen when its descendents
are executing. Thus a {\em finish} statement, translated to the
{\texttt sync} method, might cause the invoking task to suspend on
completion of non-terminated children. The value of the {\texttt
  result} variable is returned to the parent task on invocation of the
{\texttt setupReturn} method. 

In a program with sufficient parallelism, the slow version is expected
to execute infrequently. The design tries to minimize the overheads in
the fast version even at the expense of the slow version. 

\subsection{Support for Properly Nested Tasks}

Each worker contains a dequeue of closures. Closures are objects used
to return values from the spawned tasks to their parents in the
presence of work stealing.  

Each closure maintains a stack of frames. Frames corresponding to
spawned tasks are pushed into the stack on entry, and popped on
return. In the fast path, return values are propagated as they would
be in a sequential program. A stealing worker, referred to as the
{thief}, steals a closure together with the bottom-most available
frame, in another worker, referred to as the {\em victim}.

When a task, in the form of the bottom-most frame in a closure, is
stolen, its descendents continue to execute. In order to return values
from the descendents to the parent, a new closure is created that on
completion returns the result to the parent closure that was stolen. 

Thus the closures form a tree of return value propagation
corresponding to the steal operation performed. Termination can be
detected when the closure corresponding to the task inserted by the
driver thread returns. 

The procedure executed by the workers to handle properly nested tasks
is shown in Fig.~\ref{fig:worker-code}(a). On completing execution of
a closure, a worker first attempts to obtain another closure from its
local queue (method:{\texttt extractBottom}). If no local closure is
available to execute, the worker attempts to obtain a task either by
stealing or from the global queue (method:{\texttt getTask}). It then
executes the slow version of the task obtained (method:{\texttt
  execute}). 

This is the support provided by Cilk. We subsequently look at the
improvements to the basic model.


\subsection{Support for Improperly Nested Tasks}

Properly nested tasks t satisfy the property that at the moment when
the slow version terminates (method:{\texttt compute}) the frame at
the bottom of the worker's dequeue is t. Hence the task can be
completed (i.e., removed from the dequeue) by including a {\texttt
w.popFrame()} call at the end of the compute method. In essence, if a
worker is executing only properly nested tasks (this is true when it
is executing Cilk code), there is a one-to-one correspondence between
the frame stack and the tasks being processed.

X10 permits improperly nested tasks. Such tasks q are used, for
instance, to implement the pseudo-depth-first search discussed in this
paper. Such a task may add a task r to the deque of its worker (say w)
without necessarily transferring control to r. This has two
consequences. First, recall that as soon as a worker's dequeue
contains more than one task the worker may be the target of a
theft. Therefore as soon as q pushes r onto w's dequeue, q is
available to be stolen.  Therefore q's compute method must record the
fact that is computation has begun so that the stealing worker z may
do the right thing. For instance, if q's compute method does not
contain any internal suspension point then z must immediately
terminate execution of q and pop q off its deque. This can be
accomplished by defining a volatile int PC field in q, and adding the
following code at the beginning of q's compute method

\begin{verbatim}
if (PC==1) {
 w.popStackFrame();
 return;
}
PC=1;
\end{verbatim}

Second for an improperly nested task when control returns from g's
compute method, it may not be the case that the last frame on the
dequeue is g. Therefore a call to popStackFrame() *at the end* of g's
compute method would be incorrect. Instead, the compute method returns
(without attempting to pop the last frame on the deque). Now whenever
the task reaches the bottom of the dequeue, the worker will, as usual,
invokes its compute method. However, the code sequence described above
will execute, thereby popping the frame from the deque. Thus the code
sequence above serves two purposes -- it does the cleanup necessary
when the task is stolen as well as when it is completed.

The changes to the basic worker code necessary to support improperly
nested tasks are shown in Fig.~\ref{fig:worker-code}(b). With
improperly nested tasks, a worker no long enjoys the property that
when control returns to it from the invocation of an execute method on
the top-level task, the deque is empty. Indeed, control may return to
the scheduler leaving several tasks on the deque, including the task
whose execute method has just returned. The scheduler must now enter a
phase in which it executes the task at the bottom of the deque:

\begin{figure*}
\begin{minipage}{0.5\textwidth}
\begin{verbatim}
public void run() {
  Executable cl=null; //frame/closure
  int yields = 0;
  while (!done) {
    if (cl == null ) {
      //Extract work from local queue.
      //It will be a closure
      //cl may be null. When non-null
      //cl is typically RETURNING.
      lock(this);
      try {
        cl = extractBottom(this);
      } finally {
        unlock();
      }
    }
  }
  if (cl == null)
    //Steal or get from global queue
    cl = getTask(true);  
  if (cl !=null) {
    // Found some work! Execute it.
    Executable cl1 = cl.execute(this);
    cl=cl1;
    cache.reset();
  } else Thread.yield();
}
                (a)
\end{verbatim}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\begin{verbatim}
public void run() {
  Executable cl=null; //frame or closure
  int yields = 0;
  while (!done) {
    if (cl == null ) {
      //Addition for GlobalQuiescence. 
      //Keep executing current frame 
      //until dequeue becomes empty.
      if (jobMayHaveImproperTask) {
        Cache cache = this.cache;
        for(;;) {
          if(!cache.empty())
            Frame f=cache.currentFrame();
          if (f == null) break;
          Executable cl1=f.execute(this);
          if (cl1 != null) {
            cl=cl1;
            break;
          }
        }
      } 
      //Rest of worker code same as for
      //properly nested tasks ...
    }
  }
}
                 (b)
\end{verbatim}
\end{minipage}
\caption{Code executed by workers for (a) only properly nested tasks (b)
  properly and improperly nested tasks. Note that (b) is an extension of (a)}
\label{fig:worker-code}
\end{figure*}


\subsection{Work Stealing}


%% In the fast version of an async, any modified local variables are
%% copied back into the Frame object before entering a spawned
%% child. This ensures that the updated values are available to any thief
%% that may steal the parent frame before the execution of the current
%% worker can return to it. On return from a spawned task, the task clone
%% checks for 

The frames corresponding to the tasks form a stack. We denote the head
and tail of the task by $H$ and $T$, respectively. When a task is
spawned, the corresponding frame is pushed into the head of the stack
($H=H+1$). When a worker returns from a spawned task is checks that
whether the current frame is stolen by executing one-half Dekker:

\begin{verbatim}
  --T; 
  StoreLoadBarrier; // implied in Java if T is volatile
  if (H >= T) 
  // Stolen 
  else 
  // Not stolen
\end{verbatim}

When a worker is out of work, it randomly selects a victim and
attempts to steal from its stack's head. This implies that a task is
stolen only if all its parents have already been stolen. 

is the checking whether the parent frame, corresponding to the
spawning task, has been stolen. This is done in a lock-free manner as
discussed below, to minimize overheads during the normal course of the
program.




Checking for stealing is done through the Dekker's algorithm ....

Dekker -----


\subsection{Global Quiescence}

(stub)

In fully-strict computations, i.e., those involving properly nested
tasks, completion of the first task and the return of the
corresponding Closure indicates computation termination. For
improperly nested tasks, we present a mechanism to efficiently
identify termination. 

The action to be performed on termination is registered with an object
that also maintains an active worker counter. When a worker begins
stealing, it checks in with the counter, and checks out when it has
work to execute. When all workers are identify to be stealing, when
the counter reaches number of workers, the computation is identified
as terminated. 


\subsection{Explicitly Partitioned Programs}

(stub)

While work-stealing provides a convenient abstraction for
load-balancing fine-grained programs, the performance of certain
applications can benefit from the explicitly partitioned programs that
would exist as a typical parallel global address space program. For
example, in the Shiloach-Vishkin algorithm ...

This is achieved in cws by ...



\subsection{Performance Analysis}

In this section, we discuss the performance implications of the
different components of the runtime. 

The overheads of the fast version over the sequential execution are:

\begin{enumerate}
\item Method's frame needs to be allocated, initialized, pushed onto
  the deque. Cost: A few assembly instructions.
\item Method's state needs to be saved before each spawn. Cost: writes of
  local dirty variables and {\em PC}, and a StoreLoad barrier
\item Method must check if its frame has been stolen after each
  return from a spawn. Cost: two reads, compare, branch and a 
  StoreLoad barrier 
\item On return, frame must be freed. Cost: A few assembly instructions
\item An extra variable is needed to hold the frame pointer. Cost: Increased
  register pressure.
\end{enumerate}

Note that average cost of allocating and de-allocating memory can be
reduced to a couple of statements with an efficient concurrent memory
allocation scheme. Thus the overhead in the fast path is very
little, as is also demonstrated in the experimental evaluation.

The number of closures created and invocations of the slow version of
an async is proportional to the number of successful steals. The
number of locks requested is proportional to the numbers of attempted
steals. 

Improperly nested tasks take advantage of the lack of return value to
avoid the creation of closures, further reducing overhead. 

The computation is identified as terminated, the moment the last
worker starts trying to steal. Thus there is no significant delay
between the actual termination of the computation and its detection.
The mechanism itself incurs the overhead of two atomic updates to a
shared counter for every successful steal. 

Explicitly partitioned programs -- overheads???

Thus the common execution path in which all workers are busy involves
little overhead. The remaining in other execution paths are
proportional to the number of attempted and successful steals
performed by the workers. The experimental evaluation section
demonstrates that both are far lower than the number of asyncs
spawned, effectively enabling load-balanced execution of fine-grained
parallel programs.

