
\section{Introduction}
\label{s:intr}

 Graph theoretic problems arise in traditional and emerging scientific disciplines such as VLSI design, optimization, databases, and computational biology. There are plenty of theoretically fast parallel algorithms, for example, work-time optimal PRAM algorithms, for graph problems; however, in
 practice few parallel implementations beat the best sequential implementations for arbitrary, sparse
 graphs. The mismatch between theory and practice suggests a large gap between algorithmic model and the actual architecture. We observe that the gap is increasing as new diversified architectures emerge. Elegant solutions with high performance seem hard to come by even from combined efforts of algorithmic and architectural improvement. 

Due to their irregular and combinatorial nature, large-scale graph problems are challenging to solve in parallel. Many important real world graph instances, for example, the Internet, social interaction networks, transportation networks, and protein-protein interaction networks, are irregular. These graphs can be modeled as `scale-free'' graphs \cite{CZF04}. For random and scale-free graphs no known efficient partitioning technique exists, which makes them extremely hard to solve on distributed-memory systems. Moreover, the irregular memory access pattern dictated by the input instances is not cache-friendly. Obtaining high performance on shared-memory systems is challenging. Compared with their numerical counterparts, parallel graph algorithms take drastically different approaches than the sequential algorithms, and usually employ fine-grained parallelism. For example, depth-first search (DFS) or breadth-first search (BFS) are two popular sequential algorithms for the spanning tree problem. Many parallel spanning tree algorithms,  represented by the Shiloach-Vishkin algorithm \cite{SV82}, take the ``graft-and-shortcut'' approach, and provide massive amount of fine-grained parallelism in the order of $O(n)$. In the absence of efficient scheduling support of parallel activities, fine-grained parallelism incurs large overhead on current systems, and oftentimes the algorithms do not show practical performance advantage. Graph algorithms also tend to be load/store intensive \cite{G06}, and they lay great pressure on the memory subsystem. It gets even worse on distributed-memory architectures if necessary task management and memory affinity scheduling are not provided.  
 
Features of \Xten{} such as shared-memory interface, asynchronous parallelism, and runtime task scheduling make it ideal for solving large-scale graph problems. The shared-memory address space obviates the need to partition a graph and issue requests explicitly to access remote data. Without this, writing code for  irregular graph algorithms is daunting. In fact, none of the SSCA \cite{KK05} graph benchmarks has implementations on distributed-memory systems. \Xten{} in addition allows specifying the location of a parallel activity so that the affinity between tasks and data is exploited. 
Efficient mapping of fine-grained parallelism to target architectures with high performance is the highlight of the \Xten{} programming model. \Xten{} provides a rich collection of programming constructs that may be used to express various levels of parallelism and synchronization scheme. Depending on the input and the target systems, different algorithms can be easily implemented to fit with the architecture. \Xten{} runtime manages parallel activities effectively with low cost.
\Xten{} effectively helps reduce the gap between theory and practice in solving large-scale graph problems. With \Xten{} fine-grained parallelism is elegantly expressed, and compared with the best native C implementation, \Xten{} program achieves comparable, and sometimes even better performance. 


 In this paper we present solving irregular graph problems in \Xten{} on a cluster of SMPs. As most current and emerging supercomputers are clusters of SMPs, it is important to solve the problems efficiently on these platforms. As \Xten{} is an on-going project, we present performance results on SMP nodes as across-node runtime support is still in development. The algorithms we consider include both PRAM algorithms and efficient algorithms that based on more realistic models such as the SMP model \cite{HJ01}. PRAM algorithms are synchronous and provide massive amount of parallelism. The other types of algorithms are either asynchronous or bulk-synchronous with limited amount of parallelism that maps well to architectures with a moderate number of processors. Both classes of algorithms can be expressed and implemented efficiently in \Xten{}.

As an example, we consider the spanning tree problem. Despite dozens of parallel spanning tree algorithms, it is notoriously hard to achieve good parallel performance \cite{BC04a}. We design and/or implement in \Xten{} three parallel algorithms that are representative of different algorithmic approaches. The performance is comparable to or better than the best known prior implementations. Moreover, the algorithm expressed in \Xten{} code is concise and elegant.

 The rest of the paper is organized as follows. Section~\ref{s:x10} describes the language features of \Xten{}. Section~\ref{s:design} presents spanning tree algorithms in \Xten{}. Section~\ref{s:runtime} presents the runtime support for \Xten{}, especially for activity scheduling, with comparison to other runtime systems. 
 Section~\ref{s:results} gives our experimental results. In Section~\ref{s:concl} we conclude and describe future work. 
 %% Throughout the paper, we
%%  use $n$ and $m$ to denote the number of vertices and the number of
%%  edges of an input graph $G=(V,E)$, respectively. 
  