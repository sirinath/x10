
\section{Introduction}
\label{s:intr}

 Graph theoretic problems arise in traditional and emerging scientific disciplines such as VLSI design, optimization, databases, and computational biology. There are plenty of theoretically fast parallel algorithms, for example, work-time optimal PRAM algorithms, for graph problems; however, in
 practice few parallel implementations beat the best sequential implementations for arbitrary, sparse
 graphs. The mismatch between theory and practice suggests a large gap between algorithmic model and the actual architecture. We observe that the gap is increasing as new diversified architectures emerge. Elegant solutions with high performance seem hard to come by from even combined efforts of algorithmic and architectural improvement. 

Due to their irregular and combinatorial nature, large-scale graph problems are challenging to solve in parallel. Many important real world graph instances, for example, the Internet, social interaction networks, transportation networks, and protein-protein interaction networks, are irregular. These graphs can be modeled as `scale-free'' graphs \cite{CZF04}. For random and scale-free graphs no known efficient partitioning technique exists, which makes them extremely hard to solve on distributed-memory systems. Moreover, the irregular memory access pattern dictated by the input instances is not cache-friendly. Obtaining high performance on shared-memory systems is challenging. Compared with their numerical counterparts, parallel graph algorithms take drastically different approaches than the sequential algorithms, and usually employ fine-grained parallelism. For example, depth-first search (DFS) or breadth-first search (BFS) are two popular sequential algorithms for the spanning tree problem. Many parallel spanning tree algorithms,  represented by the Shiloach-Vishkin algorithm \cite{SV82}, take the ``graft-and-shortcut'' approach, and provide massive amount of fine-grained parallelism in the order of $O(n)$. In the absence of efficient scheduling support of parallel activities, fine-grained parallelism incurs large overhead on current systems, and oftentimes the algorithms do not show practical performance advantage. Graph algorithms also tend to be load/store intensive \cite{G06}, and they lay great pressure on the memory subsystem. It gets even worse on distributed-memory architectures if necessary task management and memory affinity scheduling are not provided.  
 
Features of X10 such as shared-memory interface, asynchrous parallelism, and runtime task scheduling make it ideal for solving large-scale graph problems. The shared-memory address space obviates the need to partition a graph and issue requests explicitly to access remote data. Otherwise implementation itself for irregular graph algorithms is daunting. In fact, none of the SSCA \cite{KK05} graph benchmarks has implementations on distributed-memory systems. X10 in addition allows specifying the location of a parallel activity so that the affinity between tasks and data is exploited. 
Efficient mapping of fine-grained parallelism to target architectures with high performance is the highlight of the X10 programming model. X10 provides a rich collection of programming constructs that may be used to express various levels of parallelism and synchronization scheme. Depending on the input and the target systems, different algorithms can be easily implemented to fit with the architecture. X10 runtime manages parallel activities effectively with low cost.
X10 effectively helps reduce the gap between theory and practice in solving large-scale graph problems. With X10 fine-grained parallelism is elegantly expressed, and compared with the best native C implementation, X10 program achieves comparable, and sometimes even better performance. 


 In this paper we present solving irregular graph problems in X10 on a cluster of SMPs. As most current and emerging supercomputers are clusters of SMPs, it is important to solve the problems efficiently on these platforms. The algorithms we consider include both PRAM algorithms and efficient algorithms that based on more realistic models such as the SMP model \cite{HJ01}. PRAM algorithms are synchronous and provide massive amount of parallelism. The other types of algorithms are either asynchronous or bulk-synchronous with limited amount of parallelism that maps well to architectures with a moderate number of processors. Both classes of algorithms can be expressed and implemented efficiently in X10.

As an example, we consider the spanning tree problem. Despite dozens of parallel spanning tree algorithms, it is notoriously hard to achieve good parallel performance \cite{BC04a}. We design and/or implement in X10 three parallel algorithms that are representative of different algorithmic approaches. The performance is comparable to or better than the best known prior implementations. Moreover, the algorithm expressed in X10 code is concise and elegant.

 The rest of the paper is organized as follows. Section~\ref{s:x10} describes the language features of X10. Section~\ref{s:design} presents spanning tree algorithms in X10. Section~\ref{s:runtime} presents the runtime support for X10, especially for activity scheduling, with comparison to other runtime systems. 
 Section~\ref{s:results} gives our experimental results. In Section~\ref{s:concl} we conclude and give future work. 
 %% Throughout the paper, we
%%  use $n$ and $m$ to denote the number of vertices and the number of
%%  edges of an input graph $G=(V,E)$, respectively. 
  


