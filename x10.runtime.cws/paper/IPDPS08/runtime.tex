\section{Runtime Support for \Xten{} programs}
\label{s:runtime}

We have implemented a runtime system that supports co-existence of
multiple parallel programming, as envisioned by the \Xten{} programming
model. As an initial implementation, we have developed a Java-based
runtime system to support shared-memory parallelization. It is
distributed as a Java package, {\java x10.runtime.cws}, under an
open-source license~\cite{x10-webpage}. In this section, we discuss the
implementation key features of the runtime system.

The computation is organized as collection of {\em tasks}. A task is a
sequence of instructions that can spawn other tasks and wait for
completion of the spawned tasks. The computation begins with a single
task and is considered complete when there are no more tasks executing
in the system. 

Cilk is a popular programming model for shared-memory parallelization
that such computations. Cilk requires {\em fully-strict} programs
in which a task waits for all its descendents to complete before
returning. Such tasks are also called properly nested tasks. 

The \Xten{} runtime system is designed to leverage the Cilk design while
supporting a larger class of programs. \Xten{} provides support for {\em
strict} computations, in which a ancestor task need not wait for its
descendent tasks to be completed. Such tasks are said to be improperly
nested. {\bf (also talk about algorithmic properties such as
deadlock-freedom in this context)}

The execution model consists of a pool of {\em workers} that
co-operatively execute the tasks until termination is detected. A task
is created as an instance of a sub-class of {\java x10.runtime.cws.Frame}. The program begins execution when a driver
thread submits a task a global task queue shared by the workers. 

One of the workers retrieves the task from the global queue and begins
executing it. When a worker does not have tasks to execute it steals
tasks available at other workers. Assuming the computation contains
sufficient parallelism stealing happens infrequently. The design
ensures that there are few overheads during normal execution, referred
to as the fast path. The additional overheads incurred to load-balance
the computation are proportional to the number of steals in the
execution. The design of the worker for the the task types supported
is discussed in subsequent sections.

%The normal execution corresponds to the depth-first
%sequential execution of the tasks spawned. Thus the execution
%corresponds to a sequential execution when there is only one worker. 

\subsection{Task Execution}

Every async in the \Xten{} program is compiled into a sub-class of
Frame. An async is said to consist of threads, where a thread is a
non-blocking sequence of instructions terminating in the spawning of or
waiting on an async. 

Each class created for an async contains as member variables the local
variables used in the async body, a counter ({\em PC}) that specifies
the next instruction in the body of the async after the current
thread.

\begin{figure*}
\begin{minipage}{0.25\textwidth}
\scriptsize
\begin{verbatim}
int fib(int n) {
  int x, y;
  if(n<2) return n;
  finish {
    async x=fib(n-1);
    async y=fib(n-2);
 }
  return x+y;
}
    (a)
\end{verbatim}
\end{minipage}%
\begin{minipage}{0.4\textwidth}
\scriptsize
\begin{verbatim}
int fib(Worker w, int n) 
 throws StealAbort { 
  int x, y;
  if (n < 2) return n;

  FibFrame frame = new FibFrame(n);
  frame.PC=LABEL_1;
  w.pushFrame(frame);

  x = fib(w, n-1);
  w.abortOnSteal(x);

  frame.x=x;
  frame.PC=LABEL_2;

  y=fib(w, n-2);
  w.abortOnSteal(y);

  w.popFrame();
  return frame.x+y;
}

      (b)
\end{verbatim}
\end{minipage}%
\begin{minipage}{0.45\textwidth}
\scriptsize
\begin{verbatim}
void compute(Worker w, 
 Frame frm) throws StealAbort {
  int x, y;
  FibFrame f=(FibFrame)frm;
  int n = f.n;
  switch (f.PC) {
  case ENTRY: 
    if (n < 2) {
      result = n;
      setupReturn();
      return;
   }
    f.PC=LABEL_1;
    x = fib(w, n-1);
    w.abortOnSteal(x);
    f.x=x;
  case LABEL_1: 
    f.PC=LABEL_2;
    int y=fib(w,n-2);
    w.abortOnSteal(y);
    f.y=y;
  case LABEL_2: 
    f.PC=LABEL_3;
    if (sync(w)) return;
  case LABEL_3:
    result=f.x+f.y;
    setupReturn();
 }
}
       (c)
\end{verbatim}
\end{minipage}%
\caption{(a) \Xten{} program for Fibonacci. (b) Fast version. (c) Slow version}%
\label{fig:fib-ill}
\end{figure*}

Two versions of the async body are created -- the fast and the slow
versions. The fast version of the program is executed during the
normal course of the program. The slow version is invoked by the
worker to initiate processing of a task. Fig.~\ref{fig:fib-ill} shows
the fast and slow versions for the Fibonacci method shown in
Fig.~\ref{fig:fib-ill}(a).

Each worker maintains a deque consisting of stack of Frames. On
entering a fast version, a frame object is created and pushed onto the
stack (method: {\java pushFrame()}). This frame contains the
up-to-date values of variables whenever any other worker might steal
this task. This frame is popped from the stack (method: {\java popFrame()}) 
when this method returns.

Whenever a task is spawned, the worker immediately proceeds to execute
the spawned task like a sequential method call. When the spawned task
finishes execution, the worker checks whether the current frame was
stolen. If so, the result computed by the child task is stored to be
passed onto its parent and the execution of the task aborts (method:
{\java abortOnSteal()}). The method call stack is unwound by throwing
an exception ({\java StealAbort}) that is caught in the main
routine executed by the worker. The values in the frame of the parent
stack are updated before proceeding to execute a spawned child, as the
worker now has a non-executing task and hence is target of a steal. 

All the descendents are guaranteed to be completed in the fast version
when a {\java finish} is encountered. Hence the finish statements
are ignored.

The slow version restores any local variables and uses the {\em PC} to
start execution of the task past the execution point at which it might
have been stolen. A task might have been stolen when its descendents
are executing. Thus a {\em finish} statement, translated to the
{\java sync()} method, might cause the invoking task to suspend on
completion of non-terminated children. The value of the {\java result} 
variable is returned to the parent task on invocation of the
{\java setupReturn()} method. 

In a program with sufficient parallelism, the slow version is expected
to execute infrequently. The design tries to minimize the overheads in
the fast version even at the expense of the slow version. 

\subsection{Support for Properly Nested Tasks}

Each worker contains a dequeue of closures. Closures are objects used
to return values from the spawned tasks to their parents in the
presence of work stealing.  

Each closure maintains a stack of frames. Frames corresponding to
spawned tasks are pushed into the stack on entry, and popped on
return. In the fast path, return values are propagated as they would
be in a sequential program. A stealing worker, referred to as the
{\java thief}, steals a closure together with the bottom-most available
frame, in another worker, referred to as the {\em victim}.

When a task, in the form of the bottom-most frame in a closure, is
stolen, its descendents continue to execute. In order to return values
from the descendents to the parent, a new closure is created that on
completion returns the result to the parent closure that was stolen. 

Thus the closures form a tree of return value propagation
corresponding to the steal operation performed. Termination can be
detected when the closure corresponding to the task inserted by the
driver thread returns. 

The procedure executed by the workers to handle properly nested tasks
is shown in Fig.~\ref{fig:worker-code}(a). On completing execution of
a closure, a worker first attempts to obtain another closure from its
local queue (method:{\java extractBottom()}). If no local closure is
available to execute, the worker attempts to obtain a task either by
stealing or from the global queue (method:{\java getTask()}). It then
executes the slow version of the task obtained (method:{\java
  execute()}). 

This is the support provided by Cilk. We subsequently look at the
improvements to the basic model.


\subsection{Support for Improperly Nested Tasks}

Properly nested tasks t satisfy the property that at the moment when
the slow version terminates (method:{\java compute()}) the frame at
the bottom of the worker's dequeue is t. Hence the task can be
completed (i.e., removed from the dequeue) by including a {\java
w.popFrame()} call at the end of the compute method. In essence, if a
worker is executing only properly nested tasks (this is true when it
is executing Cilk code), there is a one-to-one correspondence between
the frame stack and the tasks being processed.

\Xten{} permits improperly nested tasks. Such tasks q are used, for
instance, to implement the pseudo-depth-first search discussed in this
paper. Such a task may add a task r to the deque of its worker (say w)
without necessarily transferring control to r. This has two
consequences. First, recall that as soon as a worker's dequeue
contains more than one task the worker may be the target of a
theft. Therefore as soon as q pushes r onto w's dequeue, q is
available to be stolen.  Therefore q's compute method must record the
fact that is computation has begun so that the stealing worker z may
do the right thing. For instance, if q's compute method does not
contain any internal suspension point then z must immediately
terminate execution of q and pop q off its deque. This can be
accomplished by defining a volatile int PC field in q, and adding the
following code at the beginning of q's compute method

{\scriptsize
\begin{verbatim}
  if (PC==1) {
    w.popFrame();
    return;
  }
  PC=1;
\end{verbatim}
}

Second for an improperly nested task when control returns from g's
compute method, it may not be the case that the last frame on the
dequeue is g. Therefore a call to {\java popFrame()} at the end of
g's compute method would be incorrect. Instead, the compute method
returns (without attempting to pop the last frame on the deque). Now
whenever the task reaches the bottom of the dequeue, the worker will,
as usual, invokes its compute method. However, the code sequence
described above will execute, thereby popping the frame from the
deque. Thus the code sequence above serves two purposes -- it does the
cleanup necessary when the task is stolen as well as when it is
completed.

The changes to the basic worker code necessary to support improperly
nested tasks are shown in Fig.~\ref{fig:worker-code}(b). With
improperly nested tasks, a worker no long enjoys the property that
when control returns to it from the invocation of an execute method on
the top-level task, the deque is empty. Indeed, control may return to
the scheduler leaving several tasks on the deque, including the task
whose execute method has just returned. The scheduler must now enter a
phase in which it executes the task at the bottom of the deque:

\begin{figure*}
\begin{minipage}{0.5\textwidth}
\scriptsize
\begin{verbatim}
public void run() {
  Executable cl=null; //frame/closure
  int yields = 0;
  while (!done) {
    if (cl == null ) {
      //Extract work from local queue.
      //It will be a closure
      //cl may be null. When non-null
      //cl is typically RETURNING.
      lock(this);
      try {
        cl = extractBottom(this);
      } finally {
        unlock();
      }
    }
  }
  if (cl == null)
    //Steal or get from global queue
    cl = getTask(true);  
  if (cl !=null) {
    // Found some work! Execute it.
    Executable cl1 = cl.execute(this);
    cl=cl1;
    cache.reset();
  } else Thread.yield();
}
                (a)
\end{verbatim}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\scriptsize
\begin{verbatim}
public void run() {
  Executable cl=null; //frame or closure
  int yields = 0;
  while (!done) {
    if (cl == null ) {
      //Addition for GlobalQuiescence. 
      //Keep executing current frame 
      //until dequeue becomes empty.
      if (jobMayHaveImproperTask) {
        Cache cache = this.cache;
        for(;;) {
          if(!cache.empty())
            Frame f=cache.currentFrame();
          if (f == null) break;
          Executable cl1=f.execute(this);
          if (cl1 != null) {
            cl=cl1;
            break;
          }
        }
      } 
      //Rest of worker code same as for
      //properly nested tasks ...
    }
  }
}
                 (b)
\end{verbatim}
\end{minipage}
\caption{Code executed by workers for (a) only properly nested tasks (b)
  properly and improperly nested tasks. Note that (b) is an extension of (a)}
\label{fig:worker-code}
\end{figure*}

\begin{figure*}
\begin{minipage}{0.5\textwidth}
\scriptsize
\begin{verbatim}
Closure steal(Worker thief) {
  lock(thief); //lock victim deque
  Closure cl = peekTop(thief, victim);
  if (cl==null) 
    return null; //nothing to steal
  //cl = Closure that can be stolen
  cl.lock(thief);
  Status status = cl.status();
  if (status == READY) {
    //Closure not processed by victim
    //steal the Closure
  }
  else if (status == RUNNING) {
    //Possible contention with victim
    //Need to steal head frame in Closure
    if (cache.dekker(thief)) {
      //>1 Frame available in Closure
      //Promote child frame to Closure 
      //Steal this Closure & head frame
    } 
  }
  return null; //No Closure to steal
}
         (a)
\end{verbatim}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\scriptsize
\begin{verbatim}
Frame stealFrame(Worker thief) {
  Worker victim = this;
  lock(thief);

  //>1 frame in victm's frame stack?
  boolean b=cache.dekker(thief);
  if (b) {
    //Frame available to steal
    Frame frame = cache.headFrame();
    //Mark this frame as stolen
    cache.incHead(); //H=H+1
    return frame;
  }
  return null; //No frame to steal
}
          (b) 
\end{verbatim}
\end{minipage}
\caption{Work stealing algorithm for (a) Properly nested tasks (b)
  Improperly nested tasks. Both are invoked on victim's
  Worker object (victim==this). Locks held are freed before returning.}%
\label{fig:stealing-alg}
\end{figure*}



\subsection{Work Stealing}


%% In the fast version of an async, any modified local variables are
%% copied back into the Frame object before entering a spawned
%% child. This ensures that the updated values are available to any thief
%% that may steal the parent frame before the execution of the current
%% worker can return to it. On return from a spawned task, the task clone
%% checks for 

The frames corresponding to the tasks form a stack. We denote the head
and tail of the task by $H$ and $T$, respectively. When a task is
spawned, the corresponding frame is pushed into the head of the stack
($H=H+1$). When a worker returns from a spawned task is checks that
whether the current frame is stolen by executing one-half of lock-free
Dekker's algorithm~\cite{dekker}:

{\scriptsize
\begin{verbatim}
  --T; 
  StoreLoadBarrier;
  if (H >= T) 
    // Stolen 
  else 
    // Not stolen
\end{verbatim}
}

Note that the {\java StoreLoadBarrier} is implied in Java if
{\java T} is declared to be {\java volatile}.

When a worker is out of work, it randomly selects a victim and
attempts to steal from its frame stack. The procedures employed to
steal in the case of properly- and improperly-nested tasks are shown
in Fig.~\ref{fig:stealing-alg}(a) and Fig.~\ref{fig:stealing-alg}(b),
respectively. Note that the victim, and multiple thieves might attempt
to operate on the same frame stack. The thief first obtains a lock on
the victim's deque to avoid contention with other thieves trying to
steal from the same victim.

When properly nested tasks are being processed, the thief identifies a
closure at the end of the deque and locks it. If the victim is
processing some other closure and this closure is in a {\java READY}
state, there is no contention with the victim on this closure. The
thief extracts the Closure and steals it. If the closure is in
{\java RUNNING} state, the victim is potentially adding and deleting
frames on the frame stack associated with the closure. Dekker's
algorithm is used to determine whether there is more than one frame in
the frame stack. If there is, the locked closure together with the
frame at the head of the stack are stolen. The immediate child frame
of the stolen frame is promoted to a closure which is left in the
victim's deque. The child task returns its result to the parent task
through this promoted closure.

In computations on improperly nested tasks, there is only one stack of
frames used to represent the tasks. The thief directly locks the stack
of frames (labeled {\java cache} in the algorithm) and attempts to
steal from the stack's head when more than one tasks in available in
the task. The frame is marked as stolen by incrementing the stack's
head. 

Since the tasks are pushed at the tail, the algorithm implies that a
task is stolen only if all its parents have already been
stolen. Parent tasks represent more work than child tasks, since they
have potential to generate a greater number of tasks. The algorithms
thus favor stealing of tasks that represent a large portion of work
rather than fine-grained descendent tasks that do limited
processing. This leads to better load balancing of the computation and
reduces the number of steal attempts by workers.



\subsection{Global Quiescence}

In fully-strict computations, i.e., those involving properly nested
tasks, completion of the first task and the return of the
corresponding Closure indicates computation termination.
Improperly-nested tasks that do not require a return call chain can do
away with the closures. We have implemented a mechanism to efficiently
identify termination without closures.

The workers share a barrier. The barrier is used to determine when all
workers are out of work. Every worker notifies the barrier of its
state through two methods. {\java checkIn()} is used to enter the
barrier barrier and notify that the worker is out of work. When such a
worker steals work from a victim, it invokes {\java checkOut()} to
leave the barrier. The barrier maintains a {\java checkoutCount} on
the number of workers checked out. It is triggered when all the workers
are in it. The action associated with the barrier is triggered and it
signals that the computation has terminated.

The algorithm maintains the invariant:

\[
\mbox{{\texttt (\#workers - checkoutCount)}} = \mbox{\#(workers that know they don't
  have work )}
\]

A worker knows it has no work if it stealing. Note that the
checkoutCount is not always equal to the number of workers with work
to do. In particular, consider a victim that finds the current frame
as stolen. The victim cannot identify whether it has work without
locking its deque. While it aborts, the thief has the stolen
frame and could have invoked {\java checkOut()}. The barrier
identifies both workers as having checked out even though there is one
task between them. Note that allowing the victim to {\java
  checkIn()} when it identifies a steal would lead to the barrier
being incorrectly triggered while the thief still has the stolen task
but it yet to invoke {\java checkOut()}.


\subsection{Phased Computations}

We also added support for phased computations in which tasks in this
phase create tasks to be executed in the next phase. The
implementation of the breadth-first search algorithm proceeds one
level at a time. The nodes processed at this level are used to
determine the nodes to be processed in the next level.

Phased computations are supported as a generalization of global
quiescence. Each worker maintains two stacks of frames, referred to as
caches. Depending on the phase specified when spawning tasks, a task
can be added to the current cache or the next cache, the cache for the
next phase. 

When global quiescence is detected for this phase, the barrier action
invokes {\java advancePhase()} that steps the computation into the
next phase. When a worker runs out of tasks, it checks that the
current phase of the worker is the same as the global phase of the
computation. If the phase of the computation has advanced further, the
workers updates its phase information and swaps the current and next
task collections. 

Each worker specifies the number of tasks it has
outstanding for the next phase when it invokes {\java
  checkIn()}. This information is used to identify if the next phase
has any tasks left to be processed. 

Note that the global phase could have advanced much further than the
phase operated on by this worker. This would happen when this worker
has no work for current phase and has checked-in notifying that it has
not tasks for the next phase. The other workers could then progress
multiple phases before this worker observes the computation progress. 

When global quiescence for this phase is triggered, the number of
workers with tasks for the next phase is known. The computation is
said to have terminated when the current phase has quiesced and no
worker has any task for the next phase. 

For a given phase, maintaining the invariant mentioned above for
global quiescence is more involved with multiple phases. For example,
consider a worker advancing its phase to match the global phase of the
computation and its next cache is non-empty. Since the worker now has
local tasks in this phase, it implicitly checks out of the barrier.


\subsection{Explicitly Partitioned Programs}

While work-stealing provides a convenient abstraction for
load-balancing fine-grained programs, the performance of certain
applications can benefit from the explicitly partitioned programs that
would exist as a typical parallel global address space program. For
example, in the Shiloach-Vishkin algorithm we observe that explicit
partitioning of the edges and vertices amongst the workers leads to
better performance. This is achieved by having the task, called the
job task, submitted by the driver thread spawns tasks, called worker
tasks. The number of worker tasks spawned is equal to the number of
workers in the system. The job task is improperly nested and returns
upon spawning the worker tasks. Each of the spawned tasks now execute
one part of the program. Note that these tasks are spawned and
available at the worker that executed the job task. This worker now
has work and it executes one of the worker tasks. The other workers
identify the presence of work in this worker while stealing and steal
a worker task. Assuming the worker tasks are sufficiently
load-balanced, the rest of the computation proceeds without
work-stealing with each worker executing a worker task. 

\subsection{Performance Analysis}

In this section, we discuss the performance implications of the
different components of the runtime. 

The overheads of the fast version over the sequential execution are:

\begin{enumerate}
\item Method's frame needs to be allocated, initialized, pushed onto
  the deque. Cost: A few assembly instructions.
\item Method's state needs to be saved before each spawn. Cost: writes
  of local dirty variables and {\em PC}, and a {\java
  StoreLoadBarrier}
\item Method must check if its frame has been stolen after each
  return from a spawn. Cost: two reads, compare, branch and a 
  {\java StoreLoadBarrier} 
\item On return, frame must be freed. Cost: A few assembly instructions
\item An extra variable is needed to hold the frame pointer. Cost: Increased
  register pressure.
\end{enumerate}

Note that average cost of allocating and de-allocating memory can be
reduced to a couple of statements with an efficient concurrent memory
allocation scheme. Thus the overhead in the fast path is very
little, as is also demonstrated in the experimental evaluation.

The number of closures created and invocations of the slow version of
an async is proportional to the number of successful steals. The
number of locks requested is proportional to the numbers of attempted
steals. 

Improperly nested tasks take advantage of the lack of return value to
avoid the creation of closures, further reducing overhead. 

The computation is identified as terminated, the moment the last
worker starts trying to steal. Thus there is no significant delay
between the actual termination of the computation and its detection.
The mechanism itself incurs the overhead of two atomic updates to a
shared counter for every successful steal. The overheads incurred in
supported phased execution are similar.

Explicitly partitioned programs begin execution with all the tasks on
one worker. There is a delay before which all workers attempt to steal
from this worker and obtain a worker task. Assuming a truly random
scheme in the choice of the victim to steal from, the worst case in
the number of steals before work is found is proportional to the
number of workers. Since the worker tasks are load-balanced and there
is no work-stealing after this initial delay, the number of steals in
the worst case is independent of the program running time and problem
size, and hence much smaller than that incurred in a typical
work-stealing strategy. We observe this in our experimental evaluation
as well. 


Thus the common execution path in which all workers are busy involves
little overhead. The remaining in other execution paths are
proportional to the number of attempted and successful steals
performed by the workers. The experimental evaluation section
demonstrates that both are far lower than the number of asyncs
spawned, effectively enabling load-balanced execution of fine-grained
parallel programs.

