%\documentclass[onecolumn,12pt]{ieee}
\documentclass[10pt]{article}
%------------------------------------------------------------------------- 
% 
% Use \documentclass[pagenumbers]{ieee}
%
% to produce page numbers, bottom centered, in the output. Default is 
% no page numbers for camera-ready copy.
%
%------------------------------------------------------------------------- 
\usepackage[dvips,letterpaper,margin=1in]{geometry}
\usepackage[pdftex]{graphicx}
\usepackage{times}
\usepackage{epsfig}
\usepackage{enumerate, amscd,  amsmath}  
\usepackage{amssymb}
\usepackage{algorithm2e}
\usepackage{url}
\numberwithin{equation}{section}

%\newenvironment{javaen}{\begin{small}\begin{verbatim}}{\end{verbatim}\end{small}}
\newcommand{\java}{\tt}
\def\Xten{{\sf X10}}
\def\XWS{{\sf XWS}}

\renewcommand{\baselinestretch}{1.0}

\begin{document}

\DeclareGraphicsExtensions{.jpg, .pdf, .mps, .gif, .png}
\title{Solving Large, Irregular Graph Problems in \Xten}

\author{
Guojing Cong,  Vijay Saraswat, and Tong Wen\\
IBM T. J. Watson Research Center\\
 \{ gcong, vsaraswa, tongwen \}@us.ibm.com\\ 
\vspace*{-2ex} \\
Sreedhar Kodali\\
IBM Systems and Technology Group\\
srkodali@in.ibm.com\\
\vspace*{-2ex} \\
Sriram Krishnamoorthy \\
Ohio State University\\
krishnsr@cse.ohio-state.edu
\vspace*{-3ex} \\
}

\date{}

\maketitle
\thispagestyle{empty}


\begin{abstract}

Graph problems have many applications in high performance computing
disciplines. Obtaining practical efficient implementations for large,
irregular graph instances remains a challenge. Current hardware and
software systems do not support fine-grained, irregular parallelism
well. Implementing a custom framework for fine-grained parallelism for
each new graph algorithm is impractical.

% language. 

In this paper we take the approach of writing graph algorithms in a
high-level concurrent language with dynamic fine-grained parallelism
(\Xten). The \Xten{} compiler translates this code into calls to an
\Xten{} runtime (written in Java) organized around a Cilk-like
work-stealing scheduler. \footnote{ The compiler is still under
development, all tests in this paper were performed on code manually
compiled from \Xten{} source.}  The scheduler extends the Cilk
scheduler with several features necessary to efficiently implement
graph algorithms, viz., support for improperly nested procedures,
support for global termination detection, and support for phased
computation.

We show how to express spanning tree algortihms using (a)~pseudo-depth
first search, (b)~breadth-first search and (c)~Shiloach-Vishkin (SV)
algorithm in a simple elegant fashion. We compare the performance of
the code with code written manually in C, and with code written in
Cilk. (The Cilk and C codes perform no garbage-collection.) Tests are
performed on a 32-way Sun Fire T200 server (moxie), and an 8-way
SunFire V40Z server (altair). We compare performance on (i) graphs
with randomly selected edges (Random), (ii) graphs with all vertices
having a fixed outdegree (KGraph),and randomly selected edges, (iii)
Torus planar graphs (each vertex is connected to its four
neighbors). For the SV algorithm, we were unable to write a Cilk
program; we show that the \Xten{} program scales at the same rate as
the hand-written C program, but its performance is marginally poorer.
For DFS and BFS algorithms, we show that on large Random and KGraphs,
\Xten{} programs perform comparably to hand-written C, and better than
Cilk, whereas on large Torus graphs, \Xten{} performs better than C
and substantially better than Cilk. We note that the BFS and DS
\Xten{} programs are approximately a dozen lines long, whereas the
hand-written C code is several hundred lines long.

We conclude that a fine-grained concurrent language such as \Xten{} with a
work-stealing based scheduler may provide an attractive framework for
the implementation of graph algorithms.

\end{abstract}


\section{Introduction}
\label{s:intr}

%% Graph problems are very interesting.

Graph theoretic problems arise in traditional and emerging scientific
disciplines such as VLSI design, optimization, databases, and
computational biology, social network analysis, and transportation
networks.

%% They are very hard to solve.
Large-scale graph problems are challenging to solve in parallel
(e.g.{} on a shared memory symmetric multiprocessor or on a multicore
system) because of their irregular and combinatorial nature.
Irregular graphs arise in many important real world settings, for
example, the Internet, social interaction networks, transportation
networks, and protein-protein interaction networks.  These graphs can
be modeled as `scale-free'' graphs \cite{CZF04}. For random and
scale-free graphs no known efficient static partitioning techniques exist,
and hence the load must be balanced dynamically. 
Moreover, the irregular memory access pattern dictated by the input
instances is not cache-friendly; graph algorithms also tend to be
load/store intensive \cite{G06}, and they lay great pressure on the
memory subsystem.  
Compared with their numerical counterparts, parallel
graph algorithms take drastically different approaches than the
sequential algorithms, and usually employ fine-grained
parallelism. For example, depth-first search (DFS) or breadth-first
search (BFS) are two popular sequential algorithms for the spanning
tree problem. Many parallel spanning tree algorithms, represented by
the Shiloach-Vishkin algorithm \cite{SV82}, take the
``graft-and-shortcut'' approach, and provide $O(n)$ fine-grained
parallelism. In the absence of efficient scheduling support of
parallel activities, fine-grained parallelism incurs large overhead on
current systems, and often the algorithms do not show practical
performance advantage.

We take the spanning tree problem as our example. Finding a spanning
tree of a graph is an important building block for many graph
algorithms, for example, biconnected components and ear decomposition
\cite{MR86}, and can be used in graph planarity testing \cite{KR88}.
Spanning tree represents a wide range of graph problems that have fast
theoretic parallel algorithms but no known efficient parallel
implementations that achieve speedup without serious restricting
assumptions about the inputs.

%% Case study. Bader and Cong's algorithm. load-balancing is an issue.
%% How should the data-structures be organized.
Bader and Cong \cite{BC04a} presented the first fast parallel spanning
tree algorithm that achieved good speedups on SMPs. Their algorithm is
based on a graph traversal approach, and is similar to DFS or BFS.
There are two steps to the algorithm. First a small stub tree of size
$O(p)$ is generated by one worker through a random walk of the
graph. The vertices of this tree are then evenly distributed to each
worker.  Each worker then traverses the graph in a manner similar to
sequential DFS or BFS, using efficient atomic operations (e.g.{}
Compare-and-Swap) to update the state of each node (e.g.{} update the
{\tt parent} pointer). The set of nodes being worked on is kept in a local queue.
When a worker is finished with its portion (its queue is empty), it
checks randomly for any other worker with a non-empty queue, and
``steals'' a portion of that work for itself (by removing it from the
victim's queue, and adding it to its own queue).

For efficient execution, it is very important that the queue be
managed carefully. For instance, the operation of adding work (a node)
to the local queue should be efficient (i.e.{} should not require
locking) since it will be performed frequently. Stealing is however
relatively infrequent and it is preferrable to shift the cost of
stealing from the victim to the thief since the thief has no work to
do (the ``work first'' principle). The graph algorithm designer now
faces a choice. The designer may note \cite{BC04} that correctness is
not compromised by permitting a thief to {\em copy} the set of nodes
that the victim is working on. Here the victim is permitted to write
to the queue without acquiring a lock. Now the price to be paid is
that the thief and the victim may end up working on the same node
(possibly at the same time).  While work may thus be duplicated,
correctness is not affected since the second worker to visit a node
will detect that the node has been visited (e.g.{} because its atomic
operation will fail) and do nothing. Alternatively, the designer may
use a modified version of the Dekker protocol \cite{cilk}, by ensuring
that the thief and victim each writes to a volatile variable and read
the variable written by the other. This guarantees that no work will
be duplicated, but the mechanism used is very easy to get wrong,
leading to subtle concurrency errors.

Thus the design of such high-performance concurrent data-structures is
difficult and error-prone. Those concerns that are of interest to the
graph algorithm designer (e.g.{} expressing breadth-first vs
depth-first search) are mixed in with the concerns for efficient
parallel representation.  This suggests packaging the required
components in a library or a framework and exposing a higher-level
interface to programmers.

\subsection{Cilk Work-stealing}

Such a framework has been proposed for a class of parallel programs 
for shared memory machines by
the designers of Cilk. Cilk is an extension to C which permits the
programmer to specify that some statements may be executed in parallel
{\tt spawn}, and to specify that execution of the current procedure
body should suspend until all statements spawned during its execution
are terminated ({\tt sync}).

The Cilk compiler translates source code into C by using a
``continuation passing transformation'' and adding calls to the
runtime (Cilk work-scheduler) which manages a double-ended queue
(deque) for each worker (= thread). On each procedure entry an
activation frame is pushed onto thhe bottom of the deque. On executing
a spawn a field in the activation frame is updated to point to the
instruction after the spawn, and the code in the body of the spawn is
executed as a sequential procedure call. Now if a thief visits this
worker looking for work it may steal the current activation frame and
continue executing the body of the procedure at the instruction after
the spawn. When the victim returns from executing the spawn it will
discover (using the Dekker protocol) that the parent frame has been
stolen and will now itself go looking for work (by randomly selecting
a victim worker and attempting to steal from it). The sync statement
is executed by keeping track of the number of live children, and
setting up the remainder of the body of the procedure to be executed
once this count reaches zero. 

The Cilk compiler gains further performance improvements by generating
two copies of each procedure (one for the ``fast path'' and one for
the slow path). The fast path corresponds to execution with no steals
and gives efficiency very close to sequential execution.

Since synchronization in Cilk is naturally tied 
Cilk naturally lends itself to the expression of recursive
data-structures since synchronization is tied with procedure
invocation. The literature has many examples of elegant exploitation
of such recursive parallelism (e.g.{} cache-oblivious algorithms
\cite{frigo}). 

\subsection{\Xten{} work-stealing}
%% However, there are problems with work-stealing. 

However, there are problems with Cilk work-stealing when applied to
graph algorithms. Consider the code for a parallel form of
pseudo-depth-first traversal (for spanning tree) in
Table~\ref{alg:st-x10}. (The code is written in \Xten{} \cite{x10}.)
Here on visiting a node (in the body of {\tt
traverse}) we wish to spawn parallel activities ({\tt async}) to visit
each neighboring vertex that has not been visited before. However,
there is no reason for the current activity to wait for all these
activities to terminate before returning from the body of the {\tt
traverse} routine. Specifically, the invocation of {\tt traverse} does
not return a value -- it merely updates the heap to record the parent
for the visited node. Hence there is no need for the invoker to wait
for a return value. Thus the body does not need a {\tt sync}
statement. But this makes {\tt traverse} illegal as a Cilk
procedure. Said differently, a natural expression of parallel graph
algorithms requires the use of improperly nested computation.

Further, note the use of {\tt finish} in the body of the {\tt tree}
method.  This states that the invoking activity must wait until all
activities spawned during the execution of {\tt traverse()} have
terminated. {\tt tree()} is invoked precisely once -- at the root node
for the spanning tree -- and hence there is exactly one sync for the
duration of the spanning tree computation. Thus another refinement of 
Cilk is natural and useful here: the implementation must support 
global termination detection (i.e.{} it must detect when the 
dequeues associated with all queues are empty). 

\begin{table}
\centering
\scriptsize
\begin{tabular}{c}
\begin{minipage}[t]{0.5\textwidth}
\begin{verbatim} 
  class V {
    V parent;
    V[] neighbors;
    void dfsTree() {
      parent=this;
      finish dfsTraverse();
    }
    void dfsTraverse() {
      for(V v : neighbors) {
        atomic v.parent = (v.parent==null?this:v.parent);
        if (v.parent == this)
           async v.dfsTraverse();
      }
    }
  }
\end{verbatim}
\end{minipage} 
\end{tabular}
\caption{Pseudo Depth-first spanning tree algorithm core in \Xten{}}
\label{alg:dfs-x10}
\end{table}

\paragraph{Phased computation}
Finally, we note that breadth-first search requires another idea, that
of {\em phased computation}. Here we mark vertices with a level number
corresponding to the distance from the root node. It is necessary to
ensure that a vertex is visited at distance $i$ is visited in the
$i$th phase. This means that activities cannot be launched immediately
for vertices marked in the $i$th phase. Instead all these activities
must wait for all the vertices marked in the $i-1$st phase to be
processed, i.e.{} one must integrate barrier-based synchronization
with work-stealing.

\begin{table}
\centering
\scriptsize
\begin{tabular}{c}
\begin{minipage}[t]{0.5\textwidth}
\begin{verbatim} 
  class V {
    V parent;
    V[] neighbors;
    void bfsTree() {
      parent=this;
      finish async { 
        clock c = new clock();
        bfsTraverse(c);
      }
    }
    void bfsTraverse(clock c) {
      for(V v : neighbors) {
        atomic v.parent = (v.parent==null?this:v.parent);
        if (v.parent == this)
           async clocked(c) { next; v.bfsTraverse();}
      }
    }
  }
\end{verbatim}
\end{minipage} 
\end{tabular}
\caption{Breadth-first spanning tree algorithm core in \Xten}
\label{alg:bfs-x10}
\end{table}

We show the program for expressing this computation in
Table~\ref{alg:bfs-x10}. While this program will be discussed in
detail later, we mention now that {\tt c} should be thought of as a
{\em barrier}. The activity spawned to examine a newly discovered
vertex must wait until the barrier is reached ({\tt next}) before
commencing its own traversal. Thus the program combines barrier-based
synchronization with fine-grained parallelism. The program clearly
expresses the strong relationship with the depth-first search variant
-- there are no details about the work-stealing implementation
polluting the program.

\subsection{Rest of this paper}

In this paper we show that pseudo- Depth-first, breadth-first and
Shiloach-Vishkin algorithms for spanning tree can be programmed quite
elegantly in \Xten. Further we present the design of \XWS, the \Xten{}
work-stealing scheduler, which extends the Cilk work-stealing
scheduler with support for improperly nested computation, termination
detection and phased computation. These mechanisms are sufficient to
execute the three graph algorithms efficiently. 


We use a collection of sparse graph generators to evaluate these algorithms.
Our generators include several employed in previous
experimental studies of parallel graph algorithms for related
problems. For instance, we include the torus topologies used in the
connectivity studies of Greiner \cite{Gre94}, Krishnamurthy \emph{et
al.} \cite{KLC97}, Hsu \emph{et al.} \cite{HRD97}, Goddard \emph{et
al.} \cite{GKP97}, and Bader and Cong \cite{BC04b}, the random graphs
used by Greiner \cite{Gre94}, Hsu \emph{et al.}  \cite{HRD97}, and
Goddard \emph{et al.} \cite{GKP97}, the geometric graphs used by
Greiner \cite{Gre94}, Hsu \emph{et al.} \cite{HRD97}, Krishnamurthy
\emph{et al.} \cite{KLC97}, and Goddard \emph{et al.} \cite{GKP97}.

\begin{itemize}
\itemsep0pt

\item \textbf{2D Torus} The vertices of the graph are placed on a 2D
  mesh, with each vertex connected to its four neighbors.  

\item \textbf{Random Graph} We create a random graph of $n$ vertices
  and $m$ edges by randomly adding $m$ unique edges to the vertex
  set. Several software packages generate random graphs this way,
  including LEDA \cite{MN99}.
  
\item \textbf{Geometric Graphs and AD3} In these $k$-regular graphs,
  %$n$ points are chosen uniformly and at random in a unit square in
  %the Cartesian plane, and 
  each vertex is connected to its $k$ %nearest
  neighbors.  Moret and Shapiro \cite{MS94} use these
  in their empirical study of sequential MST algorithms. \textbf{AD3}
  is a geometric graph with $k=3$.  
\end{itemize}

We compare the performance of \XWS{} to two other programs. First is a
hand-written C program, a variant of \cite{BC04a}. Second, we have
written versions of pseudo-DFS and BFS in Cilk. (We were unable to
write the SV algorithm in Cilk.)

We present performance data on two machines. Altair is an 8-way Sun
Fire V40Z server, running four dual-core AMD Opteron processors at
2.4GHz (64KB instruction cache/core, 64KB data cache/core, 16GB
physical memory). Moxie is a 32-way Sun Fire T200 Server running
UltraSPARC T1 processor at 1.2 GHz (16KB instruction cache/core, 8KB
data cache/processor, 2MB integrated L2 cache, 32GB physical
memory). We are in the process of benchmarking these programs on a
64-way Power5 SMP as well. 

We discuss in detail the design of \XWS, including our extensions to CWS.

Finally we conclude with a discussion of related and future work.


%%  Throughout the paper, we
%%  use $n$ and $m$ to denote the number of vertices and the number of
%%  edges of an input graph $G=(V,E)$, respectively. 
  
\section{\Xten{}: Designed for High Productivity}\label{s:x10}
\paragraph{}\Xten{} is a new Partitioned Global Address Space (PGAS) language being developed at IBM as part of the DARPA HPCS project~\cite{X10-OOPSLA05}. It is designed to address both programmer productivity and parallel performance for modern architectures from the multicores, to the heterogeneous accelerators (as in the Cell processor), and to the scale-out clusters of SMPs such as Blue Gene.  The language is based on sequential Java with extensions for programming fine-grained and massive parallelism. Unlike other PGAS languages such as Co-Array Fortran, Titanium, and UPC whose model of parallelism is Single Program Multiple Data (SPMD), \Xten{} supports dynamic and structured concurrency with SPMD as a special case. In this section, we provide a brief introduction to the basic concepts in the \Xten{} programming model and the language constructs used to implement the graph algorithms of interest. For more details and other features of \Xten{}, readers please refer to~\cite{X10-OOPSLA05}.
 
\begin{enumerate}
\item{\bf Activities}~--~All concurrency in \Xten{} is expressed as asynchronous {\em activities}. An activity is a lightweight thread of execution, which can be spawned recursively in a fork-join manner. The syntax of spawning an activity is {\tt async S}, where a new child activity is created executing statement {\tt S}. The granularity of an activity is arbitrary~--~{\tt S} can be a single statement reading a remote variable or a sequence of statements performing a stencil operation on a grid.  Our experience has been that this single notion of an asynchronous activity can subsume many levels of parallelism that  a programmer may encounter such as threads, structured parallelism (including OpenMP), messaging (including MPI), and DMA transfers.  
\item{\bf Places}~--~The main program starts as single activity at {\em place} 0. Place is an \Xten{} concept which can be considered as a virtual SMP, but multiple places can be mapped to one physical SMP node. The global address space is partitioned across places. Data and activities have affinity with and only with one place. Activities can only operate on data local to them, that is, within the same place. To access data at another place, a new activity has to be spawned there to perform the operation. The syntax for spawning an activity at place $p$ is {\tt async (p) S}. The diagram in Figure \ref{fig 1} describes the X0 programming model.

\begin{figure}
\begin{center}
\pdfimage width 14cm  {X10ProgModel.jpg}
\end{center}

Dynamic parallelism with a Partitioned Global Address Space. All
concurrency is expressed as asynchronous activities.  Each vertical
green rectangle above represents the stack for a single activity. An
activity may hold references to remote objects, that is, at a
different place.  However, if it attempts to operate on a remote
object, then it has to spawn a new activity at the remote place to
perform the operation.  Immutable (read only) data is special which
can be accessed freely from any place providing opportunity for
single-assignment parallelism.
\caption{\Xten{} Programming Model}
\label{fig 1}

\end{figure}


\item{\bf \Xten{} arrays}~--~\Xten{} supports a rich set of multidimensional array abstractions and domain calculus as in Titanium~\cite{titaniumDoc}. The array index space is global where each index is an integer vector named {\em point}, and a {\em domain} is a set of points which can be either rectangular or not. The distribution of an array across places is specified by a {\em distribution} which can be defined by users. Each distribution maps a set of points in a region to a set of places. 

%For example, the following code defines a 1D block-distributed array initialized in a way such that each element contains its own index.

%\begin{center}
%{\scriptsize
%\begin{verbatim}
%region R = [0:TableSize-1];
%dist RD = dist.factory.block(R);
%long [.] Table = new long[DD] (point [i]) {return i;};
%\end{verbatim}
%}
%\end{center}

\item{\bf Parallel loops}~--~There are two kinds of parallel loop in \Xten{}: {\tt foreach} and {\tt ateach}, for looping over a region and a distribution respectively. Their difference is that the {\tt for} loop spawns activities locally, whereas the {\tt ateach} loop spawns activities at the places specified by the distribution. 

%shown by the following two pairs of equivalent statements.
%\begin{center}
%{\scriptsize
%\begin{verbatim}
%foreach (point [i] : R) Table[i]+=i; is equivalent to for (point [i] : R) async Table[i]+=i;
%ateach (point [i] : RD) Table[i]+=i; is equvalent to for (point [i] : R) async (RD[i]) Table[i]+=i;
%\end{verbatim}
%}
%\end{center} 

\item{\bf Finish and clock}~--~The statement {\tt async S} returns immediately when it is executed even if the statement {\tt S} is not finished, which may also spawn other activities. The {\tt finish S} construct is used to specify that the next instruction is executed only when the statement {\tt S} has finished globally, that is, all transitively spawned child activities have finished.
 %For example, 
%\begin{center}
%{\scriptsize
%\begin{verbatim}
%finish foreach (point [i] : R) Table[i]*=i; 
%Table[0]+=1;.
%\end{verbatim}
%}
%\end{center}
%Here, after the above two statements are executed, the value of {\tt Table[0]} is guaranteed to be $1$. 
While {\tt finish} permits the detection of global termination, there are many cases in which a barrier-like coordination is needed for a set of activities during the middle of their computation. \Xten{} uses {\em clock} to coordinate such a set of activities. A clock has phases, and the activities registered with this clock can be synchronized by waiting for their finish of the current clock phase. An activity can be registered with multiple clocks and it can drop any of them at any time.
\item{\bf Atomic blocks}~--~\Xten{} uses {\em atomic} blocks for mutual exclusion. An atomic statement/method is conceptually executed in a single step, while other activities are suspended. An atomic block must be nonblocking, sequential (without spawning activities), and local (no remote data access). %Without using the atomic block, the following code will generate race conditions.
%\begin{center}
%{\scriptsize
%\begin{verbatim}
%double sum = 0;
%finish foreach (point [i] : R) sum+=1; //should use atomic sum+=1 instead;
%\end{verbatim}
%}
%\end{center}
{\em Conditional} atomic block is another parallel language construct of \Xten{} which can be used, for example, to implement point-to-point synchronization. The syntax for a conditional atomic block is {\tt when (E) S}, where the executing activity suspends until the boolean expression {\tt E} is true, then {\tt S} is executed atomically. 
\end{enumerate}

\section{\XWS}
\label{s:runtime}

We have implemented a runtime system for \Xten{}. 
As an initial implementation, we have developed a Java-based
runtime system to support shared-memory parallelization. It is
distributed as a Java package, {\java x10.runtime.cws}, under an
open-source license~\cite{x10-webpage}. In this section, we discuss the
implementation of key features of the runtime system.

The computation is organized as collection of {\em tasks}. A task is a
sequence of instructions that can spawn other tasks and wait for
completion of the spawned tasks. The computation begins with a single
task and is considered complete when there are no more tasks executing
in the system. 

\subsection{Cilk work-stealing}
This section describes Cilk work stealing, as implemented in \XWS. It
closely follows the description of the implementation of Cilk in
\cite{frigo98implementation}.

In summary, Cilk Work Stealing (CWS) is organized around a collection
of cooperating threads called {\em workers}. Each worker maintains a
double-ended queue (deque) of tasks. During execution as a worker
creates more tasks it pushes them at the bottom of the queue. When it
needs more tasks it retrieves the current task from the bottom of the
deque. When a worker runs out of work (its deque is empty), it
randomly chooses another worker (the {\em victim}) and attempts to
steal a task by fetching it from the top of the deque. Program begins
execution when the environment submits a task to a central task queue.

One of the workers retrieves the task from the global queue and begins
executing it. When a worker does not have tasks to execute it {\em steals}
tasks available at other workers. Assuming the computation contains
sufficient parallelism stealing happens infrequently. The design
ensures that there are few overheads during normal execution, referred
to as {\em the fast path}. The additional overheads incurred to load-balance
the computation are proportional to the number of steals in the
execution. The design of the worker for the the task types supported
is discussed in subsequent sections.

The normal execution corresponds to the depth-first sequential
execution of the tasks spawned. Thus the execution corresponds to a
sequential execution when there is only one worker.

We now describe the details.

Every \emph{async} in the \Xten{} program is compiled into a sub-class
of {\tt Frame}. An \emph{async} is said to consist of {\em threads},
where a thread is a non-blocking sequence of instructions terminating
in a spawn or waiting on an \emph{async}.

Each class created for an \emph{async} contains as member variables the local
variables used in the \emph{async} body, a counter ({\em PC}) that specifies
the next instruction in the body of the \emph{async} after the current
thread.

\begin{figure*}
\begin{minipage}{0.25\textwidth}
\scriptsize
\begin{verbatim}
int fib(int n) {
  int x, y;
  if(n<2) return n;
  finish {
    async x=fib(n-1);
    async y=fib(n-2);
 }
  return x+y;
}
    (a)
\end{verbatim}
\end{minipage}%
\begin{minipage}{0.4\textwidth}
\scriptsize
\begin{verbatim}
int fib(Worker w, int n) 
 throws StealAbort { 
  int x, y;
  if (n < 2) return n;

  FibFrame frame = new FibFrame(n);
  frame.PC=LABEL_1;
  w.pushFrame(frame);

  x = fib(w, n-1);
  w.abortOnSteal(x);

  frame.x=x;
  frame.PC=LABEL_2;

  y=fib(w, n-2);
  w.abortOnSteal(y);

  w.popFrame();
  return frame.x+y;
}

      (b)
\end{verbatim}
\end{minipage}%
\begin{minipage}{0.45\textwidth}
\scriptsize
\begin{verbatim}
void compute(Worker w, 
 Frame frm) throws StealAbort {
  int x, y;
  FibFrame f=(FibFrame)frm;
  int n = f.n;
  switch (f.PC) {
  case ENTRY: 
    if (n < 2) {
      result = n;
      setupReturn();
      return;
   }
    f.PC=LABEL_1;
    x = fib(w, n-1);
    w.abortOnSteal(x);
    f.x=x;
  case LABEL_1: 
    f.PC=LABEL_2;
    int y=fib(w,n-2);
    w.abortOnSteal(y);
    f.y=y;
  case LABEL_2: 
    f.PC=LABEL_3;
    if (sync(w)) return;
  case LABEL_3:
    result=f.x+f.y;
    setupReturn();
 }
}
       (c)
\end{verbatim}
\end{minipage}%
\caption{(a) \Xten{} program for Fibonacci. (b) Fast version. (c) Slow version}%
\label{fig:fib-ill}
\end{figure*}

Cilk implements a scheme for handling continuations.  Two versions of
the \emph{async} body are created -- the fast and the slow
versions. The fast version of the program is executed during the
normal course of the program. The slow version is invoked by the
worker to initiate processing of a task. Fig.~\ref{fig:fib-ill} shows
the fast and slow versions for the Fibonacci method shown in
Fig.~\ref{fig:fib-ill}(a).

Each worker maintains a deque consisting of stack of Frames. On
entering a fast version, a frame object is created and pushed onto the
stack (method: {\java pushFrame()}). This frame contains the
up-to-date values of variables whenever any other worker might steal
this task. This frame is popped from the stack (method: {\java popFrame()}) 
when this method returns.

Whenever a task is spawned, the worker immediately proceeds to execute
the spawned task like a sequential method call. When the spawned task
finishes execution, the worker checks whether the current frame was
stolen. If so, the result computed by the child task is stored to be
passed onto its parent and the execution of the task aborts (method:
{\tt abortOnSteal()}). The method call stack is unwound by throwing
an exception ({\tt StealAbort}) that is caught in the main
routine executed by the worker. The values in the frame of the parent
stack are updated before proceeding to execute a spawned child, as the
worker now has a non-executing task and hence is target of a steal. 

All the descendents are guaranteed to be completed in the fast version
when a {\java finish} is encountered. Hence the finish statements
are ignored.

The slow version restores any local variables and uses the {\em PC} to
start execution of the task past the execution point at which it might
have been stolen. A task might have been stolen when its descendents
are executing. Thus a {\em finish} statement, translated to the
{\java sync()} method, might cause the invoking task to suspend on
completion of non-terminated children. The value of the {\java result} 
variable is returned to the parent task on invocation of the
{\java setupReturn()} method. 

In a program with sufficient parallelism, the slow version is expected
to execute infrequently. The design tries to minimize the overheads in
the fast version even at the expense of the slow version (the
``work-first'' principle).

\subsection{Support for Properly Nested Tasks}
Cilk requires {\em fully-strict} programs
in which a task waits for all its descendents to complete before
returning. Such tasks are also called properly nested tasks. 

The \Xten{} runtime system is designed to leverage the Cilk design while
supporting a larger class of programs. \Xten{} provides support for {\em
strict} computations, in which a ancestor task need not wait for its
descendent tasks to be completed. Such tasks are said to be {\em improperly
nested}. 

Each worker contains a {\em closure}. A closure is an  object used
to return values from the spawned tasks to their parents in the
presence of work stealing.  

Each closure maintains a stack of frames. Frames corresponding to
spawned tasks are pushed into the stack on entry, and popped on
return. In the fast path, return values are propagated as they would
be in a sequential program. The thief steals a closure together with
the bottom-most available frame from its
{\em victim}.

When a thief steals a task, the descendants of the task in the
victim's dequeue continue to execute.  In order to return values from
the descendents to the parent, a new closure is created that on
completion returns the result to the parent closure that was stolen.

Thus the closures form a tree of return value propagation
corresponding to the steal operation performed. Termination is
detected when the closure corresponding to the task inserted by the
driver thread returns. 

The procedure executed by the workers to handle properly nested tasks
is shown in Fig.~\ref{fig:worker-code}(a). On completing execution of
a closure, a worker first attempts to obtain another closure from its
local queue (method:{\java extractBottom()}). If no local closure is
available to execute, the worker attempts to obtain a task either by
stealing or from the global queue (method:{\java getTask()}). It then
executes the slow version of the task obtained (method:{\java
  execute()}). 


\subsection{Support for Improperly Nested Tasks}

Properly nested tasks $t$ satisfy the property that at the moment when
the slow version terminates (method:{\java compute()}) the frame at
the bottom of the worker's dequeue is $t$. Hence the task can be
completed (i.e., removed from the dequeue) by including a {\java
w.popFrame()} call at the end of the compute method. In essence, if a
worker is executing only properly nested tasks (this is true when it
is executing Cilk code), there is a one-to-one correspondence between
the frame stack and the tasks being processed.

\Xten{} permits improperly nested tasks. Such tasks $q$ are used, for
instance, to implement the pseudo-depth-first search discussed in this
paper. Such a task may add a task $r$ to the deque of its worker (say $w$)
without necessarily transferring control to $r$. This has two
consequences. First, recall that as soon as a worker's dequeue
contains more than one task the worker may be the target of a
theft. Therefore as soon as $q$ pushes $r$ onto $w$'s dequeue, $q$ is
available to be stolen.  Therefore $q$'s compute method must record the
fact that is computation has begun so that the stealing worker $z$ may
do the right thing. For instance, if $q$'s compute method does not
contain any internal suspension point then $z$ must immediately
terminate execution of $q$ and pop $q$ off its deque. This can be
accomplished by defining a volatile int PC field in $q$, and adding the
following code at the beginning of $q$'s compute method

{\scriptsize
\begin{verbatim}
  if (PC==1) {
    w.popFrame();
    return;
  }
  PC=1;
\end{verbatim}
}

Second for an improperly nested task when control returns from $g$'s
compute method, it may not be the case that the last frame on the
dequeue is $g$. Therefore a call to {\java popFrame()} at the end of
$g$'s compute method would be incorrect. Instead, the compute method
returns (without attempting to pop the last frame on the deque). Now
whenever the task reaches the bottom of the dequeue, the worker will,
as usual, invokes its compute method. However, the code sequence
described above will execute, thereby popping the frame from the
deque. Thus the code sequence above serves two purposes -- it does the
cleanup necessary when the task is stolen as well as when it is
completed.

The changes to the basic worker code necessary to support improperly
nested tasks are shown in Fig.~\ref{fig:worker-code}(b). With
improperly nested tasks, a worker no long enjoys the property that
when control returns to it from the invocation of an execute method on
the top-level task, the deque is empty. Indeed, control may return to
the scheduler leaving several tasks on the deque, including the task
whose execute method has just returned. The scheduler must now enter a
phase in which it executes the task at the bottom of the deque:

\begin{figure*}
\begin{minipage}{0.5\textwidth}
\scriptsize
\begin{verbatim}
public void run() {
  Executable cl=null; //frame/closure
  int yields = 0;
  while (!done) {
    if (cl == null ) {
      //Extract work from local queue.
      //It will be a closure
      //cl may be null. When non-null
      //cl is typically RETURNING.
      lock(this);
      try {
        cl = extractBottom(this);
      } finally {
        unlock();
      }
    }
  }
  if (cl == null)
    //Steal or get from global queue
    cl = getTask(true);  
  if (cl !=null) {
    // Found some work! Execute it.
    Executable cl1 = cl.execute(this);
    cl=cl1;
    cache.reset();
  } else Thread.yield();
}
                (a)
\end{verbatim}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\scriptsize
\begin{verbatim}
public void run() {
  Executable cl=null; //frame or closure
  int yields = 0;
  while (!done) {
    if (cl == null ) {
      //Addition for GlobalQuiescence. 
      //Keep executing current frame 
      //until dequeue becomes empty.
      if (jobMayHaveImproperTask) {
       Cache cache = this.cache;
        for(;;) {
          if(!cache.empty())
            Frame f=cache.currentFrame();
          if (f == null) break;
          Executable cl1=f.execute(this);
          if (cl1 != null) {
            cl=cl1;
            break;
          }
        }
      } 
      //Rest of worker code same as for
      //properly nested tasks ...
    }
  }
}
                 (b)
\end{verbatim}
\end{minipage}
\caption{Code executed by workers for (a) only properly nested tasks (b)
  properly and improperly nested tasks. Note that (b) is an extension of (a)}
\label{fig:worker-code}
\end{figure*}

\begin{figure*}
\begin{minipage}{0.5\textwidth}
\scriptsize
\begin{verbatim}
Closure steal(Worker thief) {
  lock(thief); //lock victim deque
  Closure cl = peekTop(thief, victim);
  if (cl==null) 
    return null; //nothing to steal
  //cl = Closure that can be stolen
  cl.lock(thief);
  Status status = cl.status();
  if (status == READY) {
    //Closure not processed by victim
    //steal the Closure
  }
  else if (status == RUNNING) {
    //Possible contention with victim
    //Need to steal head frame in Closure
    if (cache.dekker(thief)) {
      //>1 Frame available in Closure
      //Promote child frame to Closure 
      //Steal this Closure & head frame
    } 
  }
  return null; //No Closure to steal
}
         (a)
\end{verbatim}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\scriptsize
\begin{verbatim}
Frame stealFrame(Worker thief) {
  Worker victim = this;
  lock(thief);

  //>1 frame in victm's frame stack?
  boolean b=cache.dekker(thief);
  if (b) {
    //Frame available to steal
    Frame frame = cache.headFrame();
    //Mark this frame as stolen
    cache.incHead(); //H=H+1
    return frame;
  }
  return null; //No frame to steal
}
          (b) 
\end{verbatim}
\end{minipage}
\caption{Work stealing algorithm for (a) Properly nested tasks (b)
  Improperly nested tasks. Both are invoked on victim's
  Worker object (victim==this). Locks held are freed before returning.}%
\label{fig:stealing-alg}
\end{figure*}



%\subsection{How to steal quickly}
%
%
%%% In the fast version of an async, any modified local variables are
%%% copied back into the Frame object before entering a spawned
%%% child. This ensures that the updated values are available to any thief
%%% that may steal the parent frame before the execution of the current
%%% worker can return to it. On return from a spawned task, the task clone
%%% checks for 
%
%The frames corresponding to the tasks form a stack. We denote the head
%and tail of the task by $H$ and $T$, respectively. When a task is
%spawned, the corresponding frame is pushed into the head of the stack
%($H=H+1$). When a worker returns from a spawned task is checks that
%whether the current frame is stolen by executing one-half of lock-free
%Dekker's algorithm~\cite{dekker}:
%
%{\scriptsize
%\begin{verbatim}
%  --T; 
%  StoreLoadBarrier;
%  if (H >= T) 
%    // Stolen 
%  else 
%    // Not stolen
%\end{verbatim}
%}
%
%Note that the {\java StoreLoadBarrier} is implied in Java if
%{\java T} is declared to be {\java volatile}.
%
%When a worker is out of work, it randomly selects a victim and
%attempts to steal from its frame stack. The procedures employed to
%steal in the case of properly- and improperly-nested tasks are shown
%in Fig.~\ref{fig:stealing-alg}(a) and Fig.~\ref{fig:stealing-alg}(b),
%respectively. Note that the victim, and multiple thieves might attempt
%to operate on the same frame stack. The thief first obtains a lock on
%the victim's deque to avoid contention with other thieves trying to
%steal from the same victim.
%
%When properly nested tasks are being processed, the thief identifies a
%closure at the end of the deque and locks it. If the victim is
%processing some other closure and this closure is in a {\java READY}
%state, there is no contention with the victim on this closure. The
%thief extracts the Closure and steals it. If the closure is in
%{\java RUNNING} state, the victim is potentially adding and deleting
%frames on the frame stack associated with the closure. Dekker's
%algorithm is used to determine whether there is more than one frame in
%the frame stack. If there is, the locked closure together with the
%frame at the head of the stack are stolen. The immediate child frame
%of the stolen frame is promoted to a closure which is left in the
%victim's deque. The child task returns its result to the parent task
%through this promoted closure.
%
%In computations on improperly nested tasks, there is only one stack of
%frames used to represent the tasks. The thief directly locks the stack
%of frames (labeled {\java cache} in the algorithm) and attempts to
%steal from the stack's head when more than one tasks in available in
%the task. The frame is marked as stolen by incrementing the stack's
%head. 
%
%Since the tasks are pushed at the tail, the algorithm implies that a
%task is stolen only if all its parents have already been
%stolen. Parent tasks represent more work than child tasks, since they
%have potential to generate a greater number of tasks. The algorithms
%thus favor stealing of tasks that represent a large portion of work
%rather than fine-grained descendent tasks that do limited
%processing. This leads to better load balancing of the computation and
%reduces the number of steal attempts by workers.
%
\subsection{Global Quiescence}

In fully-strict computations, i.e., those involving properly nested
tasks, completion of the first task and the return of the
corresponding Closure indicates computation termination.
Improperly-nested tasks that do not require a return call chain can do
away with the closures. We have implemented a mechanism to efficiently
identify termination without closures.

The workers share a barrier. The barrier is used to determine when all
workers are out of work. Every worker notifies the barrier of its
state through two methods. {\java checkIn()} is used to enter the
barrier barrier and notify that the worker is out of work. When such a
worker steals work from a victim, it invokes {\java checkOut()} to
leave the barrier. The barrier maintains a {\java checkoutCount} on
the number of workers checked out. It is triggered when all the workers
are in it. The action associated with the barrier is triggered and it
signals that the computation has terminated.

The algorithm maintains the invariant:

\[
\mbox{{\texttt (\#workers - checkoutCount)}} = \mbox{\#(workers that know they don't
  have work )}
\]

A worker knows it has no work if it stealing. Note that the
checkoutCount is not always equal to the number of workers with work
to do. In particular, consider a victim that finds the current frame
as stolen. The victim cannot identify whether it has work without
locking its deque. While it aborts, the thief has the stolen
frame and could have invoked {\java checkOut()}. The barrier
identifies both workers as having checked out even though there is one
task between them. Note that allowing the victim to {\java
  checkIn()} when it identifies a steal would lead to the barrier
being incorrectly triggered while the thief still has the stolen task
but it yet to invoke {\java checkOut()}.


\subsection{Phased Computations}

We also added support for phased computations in which tasks in this
phase create tasks to be executed in the next phase. The
implementation of the breadth-first search algorithm proceeds one
level at a time. The nodes processed at this level are used to
determine the nodes to be processed in the next level.

Phased computations are supported as a generalization of global
quiescence. Each worker maintains two stacks of frames, referred to as
caches. Depending on the phase specified when spawning tasks, a task
can be added to the current cache or the next cache, the cache for the
next phase. 

When global quiescence is detected for this phase, the barrier action
invokes {\java advancePhase()} that steps the computation into the
next phase. When a worker runs out of tasks, it checks that the
current phase of the worker is the same as the global phase of the
computation. If the phase of the computation has advanced further, the
workers updates its phase information and swaps the current and next
task collections. 

Each worker specifies the number of tasks it has
outstanding for the next phase when it invokes {\java
  checkIn()}. This information is used to identify if the next phase
has any tasks left to be processed. 

Note that the global phase could have advanced much further than the
phase operated on by this worker. This would happen when this worker
has no work for current phase and has checked-in notifying that it has
not tasks for the next phase. The other workers could then progress
multiple phases before this worker observes the computation progress. 

When global quiescence for this phase is triggered, the number of
workers with tasks for the next phase is known. The computation is
said to have terminated when the current phase has quiesced and no
worker has any task for the next phase. 

For a given phase, maintaining the invariant mentioned above for
global quiescence is more involved with multiple phases. For example,
consider a worker advancing its phase to match the global phase of the
computation and its next cache is non-empty. Since the worker now has
local tasks in this phase, it implicitly checks out of the barrier.


\subsection{Explicitly Partitioned Programs}

While work-stealing provides a convenient abstraction for
load-balancing fine-grained programs, the performance of certain
applications can benefit from the explicitly partitioned programs that
would exist as a typical parallel global address space program. For
example, in the Shiloach-Vishkin algorithm we observe that explicit
partitioning of the edges and vertices amongst the workers leads to
better performance. This is achieved by having the task, called the
job task, submitted by the driver thread spawns tasks, called worker
tasks. The number of worker tasks spawned is equal to the number of
workers in the system. The job task is improperly nested and returns
upon spawning the worker tasks. Each of the spawned tasks now execute
one part of the program. Note that these tasks are spawned and
available at the worker that executed the job task. This worker now
has work and it executes one of the worker tasks. The other workers
identify the presence of work in this worker while stealing and steal
a worker task. Assuming the worker tasks are sufficiently
load-balanced, the rest of the computation proceeds without
work-stealing with each worker executing a worker task. 

\subsection{Performance Analysis}

In this section, we discuss the performance implications of the
different components of the runtime. 

The overheads of the fast version over the sequential execution are:

\begin{enumerate}
\item Method's frame needs to be allocated, initialized, pushed onto
  the deque. Cost: A few assembly instructions.
\item Method's state needs to be saved before each spawn. Cost: writes
  of local dirty variables and {\em PC}, and a {\java
  StoreLoadBarrier}
\item Method must check if its frame has been stolen after each
  return from a spawn. Cost: two reads, compare, branch and a 
  {\java StoreLoadBarrier} 
\item On return, frame must be freed. Cost: A few assembly instructions
\item An extra variable is needed to hold the frame pointer. Cost: Increased
  register pressure.
\end{enumerate}

Note that average cost of allocating and de-allocating memory can be
reduced to a couple of statements with an efficient concurrent memory
allocation scheme. Thus the overhead in the fast path is very
little, as is also demonstrated in the experimental evaluation.

The number of closures created and invocations of the slow version of
an \emph{async} is proportional to the number of successful steals. The
number of locks requested is proportional to the numbers of attempted
steals. 

Improperly nested tasks take advantage of the lack of return value to
avoid the creation of closures, further reducing overhead. 

The computation is identified as terminated, the moment the last
worker starts trying to steal. Thus there is no significant delay
between the actual termination of the computation and its detection.
The mechanism itself incurs the overhead of two atomic updates to a
shared counter for every successful steal. The overheads incurred in
supported phased execution are similar.

Explicitly partitioned programs begin execution with all the tasks on
one worker. There is a delay before which all workers attempt to steal
from this worker and obtain a worker task. Assuming a truly random
scheme in the choice of the victim to steal from, the worst case in
the number of steals before work is found is proportional to the
number of workers. Since the worker tasks are load-balanced and there
is no work-stealing after this initial delay, the number of steals in
the worst case is independent of the program running time and problem
size, and hence much smaller than that incurred in a typical
work-stealing strategy. We observe this in our experimental evaluation
as well. 


Thus the common execution path in which all workers are busy involves
little overhead. The remaining in other execution paths are
proportional to the number of attempted and successful steals
performed by the workers. The experimental evaluation section
demonstrates that both are far lower than the number of \emph{asyncs}
spawned, effectively enabling load-balanced execution of fine-grained
parallel programs.

\section{Performance Evaluation}\label{s:results}

\subsection{C implementation}
All the C implementations follow the SPMD programming style.  $P$
threads are created, and each of them is assigned a piece of work .
The DFS implemenation balances the workload through a simple, explicit
work-stealing scheme as described in \cite{BL94}.  Both the BFS and SV
implementaions simply distribute the input array evenly to each
processor to work on.  Ajacency array are used for the input
representation in BFS and DFS.  SV simulates the corresponding PRAM
algorithm, and uses the edge list representation.  In DFS,
mutual-exclusion through atomic instruction is used for
synchronization.  BFS and SV employ only barriers, and the barrier
implemenation is the usual $O(\log P)$ tree implemenation.

DFS in Cilk use the same input represenation as the C implementation.
Concurrency and synchronization are supported by Cilk runtime and Cilk
lock ( an efficient implementation through atomic instructions).

The performance numbers for handwritten C application and for the
\XWS{} code are given below, for psuedo-depth-first search,
breadth-first search and the Shiloach Vishkin algorithm for spanning
tree (from top to bottom). Numbers are presented for two machines,
altair (containing 4 dual-core Opterons) on the left and moxie, a
32-processor Niagara on the right. The best numbers for ten
consecutive runs are presented.

The Java programs were run using the experimental Java 1.7 release on
both machines. C programs were compiled with Sun cc v 5.8 and Cilk
programs with the 5.3.2 compiler.

The graphs show good scaling for the \XWS{} version of applications on
both machines. In all cases the numbers for \XWS{} are better than
that for the C code at higher processor counts.

Cilk does not perform well for BFS. (We were unable to get a Cilk
version of SV running in time for the submission.) 


\begin{figure}
 \begin{tabular}{ccc}
 \pdfimage width 8cm {altair.dfs.pdf} &
 \pdfimage width 8cm {moxie.dfs.pdf} 
 \end{tabular}
\caption{Psuedo-DFS for altair and moxie}
\end{figure}

\begin{figure}
 \begin{tabular}{ccc}
 \pdfimage width 8cm {altair.bfs.pdf} &
 \pdfimage width 8cm {moxie.bfs.pdf} 
 \end{tabular}
\caption{BFS for altair and moxie}
\end{figure}

\begin{figure}
 \begin{tabular}{ccc}
 \pdfimage width 8cm {altair.sv.pdf} &
 \pdfimage width 8cm {moxie.sv.pdf} 
 \end{tabular}
\caption{SV for altair and moxie}
\end{figure}

\section{Conclusion}\label{s:concl}

In this paper we have shown how several graph algorithms can be
expressed concisely and elegantly in \Xten. These algorithms rely
heavily on support for fine-grained concurrency. The \Xten{} runtime
(\XWS) implements fine-grained concurrency through an ehanced
work-stealing scheduler. Specifically the scheduler supports
improperly nested tasks, detection of global termination, and phased
work-stealing.  We measure the performance of spanning tree algorithms
implemented with pseudo-depth-first search (DFS), breadth-first search
(BFS) and a Shiloach-Vishkin algorithm on two multicore systems. We
show that the \XWS{} programs scale and exhibit performance comparable
with hand-written C programs.

\paragraph{Acknowledgements.} \XWS{} is being designed and implemented in collabration with Doug Lea. We thank Raj Barik for his contributions to the implementation of the C++ version of \XWS. We thank the rest of the \Xten{} team for many discussions of these issues. This material is based upon work supported by the Defense
Advanced Research Projects Agency under its Agreement No.
HR0011-07-9-0002.
{\footnotesize
\bibliographystyle{ieee}
\bibliography{paper,parallel}
}
\end{document}

