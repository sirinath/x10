
\section{Introduction}
\label{s:intr}

%% Graph problems are very interesting.

Graph theoretic problems arise in traditional and emerging scientific
disciplines such as VLSI design, optimization, databases, and
computational biology, social network analysis, and transportation
networks.

%% They are very hard to solve.
Large-scale graph problems are challenging to solve in parallel
(e.g.{} on a shared memory symmetric multiprocessor or on a multicore
system) because of their irregular and combinatorial nature.
Irregular graphs arise in many important real world settings, for
example, the Internet, social interaction networks, transportation
networks, and protein-protein interaction networks.  These graphs can
be modeled as `scale-free'' graphs \cite{CZF04}. For random and
scale-free graphs no known efficient static partitioning techniques exist,
and hence the load must be balanced dynamically. 
Moreover, the irregular memory access pattern dictated by the input
instances is not cache-friendly; graph algorithms also tend to be
load/store intensive \cite{G06}, and they lay great pressure on the
memory subsystem.  
Compared with their numerical counterparts, parallel
graph algorithms take drastically different approaches than the
sequential algorithms, and usually employ fine-grained
parallelism. For example, depth-first search (DFS) or breadth-first
search (BFS) are two popular sequential algorithms for the spanning
tree problem. Many parallel spanning tree algorithms, represented by
the Shiloach-Vishkin algorithm \cite{SV82}, take the
``graft-and-shortcut'' approach, and provide $O(n)$ fine-grained
parallelism. In the absence of efficient scheduling support of
parallel activities, fine-grained parallelism incurs large overhead on
current systems, and often the algorithms do not show practical
performance advantage.

We take the spanning tree problem as our example. Finding a spanning
tree of a graph is an important building block for many graph
algorithms, for example, biconnected components and ear decomposition
\cite{MR86}, and can be used in graph planarity testing \cite{KR88}.
Spanning tree represents a wide range of graph problems that have fast
theoretic parallel algorithms but no known efficient parallel
implementations that achieve speedup without serious restricting
assumptions about the inputs.

%% Case study. Bader and Cong's algorithm. load-balancing is an issue.
%% How should the data-structures be organized.
Bader and Cong \cite{BC04a} presented the first fast parallel spanning
tree algorithm that achieved good speedups on SMPs. Their algorithm is
based on a graph traversal approach, and is similar to DFS or BFS.
There are two steps to the algorithm. First a small stub tree of size
$O(p)$ is generated by one worker through a random walk of the
graph. The vertices of this tree are then evenly distributed to each
worker.  Each worker then traverses the graph in a manner similar to
sequential DFS or BFS, using efficient atomic operations (e.g.{}
Compare-and-Swap) to update the state of each node (e.g.{} update the
{\tt parent} pointer). The set of nodes being worked on is kept in a local queue.
When a worker is finished with its portion (its queue is empty), it
checks randomly for any other worker with a non-empty queue, and
``steals'' a portion of that work for itself (by removing it from the
victim's queue, and adding it to its own queue).

For efficient execution, it is very important that the queue be
managed carefully. For instance, the operation of adding work (a node)
to the local queue should be efficient (i.e.{} should not require
locking) since it will be performed frequently. Stealing is however
relatively infrequent and it is preferrable to shift the cost of
stealing from the victim to the thief since the thief has no work to
do (the ``work first'' principle). The graph algorithm designer now
faces a choice. The designer may note \cite{BC04} that correctness is
not compromised by permitting a thief to {\em copy} the set of nodes
that the victim is working on. Here the victim is permitted to write
to the queue without acquiring a lock. Now the price to be paid is
that the thief and the victim may end up working on the same node
(possibly at the same time).  While work may thus be duplicated,
correctness is not affected since the second worker to visit a node
will detect that the node has been visited (e.g.{} because its atomic
operation will fail) and do nothing. Alternatively, the designer may
use a modified version of the Dekker protocol \cite{cilk}, by ensuring
that the thief and victim each writes to a volatile variable and read
the variable written by the other. This guarantees that no work will
be duplicated, but the mechanism used is very easy to get wrong,
leading to subtle concurrency errors.

Thus the design of such high-performance concurrent data-structures is
difficult and error-prone. Those concerns that are of interest to the
graph algorithm designer (e.g.{} expressing breadth-first vs
depth-first search) are mixed in with the concerns for efficient
parallel representation.  This suggests packaging the required
components in a library or a framework and exposing a higher-level
interface to programmers.

\subsection{Cilk Work-stealing}

Such a framework has been proposed for a class of parallel programs 
for shared memory machines by
the designers of Cilk. Cilk is an extension to C which permits the
programmer to specify that some statements may be executed in parallel
{\tt spawn}, and to specify that execution of the current procedure
body should suspend until all statements spawned during its execution
are terminated ({\tt sync}).

The Cilk compiler translates source code into C by using a
``continuation passing transformation'' and adding calls to the
runtime (Cilk work-scheduler) which manages a double-ended queue
(deque) for each worker (= thread). On each procedure entry an
activation frame is pushed onto thhe bottom of the deque. On executing
a spawn a field in the activation frame is updated to point to the
instruction after the spawn, and the code in the body of the spawn is
executed as a sequential procedure call. Now if a thief visits this
worker looking for work it may steal the current activation frame and
continue executing the body of the procedure at the instruction after
the spawn. When the victim returns from executing the spawn it will
discover (using the Dekker protocol) that the parent frame has been
stolen and will now itself go looking for work (by randomly selecting
a victim worker and attempting to steal from it). The sync statement
is executed by keeping track of the number of live children, and
setting up the remainder of the body of the procedure to be executed
once this count reaches zero. 

The Cilk compiler gains further performance improvements by generating
two copies of each procedure (one for the ``fast path'' and one for
the slow path). The fast path corresponds to execution with no steals
and gives efficiency very close to sequential execution.

Since synchronization in Cilk is naturally tied 
Cilk naturally lends itself to the expression of recursive
data-structures since synchronization is tied with procedure
invocation. The literature has many examples of elegant exploitation
of such recursive parallelism (e.g.{} cache-oblivious algorithms
\cite{frigo}). 

\subsection{\Xten{} work-stealing}
%% However, there are problems with work-stealing. 

However, there are problems with Cilk work-stealing when applied to
graph algorithms. Consider the code for a parallel form of
pseudo-depth-first traversal (for spanning tree) in
Table~\ref{alg:st-x10}. (The code is written in \Xten{} \cite{x10}.)
Here on visiting a node (in the body of {\tt
traverse}) we wish to spawn parallel activities ({\tt async}) to visit
each neighboring vertex that has not been visited before. However,
there is no reason for the current activity to wait for all these
activities to terminate before returning from the body of the {\tt
traverse} routine. Specifically, the invocation of {\tt traverse} does
not return a value -- it merely updates the heap to record the parent
for the visited node. Hence there is no need for the invoker to wait
for a return value. Thus the body does not need a {\tt sync}
statement. But this makes {\tt traverse} illegal as a Cilk
procedure. Said differently, a natural expression of parallel graph
algorithms requires the use of improperly nested computation.

Further, note the use of {\tt finish} in the body of the {\tt tree}
method.  This states that the invoking activity must wait until all
activities spawned during the execution of {\tt traverse()} have
terminated. {\tt tree()} is invoked precisely once -- at the root node
for the spanning tree -- and hence there is exactly one sync for the
duration of the spanning tree computation. Thus another refinement of 
Cilk is natural and useful here: the implementation must support 
global termination detection (i.e.{} it must detect when the 
dequeues associated with all queues are empty). 

\begin{table}
\centering
\scriptsize
\begin{tabular}{c}
\begin{minipage}[t]{0.5\textwidth}
\begin{verbatim} 
  class V {
    V parent;
    V[] neighbors;
    void dfsTree() {
      parent=this;
      finish dfsTraverse();
    }
    void dfsTraverse() {
      for(V v : neighbors) {
        atomic v.parent = (v.parent==null?this:v.parent);
        if (v.parent == this)
           async v.dfsTraverse();
      }
    }
  }
\end{verbatim}
\end{minipage} 
\end{tabular}
\caption{Pseudo Depth-first spanning tree algorithm core in \Xten{}}
\label{alg:dfs-x10}
\end{table}

\paragraph{Phased computation}
Finally, we note that breadth-first search requires another idea, that
of {\em phased computation}. Here we mark vertices with a level number
corresponding to the distance from the root node. It is necessary to
ensure that a vertex is visited at distance $i$ is visited in the
$i$th phase. This means that activities cannot be launched immediately
for vertices marked in the $i$th phase. Instead all these activities
must wait for all the vertices marked in the $i-1$st phase to be
processed, i.e.{} one must integrate barrier-based synchronization
with work-stealing.

\begin{table}
\centering
\scriptsize
\begin{tabular}{c}
\begin{minipage}[t]{0.5\textwidth}
\begin{verbatim} 
  class V {
    V parent;
    V[] neighbors;
    void bfsTree() {
      parent=this;
      finish async { 
        clock c = new clock();
        bfsTraverse(c);
      }
    }
    void bfsTraverse(clock c) {
      for(V v : neighbors) {
        atomic v.parent = (v.parent==null?this:v.parent);
        if (v.parent == this)
           async clocked(c) { next; v.bfsTraverse();}
      }
    }
  }
\end{verbatim}
\end{minipage} 
\end{tabular}
\caption{Breadth-first spanning tree algorithm core in \Xten}
\label{alg:bfs-x10}
\end{table}

We show the program for expressing this computation in
Table~\ref{alg:bfs-x10}. While this program will be discussed in
detail later, we mention now that {\tt c} should be thought of as a
{\em barrier}. The activity spawned to examine a newly discovered
vertex must wait until the barrier is reached ({\tt next}) before
commencing its own traversal. Thus the program combines barrier-based
synchronization with fine-grained parallelism. The program clearly
expresses the strong relationship with the depth-first search variant
-- there are no details about the work-stealing implementation
polluting the program.

\subsection{Rest of this paper}

In this paper we show that pseudo- Depth-first, breadth-first and
Shiloach-Vishkin algorithms for spanning tree can be programmed quite
elegantly in \Xten. Further we present the design of \XWS, the \Xten{}
work-stealing scheduler, which extends the Cilk work-stealing
scheduler with support for improperly nested computation, termination
detection and phased computation. These mechanisms are sufficient to
execute the three graph algorithms efficiently. 


We use a collection of sparse graph generators to evaluate these algorithms.
Our generators include several employed in previous
experimental studies of parallel graph algorithms for related
problems. For instance, we include the torus topologies used in the
connectivity studies of Greiner \cite{Gre94}, Krishnamurthy \emph{et
al.} \cite{KLC97}, Hsu \emph{et al.} \cite{HRD97}, Goddard \emph{et
al.} \cite{GKP97}, and Bader and Cong \cite{BC04b}, the random graphs
used by Greiner \cite{Gre94}, Hsu \emph{et al.}  \cite{HRD97}, and
Goddard \emph{et al.} \cite{GKP97}, the geometric graphs used by
Greiner \cite{Gre94}, Hsu \emph{et al.} \cite{HRD97}, Krishnamurthy
\emph{et al.} \cite{KLC97}, and Goddard \emph{et al.} \cite{GKP97}.

\begin{itemize}
\itemsep0pt

\item \textbf{2D Torus} The vertices of the graph are placed on a 2D
  mesh, with each vertex connected to its four neighbors.  

\item \textbf{Random Graph} We create a random graph of $n$ vertices
  and $m$ edges by randomly adding $m$ unique edges to the vertex
  set. Several software packages generate random graphs this way,
  including LEDA \cite{MN99}.
  
\item \textbf{Geometric Graphs and AD3} In these $k$-regular graphs,
  %$n$ points are chosen uniformly and at random in a unit square in
  %the Cartesian plane, and 
  each vertex is connected to its $k$ %nearest
  neighbors.  Moret and Shapiro \cite{MS94} use these
  in their empirical study of sequential MST algorithms. \textbf{AD3}
  is a geometric graph with $k=3$.  
\end{itemize}

We compare the performance of \XWS{} to two other programs. First is a
hand-written C program, a variant of \cite{BC04a}. Second, we have
written versions of pseudo-DFS and BFS in Cilk. (We were unable to
write the SV algorithm in Cilk.)

We present performance data on two machines. Altair is an 8-way Sun
Fire V40Z server, running four dual-core AMD Opteron processors at
2.4GHz (64KB instruction cache/core, 64KB data cache/core, 16GB
physical memory). Moxie is a 32-way Sun Fire T200 Server running
UltraSPARC T1 processor at 1.2 GHz (16KB instruction cache/core, 8KB
data cache/processor, 2MB integrated L2 cache, 32GB physical
memory). We are in the process of benchmarking these programs on a
64-way Power5 SMP as well. 

We discuss in detail the design of \XWS, including our extensions to CWS.

Finally we conclude with a discussion of related and future work.


%%  Throughout the paper, we
%%  use $n$ and $m$ to denote the number of vertices and the number of
%%  edges of an input graph $G=(V,E)$, respectively. 
  