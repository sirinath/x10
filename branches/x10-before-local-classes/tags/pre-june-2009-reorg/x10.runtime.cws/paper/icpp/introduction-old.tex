
\section{Introduction}
\label{s:intr}\label{sec:intro}

Obtaining practical efficient implementations for large, irregular
graph problems is challenging. Current software systems and commodity
multiprocessors do not support fine-grained, irregular parallelism
well. Implementing a custom framework for fine-grained parallelism for
each new graph algorithm is impractical.

We present \XWS{}, the \Xten{} Work Stealing framework.
\XWS{} is intended as an open-source runtime for the programming
language \Xten{}, a partitioned global address space language
supporting dynamic fine-grained concurrency.  \XWS{} is also intended
as a library to be used directly by application writers. \XWS{}
extends the Cilk work-stealing framework with several features
necessary to efficiently implement graph algorithms, viz., support for
improperly nested procedures, worker-specific data-structures, global
termination detection, and phased computation.

We present simple elegant programs using \XWS{} for different spanning
tree algorithms using (pseudo-)depth-first search and breadth-first
search.  We evaluate these programs on a 32-way Niagara (moxie), and
an 8-way Opteron server (altair) and on three different bounded-degree
graphs: (i) graphs with randomly selected edges and (a) no degree
restrictions (b) fixed degree, and (ii) planar torus graphs.

We show the performance of BFS and pseudo-DFS search depends crucially
on the granularity of parallel tasks. We show that the granularity
natural to the algorithms -- the examination of a single edge ---
leads to poor performance at scale. Instead, sets of of vertices must
be grouped into {\em batches}. We show that a fixed-size batching
scheme does not perform well. For instance, batches of size $1$ yield
a peak performance of 20 MEPS (Million Edges Per Second) on Niagara.
Instead we develop an adaptive batching scheme in which the batch
size is sensitive to the instantaneous size of the work queue.  With
this scheme, pseudo-DFS shows linear scaling on altair and moxie,
achieving peak performance of over 220 MEPS on moxie and substantially
outperforming C and Cilk implementations.

% In the following discussion, nodes and vertices of a graph are used interchangeably.

\subsection{Challenges in solving large, irregular graph problems}
%% Rewrite
In the last few years we have seen an explosion of mainstream architectural
innovation --- multi-cores, symmetric multiprocessors, clusters, and
accelerators (such as the Cell processor and GPGPUs) --- that now
requires application programmers to confront varied concurrency and
distribution issues. This raises the fundamental question: what
programming model can application programmers use to productively utilize
such diverse machines and systems?

Consider for instance the problem faced by designers of graph
algorithms.  Graph problems arise in traditional and
emerging scientific disciplines such as VLSI design, optimization,
databases, computational biology, social network analysis, and
transportation networks.

Large-scale graph problems are challenging to solve in parallel --
even on shared memory symmetric multiprocessor (SMP) or on a multicore
system -- because of their irregular and combinatorial nature.
Irregular graphs arise in many important real world settings. For
random and ``scale-free'' graphs \cite{CZF04} no known efficient
static partitioning techniques exist, and hence the load must be
balanced dynamically.  

Consider the spanning tree problem. Finding a spanning tree of a graph
is an important building block for many graph algorithms such as those for
biconnected components, ear decomposition \cite{MR86} and graph planarity testing \cite{KR88}.  Spanning tree represents
a wide range of graph problems that have fast theoretic parallel
algorithms but no known efficient parallel implementations that
achieve speedup without serious restrictive assumptions about the
inputs.

Bader and Cong \cite{BC04a} presented the first fast parallel spanning
tree algorithm that achieved good speedups on SMPs. Their algorithm is
based on a graph traversal approach, and is similar to DFS or BFS.
There are two steps in the algorithm. First a small stub tree of size
$O(p^2)$ is generated by one of the $p$ worker through a random walk of the
graph. The vertices of this tree are then evenly distributed to each
worker.  Each worker then traverses the graph in a manner similar to
sequential DFS or BFS, using efficient atomic operations (e.g.{}
Compare-and-Swap) to update the state of each node (e.g.{} update the
{\tt parent} pointer). The set of nodes being worked on is kept in a
local queue.  When a worker is finished with its portion (its queue is
empty), it checks randomly for any other worker with a non-empty
queue, and ``steals'' a portion of the victim's work for itself.

For efficient execution, it is very important that the queue be
managed carefully. For instance, the operation of adding work (a node)
to the local queue should be efficient (i.e.{} should not require
locking) since it will be performed frequently. Stealing is however
relatively infrequent and it is preferable to shift the cost of
stealing from the victim to the thief since the thief has no work to
do (the ``work first'' principle). The graph algorithm designer now
faces a choice. The designer may note \cite{BC04a} that correctness is
not compromised by permitting a thief to {\em copy} the set of nodes
that the victim is working on. Here the victim is permitted to write
to the queue without acquiring a lock. Now the price to be paid is
that the thief and the victim may end up working on the same node
(possibly at the same time).  While work may thus be duplicated,
correctness is not affected since the second worker to visit a node
will detect that the node has been visited (e.g.{} because its atomic
operation Compare-and-Swap will fail) and do nothing. Alternatively, the designer may
use a modified version of the Dekker protocol \cite{BJKLRZ95} to resolve the race condition. 
%, by ensuring that the thief and victim each writes to a volatile variable and read the variable written by the other. 
This guarantees that no work will be duplicated, but the mechanism used is very easy to get wrong, leading to subtle concurrency errors.

The above illustrates that the design of such high-performance
concurrent data-structures is difficult and error-prone. 
%Those concerns that are of interest to the graph algorithm designer (e.g.{} expressing breadth-first vs depth-first search) are mixed in with the concerns for efficient parallel representation.  
This suggests packaging the required components in a library or a framework and
exposing a higher-level interface to programmers.

\subsection{\Xten{} and \XWS}
The \Xten{} programming language \cite{x10} has
been designed to address the challenges of ``productivity with
performance'' on those diverse architectures existing or emerging.  In this paper we
present the design, implementation and evaluation of a portion of the
\Xten{} runtime system for multicore and SMPs, \XWS{}. \XWS{}
implements fine-grained concurrency through an extension of Cilk Work
Stealing (CWS) \cite{BJKLRZ95}.  Work stealing is a powerful technique
organized around a collection of workers (threads) that each
maintains a double-ended queue (deque) of {\em frames} (or tasks). A
worker pops and pushes frames from the bottom of the deque. When its
deque is empty, it randomly selects another worker and attempts to
steal a frame from the top of its deque. CWS is carefully organized to
streamline parallel overhead so that execution of the code with a
single worker incurs a small constant factor overhead over execution
of the sequential code. The overhead associated with stealing is
deferred to the worker performing the steal (the {\em thief}) as
opposed to the worker being mugged (the {\em victim}). The CWS
algorithm is known to have nice properties in theory, and can be
efficiently implemented in practice.

\XWS{} extends \CWS{} to better support the programming of
applications with irregular concurrency. It removes the link between
recursion and concurrency introduced by Cilk. Crucial to this removal
is a method in \XWS{} for detecting termination of a computation
without counting all the frames created during the computation.
Further, \XWS{} integrates {\em barriers} -- essential for phased
computations such as breadth-first search -- with
work-stealing. Finally, \XWS{} support the implementation of {\em
adaptive batching} schemes by the programmer. Batching is a technique
for increasing the granularity of parallel tasks by batching together
several small tasks. Thieves steal a batch at a time. Depending on the
algorithm, the batching size may have a dramatic impact on the
performance of work-stealing -- for instance we have observed that on
the 32-way Niagara the best performance for batches of size $1$
(i.e.{} without batching) is approximately $20$ MEPS, whereas with batching it goes up to nearly $240$ MEPS.
\XWS{} permits the programmer to sense key metrics of the current
execution and use these to adjust batching size dynamically.

\XWS{} may be illustrated by the following sample programs (fragments
of running programs in Java):

\begin{example}[Psuedo-DFS] \label{example:dfs}
The parallel exploration of a graph may be implemented quite simply by the following program:
{\footnotesize
\begin{verbatim}
class V  extends VertexFrame {
   V [] neighbors;
   V parent;
   V(int i){super(i);}
   boolean tryColor() {
     return UPDATER.compareAndSet(this,0,1);
   }
   void compute(Worker w) throws StealAbort {
     w.popAndReturnFrame();
     for (V e : neighbors) 
       if (e.tryColor()) {
         e.parent = this;
         w.pushFrame(e);
       }}}}
\end{verbatim}}
The class {\tt V} represents a vertex with an array used to represent
edges. {\tt V} extends {\tt Frame} and hence can be scheduled by the
work-stealing scheduler. On being scheduled its {\tt compute} method
is run, with the worker executing the vertex being passed as the
argument. The code for {\tt compute} may schedule parallel work by
invoking {\tt w.pushFrame}. The static variable {\tt UPDATER} from the enclosing context is of type {\tt AtomicIntegerFieldUpdater}.

Note that a parallel frame corresponds to a single vertex; this code
does not implement batching. (A batched version is discussed later.)
\end{example}

\begin{example}[BFS] \label{example:bfs}
The breadth-first parallel exploration of a graph may be implemented
as follows:
{\footnotesize
\begin{verbatim}
class V  extends VertexFrame {
   V [] neighbors;
   V parent;
   V(int i){super(i);}
   boolean tryColor() {
     return UPDATER.compareAndSet(this,0,1);
   }
   void compute(Worker w) throws StealAbort {
     w.popAndReturnFrame();
     for (V e : neighbors) 
       if (e.tryColor()) {
         e.parent = this;
         w.pushFrameNext(e);
       }}}}
\end{verbatim}}

Here the code utilizes the implementation of a global {\em clock} (barrier)
by \XWS. Each worker maintains two deques, the {\em now} and the {\em
next} deque. Always the now deque is active, but execution of a frame
may cause frames to be added to the next deque.  When all the work in
the current phase has terminated (that is, all now deques across all
workers are empty), and at least one worker has added a frame to the
next deque, computation moves to the next phase and causes each worker
to swap their next and now deques.
\end{example}

\subsection{Performance experiments}

The rest of this paper is as follows. In Section~\ref{sec:XWS}, we
present the details of the design of \XWS. In Section~\ref{sec:Graph}
we examine comparable programs written using an application-specific
framework (Simple, \cite{BC04a}), as well as Cilk, and compare
performance on three different graph inputs.  Our graph generators
include several employed in previous experimental studies of parallel
graph algorithms for related problems. For instance, we include the
torus topologies, random graphs and geometric graphs, used in \cite{Gre94}, \cite{HRD97} and others.
%Krishnamurthy \emph{et al.} \cite{KLC97}, and others.

\begin{itemize}
\itemsep0pt
\item \textbf{2D Torus} The vertices of the graph are placed on a 2D
  mesh, with each vertex connected to its four neighbors.  

\item \textbf{Random Graph} We create a random graph of $n$ vertices
  and $m$ edges by randomly adding $m$ unique edges to the vertex
  set. Several software packages generate random graphs this way.
%  including LEDA \cite{MN99}.
  
\item \textbf{Geometric Graph} In these $k$-regular graphs,
  %$n$ points are chosen uniformly and at random in a unit square in
  %the Cartesian plane, and 
  each vertex is connected to its $k$ %nearest
  neighbors.  
%Moret and Shapiro \cite{MS94} use these
%  in their empirical study of sequential MST algorithms. \textbf{AD3}
%  is a geometric graph with $k=3$.  
\end{itemize}

We present performance data on two machines. Altair (Opteron) is an
8-way Sun Fire V40Z server, running four dual-core AMD Opteron
processors at 2.4GHz (64KB instruction cache/core, 64KB data
cache/core, 16GB physical memory). Moxie (Niagara) is a 32-way Sun
Fire T200 Server running UltraSPARC T1 processor at 1.2 GHz (16KB
instruction cache/core, 8KB data cache/processor, 2MB integrated L2
cache, 32GB physical memory). (We are in the process of benchmarking
these programs on a 64-way Power5 SMP as well.)

%%% SUMMARIZE RESULTS.

We show that the performance of these programs in \XWS{} can be
substantially improved with batching. We present schemes for
adaptively determining the size of the batch based upon an estimate of
the current stealing pressure.

Finally we conclude with a section on related work and acknowledgements. Due to the page limit, we can not provide more examples on how to use the \XWS{} framework. Readers please refer to the Java implementation of \XWS{} for details, available at {\tt http://x10.sf.net}.  