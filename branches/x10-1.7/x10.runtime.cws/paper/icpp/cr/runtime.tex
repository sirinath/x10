\section{\XWS}
\label{s:runtime}

We have implemented a runtime system for \Xten{}. 
As an initial implementation, we have developed a Java-based
runtime system to support shared-memory parallelization. It is
distributed as a Java package, {\java x10.runtime.cws}, under an
open-source license~\cite{x10-webpage}. In this section, we discuss the
implementation of key features of the runtime system.

The computation is organized as collection of {\em tasks}. A task is a
sequence of instructions that can spawn other tasks and wait for
completion of the spawned tasks. The computation begins with a single
task and is considered complete when there are no more tasks executing
in the system. 

\subsection{Cilk work-stealing}
This section describes Cilk work stealing, as implemented in \XWS. It
closely follows the description of the implementation of Cilk in
\cite{frigo98implementation}.

In summary, Cilk Work Stealing (CWS) is organized around a collection
of cooperating threads called {\em workers}. Each worker maintains a
double-ended queue (deque) of tasks. During execution as a worker
creates more tasks it pushes them at the bottom of the queue. When it
needs more tasks it retrieves the current task from the bottom of the
deque. When a worker runs out of work (its deque is empty), it
randomly chooses another worker (the {\em victim}) and attempts to
steal a task by fetching it from the top of the deque. Program begins
execution when the environment submits a task to a central task queue.

One of the workers retrieves the task from the global queue and begins
executing it. When a worker does not have tasks to execute it {\em steals}
tasks available at other workers. Assuming the computation contains
sufficient parallelism stealing happens infrequently. The design
ensures that there are few overheads during normal execution, referred
to as {\em the fast path}. The additional overheads incurred to load-balance
the computation are proportional to the number of steals in the
execution. The design of the worker for the the task types supported
is discussed in subsequent sections.

The normal execution corresponds to the depth-first sequential
execution of the tasks spawned. Thus the execution corresponds to a
sequential execution when there is only one worker.

We now describe the details.

Every \emph{async} in the \Xten{} program is compiled into a sub-class
of {\tt Frame}. An \emph{async} is said to consist of {\em threads},
where a thread is a non-blocking sequence of instructions terminating
in a spawn or waiting on an \emph{async}.

Each class created for an \emph{async} contains as member variables the local
variables used in the \emph{async} body, a counter ({\em PC}) that specifies
the next instruction in the body of the \emph{async} after the current
thread.

% \input{fib-figure}
Cilk implements a scheme for handling continuations.  Two versions of
the \emph{async} body are created -- the fast and the slow
versions. The fast version of the program is executed during the
normal course of the program. The slow version is invoked by the
worker to initiate processing of a task. Fig.~\ref{fig:fib-ill} shows
the fast and slow versions for the Fibonacci method shown in
Fig.~\ref{fig:fib-ill}(a).

Each worker maintains a deque consisting of stack of Frames. On
entering a fast version, a frame object is created and pushed onto the
stack (method: {\java pushFrame()}). This frame contains the
up-to-date values of variables whenever any other worker might steal
this task. This frame is popped from the stack (method: {\java popFrame()}) 
when this method returns.

Whenever a task is spawned, the worker immediately proceeds to execute
the spawned task like a sequential method call. When the spawned task
finishes execution, the worker checks whether the current frame was
stolen. If so, the result computed by the child task is stored to be
passed onto its parent and the execution of the task aborts (method:
{\tt abortOnSteal()}). The method call stack is unwound by throwing
an exception ({\tt StealAbort}) that is caught in the main
routine executed by the worker. The values in the frame of the parent
stack are updated before proceeding to execute a spawned child, as the
worker now has a non-executing task and hence is target of a steal. 

All the descendents are guaranteed to be completed in the fast version
when a {\java finish} is encountered. Hence the finish statements
are ignored.

The slow version restores any local variables and uses the {\em PC} to
start execution of the task past the execution point at which it might
have been stolen. A task might have been stolen when its descendents
are executing. Thus a {\em finish} statement, translated to the
{\java sync()} method, might cause the invoking task to suspend on
completion of non-terminated children. The value of the {\java result} 
variable is returned to the parent task on invocation of the
{\java setupReturn()} method. 

In a program with sufficient parallelism, the slow version is expected
to execute infrequently. The design tries to minimize the overheads in
the fast version even at the expense of the slow version (the
``work-first'' principle).

\subsection{Support for Properly Nested Tasks}
Cilk requires {\em fully-strict} programs
in which a task waits for all its descendents to complete before
returning. Such tasks are also called properly nested tasks. 

The \Xten{} runtime system is designed to leverage the Cilk design while
supporting a larger class of programs. \Xten{} provides support for {\em
strict} computations, in which a ancestor task need not wait for its
descendent tasks to be completed. Such tasks are said to be {\em improperly
nested}. 

Each worker contains a {\em closure}. A closure is an  object used
to return values from the spawned tasks to their parents in the
presence of work stealing.  

Each closure maintains a stack of frames. Frames corresponding to
spawned tasks are pushed into the stack on entry, and popped on
return. In the fast path, return values are propagated as they would
be in a sequential program. The thief steals a closure together with
the bottom-most available frame from its
{\em victim}.

When a thief steals a task, the descendants of the task in the
victim's dequeue continue to execute.  In order to return values from
the descendents to the parent, a new closure is created that on
completion returns the result to the parent closure that was stolen.

Thus the closures form a tree of return value propagation
corresponding to the steal operation performed. Termination is
detected when the closure corresponding to the task inserted by the
driver thread returns. 

The procedure executed by the workers to handle properly nested tasks
is shown in Fig.~\ref{fig:worker-code}(a). On completing execution of
a closure, a worker first attempts to obtain another closure from its
local queue (method:{\java extractBottom()}). If no local closure is
available to execute, the worker attempts to obtain a task either by
stealing or from the global queue (method:{\java getTask()}). It then
executes the slow version of the task obtained (method:{\java
  execute()}). 


\subsection{Support for Improperly Nested Tasks}

Properly nested tasks $t$ satisfy the property that at the moment when
the slow version terminates (method:{\java compute()}) the frame at
the bottom of the worker's dequeue is $t$. Hence the task can be
completed (i.e., removed from the dequeue) by including a {\java
w.popFrame()} call at the end of the compute method. In essence, if a
worker is executing only properly nested tasks (this is true when it
is executing Cilk code), there is a one-to-one correspondence between
the frame stack and the tasks being processed.

\Xten{} permits improperly nested tasks. Such tasks $q$ are used, for
instance, to implement the pseudo-depth-first search discussed in this
paper. Such a task may add a task $r$ to the deque of its worker (say $w$)
without necessarily transferring control to $r$. This has two
consequences. First, recall that as soon as a worker's dequeue
contains more than one task the worker may be the target of a
theft. Therefore as soon as $q$ pushes $r$ onto $w$'s dequeue, $q$ is
available to be stolen.  Therefore $q$'s compute method must record the
fact that is computation has begun so that the stealing worker $z$ may
do the right thing. For instance, if $q$'s compute method does not
contain any internal suspension point then $z$ must immediately
terminate execution of $q$ and pop $q$ off its deque. This can be
accomplished by defining a volatile int PC field in $q$, and adding the
following code at the beginning of $q$'s compute method

{\scriptsize
\begin{verbatim}
  if (PC==1) {
    w.popFrame();
    return;
  }
  PC=1;
\end{verbatim}
}

Second for an improperly nested task when control returns from $g$'s
compute method, it may not be the case that the last frame on the
dequeue is $g$. Therefore a call to {\java popFrame()} at the end of
$g$'s compute method would be incorrect. Instead, the compute method
returns (without attempting to pop the last frame on the deque). Now
whenever the task reaches the bottom of the dequeue, the worker will,
as usual, invokes its compute method. However, the code sequence
described above will execute, thereby popping the frame from the
deque. Thus the code sequence above serves two purposes -- it does the
cleanup necessary when the task is stolen as well as when it is
completed.

The changes to the basic worker code necessary to support improperly
nested tasks are shown in Fig.~\ref{fig:worker-code}(b). With
improperly nested tasks, a worker no long enjoys the property that
when control returns to it from the invocation of an execute method on
the top-level task, the deque is empty. Indeed, control may return to
the scheduler leaving several tasks on the deque, including the task
whose execute method has just returned. The scheduler must now enter a
phase in which it executes the task at the bottom of the deque:

%\input{work-stealing-figures.tex}


%\subsection{How to steal quickly}
%
%
%%% In the fast version of an async, any modified local variables are
%%% copied back into the Frame object before entering a spawned
%%% child. This ensures that the updated values are available to any thief
%%% that may steal the parent frame before the execution of the current
%%% worker can return to it. On return from a spawned task, the task clone
%%% checks for 
%
%The frames corresponding to the tasks form a stack. We denote the head
%and tail of the task by $H$ and $T$, respectively. When a task is
%spawned, the corresponding frame is pushed into the head of the stack
%($H=H+1$). When a worker returns from a spawned task is checks that
%whether the current frame is stolen by executing one-half of lock-free
%Dekker's algorithm~\cite{dekker}:
%
%{\scriptsize
%\begin{verbatim}
%  --T; 
%  StoreLoadBarrier;
%  if (H >= T) 
%    // Stolen 
%  else 
%    // Not stolen
%\end{verbatim}
%}
%
%Note that the {\java StoreLoadBarrier} is implied in Java if
%{\java T} is declared to be {\java volatile}.
%
%When a worker is out of work, it randomly selects a victim and
%attempts to steal from its frame stack. The procedures employed to
%steal in the case of properly- and improperly-nested tasks are shown
%in Fig.~\ref{fig:stealing-alg}(a) and Fig.~\ref{fig:stealing-alg}(b),
%respectively. Note that the victim, and multiple thieves might attempt
%to operate on the same frame stack. The thief first obtains a lock on
%the victim's deque to avoid contention with other thieves trying to
%steal from the same victim.
%
%When properly nested tasks are being processed, the thief identifies a
%closure at the end of the deque and locks it. If the victim is
%processing some other closure and this closure is in a {\java READY}
%state, there is no contention with the victim on this closure. The
%thief extracts the Closure and steals it. If the closure is in
%{\java RUNNING} state, the victim is potentially adding and deleting
%frames on the frame stack associated with the closure. Dekker's
%algorithm is used to determine whether there is more than one frame in
%the frame stack. If there is, the locked closure together with the
%frame at the head of the stack are stolen. The immediate child frame
%of the stolen frame is promoted to a closure which is left in the
%victim's deque. The child task returns its result to the parent task
%through this promoted closure.
%
%In computations on improperly nested tasks, there is only one stack of
%frames used to represent the tasks. The thief directly locks the stack
%of frames (labeled {\java cache} in the algorithm) and attempts to
%steal from the stack's head when more than one tasks in available in
%the task. The frame is marked as stolen by incrementing the stack's
%head. 
%
%Since the tasks are pushed at the tail, the algorithm implies that a
%task is stolen only if all its parents have already been
%stolen. Parent tasks represent more work than child tasks, since they
%have potential to generate a greater number of tasks. The algorithms
%thus favor stealing of tasks that represent a large portion of work
%rather than fine-grained descendent tasks that do limited
%processing. This leads to better load balancing of the computation and
%reduces the number of steal attempts by workers.
%
\subsection{Global Quiescence}

In fully-strict computations, i.e., those involving properly nested
tasks, completion of the first task and the return of the
corresponding Closure indicates computation termination.
Improperly-nested tasks that do not require a return call chain can do
away with the closures. We have implemented a mechanism to efficiently
identify termination without closures.

The workers share a barrier. The barrier is used to determine when all
workers are out of work. Every worker notifies the barrier of its
state through two methods. {\java checkIn()} is used to enter the
barrier barrier and notify that the worker is out of work. When such a
worker steals work from a victim, it invokes {\java checkOut()} to
leave the barrier. The barrier maintains a {\java checkoutCount} on
the number of workers checked out. It is triggered when all the workers
are in it. The action associated with the barrier is triggered and it
signals that the computation has terminated.

The algorithm maintains the invariant:

\[
\mbox{{\texttt (\#workers - checkoutCount)}} = \mbox{\#(workers that know they don't
  have work )}
\]

A worker knows it has no work if it stealing. Note that the
checkoutCount is not always equal to the number of workers with work
to do. In particular, consider a victim that finds the current frame
as stolen. The victim cannot identify whether it has work without
locking its deque. While it aborts, the thief has the stolen
frame and could have invoked {\java checkOut()}. The barrier
identifies both workers as having checked out even though there is one
task between them. Note that allowing the victim to {\java
  checkIn()} when it identifies a steal would lead to the barrier
being incorrectly triggered while the thief still has the stolen task
but it yet to invoke {\java checkOut()}.


\subsection{Phased Computations}

We also added support for phased computations in which tasks in this
phase create tasks to be executed in the next phase. The
implementation of the breadth-first search algorithm proceeds one
level at a time. The nodes processed at this level are used to
determine the nodes to be processed in the next level.

Phased computations are supported as a generalization of global
quiescence. Each worker maintains two stacks of frames, referred to as
caches. Depending on the phase specified when spawning tasks, a task
can be added to the current cache or the next cache, the cache for the
next phase. 

When global quiescence is detected for this phase, the barrier action
invokes {\java advancePhase()} that steps the computation into the
next phase. When a worker runs out of tasks, it checks that the
current phase of the worker is the same as the global phase of the
computation. If the phase of the computation has advanced further, the
workers updates its phase information and swaps the current and next
task collections. 

Each worker specifies the number of tasks it has
outstanding for the next phase when it invokes {\java
  checkIn()}. This information is used to identify if the next phase
has any tasks left to be processed. 

Note that the global phase could have advanced much further than the
phase operated on by this worker. This would happen when this worker
has no work for current phase and has checked-in notifying that it has
not tasks for the next phase. The other workers could then progress
multiple phases before this worker observes the computation progress. 

When global quiescence for this phase is triggered, the number of
workers with tasks for the next phase is known. The computation is
said to have terminated when the current phase has quiesced and no
worker has any task for the next phase. 

For a given phase, maintaining the invariant mentioned above for
global quiescence is more involved with multiple phases. For example,
consider a worker advancing its phase to match the global phase of the
computation and its next cache is non-empty. Since the worker now has
local tasks in this phase, it implicitly checks out of the barrier.


\subsection{Explicitly Partitioned Programs}

While work-stealing provides a convenient abstraction for
load-balancing fine-grained programs, the performance of certain
applications can benefit from the explicitly partitioned programs that
would exist as a typical parallel global address space program. For
example, in the Shiloach-Vishkin algorithm we observe that explicit
partitioning of the edges and vertices amongst the workers leads to
better performance. This is achieved by having the task, called the
job task, submitted by the driver thread spawns tasks, called worker
tasks. The number of worker tasks spawned is equal to the number of
workers in the system. The job task is improperly nested and returns
upon spawning the worker tasks. Each of the spawned tasks now execute
one part of the program. Note that these tasks are spawned and
available at the worker that executed the job task. This worker now
has work and it executes one of the worker tasks. The other workers
identify the presence of work in this worker while stealing and steal
a worker task. Assuming the worker tasks are sufficiently
load-balanced, the rest of the computation proceeds without
work-stealing with each worker executing a worker task. 

\subsection{Performance Analysis}

In this section, we discuss the performance implications of the
different components of the runtime. 

The overheads of the fast version over the sequential execution are:

\begin{enumerate}
\item Method's frame needs to be allocated, initialized, pushed onto
  the deque. Cost: A few assembly instructions.
\item Method's state needs to be saved before each spawn. Cost: writes
  of local dirty variables and {\em PC}, and a {\java
  StoreLoadBarrier}
\item Method must check if its frame has been stolen after each
  return from a spawn. Cost: two reads, compare, branch and a 
  {\java StoreLoadBarrier} 
\item On return, frame must be freed. Cost: A few assembly instructions
\item An extra variable is needed to hold the frame pointer. Cost: Increased
  register pressure.
\end{enumerate}

Note that average cost of allocating and de-allocating memory can be
reduced to a couple of statements with an efficient concurrent memory
allocation scheme. Thus the overhead in the fast path is very
little, as is also demonstrated in the experimental evaluation.

The number of closures created and invocations of the slow version of
an \emph{async} is proportional to the number of successful steals. The
number of locks requested is proportional to the numbers of attempted
steals. 

Improperly nested tasks take advantage of the lack of return value to
avoid the creation of closures, further reducing overhead. 

The computation is identified as terminated, the moment the last
worker starts trying to steal. Thus there is no significant delay
between the actual termination of the computation and its detection.
The mechanism itself incurs the overhead of two atomic updates to a
shared counter for every successful steal. The overheads incurred in
supported phased execution are similar.

Explicitly partitioned programs begin execution with all the tasks on
one worker. There is a delay before which all workers attempt to steal
from this worker and obtain a worker task. Assuming a truly random
scheme in the choice of the victim to steal from, the worst case in
the number of steals before work is found is proportional to the
number of workers. Since the worker tasks are load-balanced and there
is no work-stealing after this initial delay, the number of steals in
the worst case is independent of the program running time and problem
size, and hence much smaller than that incurred in a typical
work-stealing strategy. We observe this in our experimental evaluation
as well. 


Thus the common execution path in which all workers are busy involves
little overhead. The remaining in other execution paths are
proportional to the number of attempted and successful steals
performed by the workers. The experimental evaluation section
demonstrates that both are far lower than the number of \emph{asyncs}
spawned, effectively enabling load-balanced execution of fine-grained
parallel programs.

