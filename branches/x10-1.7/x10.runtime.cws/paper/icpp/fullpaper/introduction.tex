
\section{Introduction}
\label{s:intr}\label{sec:intro}

%% Rewrite
The last few years have seen an explosion of mainstream architectural
innovation --- multi-cores, symmetric multiprocessors, clusters, and
accelerators (such as the Cell processor, GPGPUs) --- that now
requires application programmers to confront varied concurrency and
distribution issues. This raises the fundamental question: what
programming model can application programmers use to productively utilize
such diverse machines and systems?

Consider for instance the problem faced by designers of graph
algorithms.  Graph problems arise in traditional and
emerging scientific disciplines such as VLSI design, optimization,
databases, computational biology, social network analysis, and
transportation networks.

Large-scale graph problems are challenging to solve in parallel --
even on shared memory symmetric multiprocessor (SMP) or on a multicore
system -- because of their irregular and combinatorial nature.
Irregular graphs arise in many important real world settings. For
random and ``scale-free'' graphs \cite{CZF04} no known efficient
static partitioning techniques exist, and hence the load must be
balanced dynamically.  

Consider the spanning tree problem. Finding a spanning tree of a graph
is an important building block for many graph algorithms, for example,
biconnected components and ear decomposition \cite{MR86}, and can be
used in graph planarity testing \cite{KR88}.  Spanning tree represents
a wide range of graph problems that have fast theoretic parallel
algorithms but no known efficient parallel implementations that
achieve speedup without serious restrictive assumptions about the
inputs.

Bader and Cong \cite{BC04a} presented the first fast parallel spanning
tree algorithm that achieved good speedups on SMPs. Their algorithm is
based on a graph traversal approach, and is similar to DFS or BFS.
There are two steps to the algorithm. First a small stub tree of size
$O(p^2)$ is generated by one worker through a random walk of the
graph. The vertices of this tree are then evenly distributed to each
worker.  Each worker then traverses the graph in a manner similar to
sequential DFS or BFS, using efficient atomic operations (e.g.{}
Compare-and-Swap) to update the state of each node (e.g.{} update the
{\tt parent} pointer). The set of vertices being worked on is kept in a
local queue.  When a worker is finished with its portion (its queue is
empty), it checks randomly for any other worker with a non-empty
queue, and ``steals'' a portion of that work for itself).

For efficient execution, it is very important that the queue be
managed carefully. For instance, the operation of adding work (a node)
to the local queue should be efficient (i.e.{} should not require
locking) since it will be performed frequently. Stealing is however
relatively infrequent and it is preferrable to shift the cost of
stealing from the victim to the thief since the thief has no work to
do (the ``work first'' principle). The graph algorithm designer now
faces a choice. The designer may note \cite{BC04a} that correctness is
not compromised by permitting a thief to {\em copy} the set of vertices
that the victim is working on. Here the victim is permitted to write
to the queue without acquiring a lock. Now the price to be paid is
that the thief and the victim may end up working on the same node
(possibly at the same time).  While work may thus be duplicated,
correctness is not affected since the second worker to visit a node
will detect that the node has been visited (e.g.{} because its atomic
operation will fail) and do nothing. Alternatively, the designer may
use a modified version of the Dekker protocol \cite{BJKLRZ95}, by ensuring
that the thief and victim each writes to a volatile variable and read
the variable written by the other. This guarantees that no work will
be duplicated, but the mechanism used is very easy to get wrong,
leading to subtle concurrency errors.

The above illustrates that the design of such high-performance
concurrent data-structures is difficult and error-prone. Those
concerns that are of interest to the graph algorithm designer (e.g.{}
expressing breadth-first vs depth-first search) are mixed in with the
concerns for efficient parallel representation.  This suggests
packaging the required components in a library or a framework and
exposing a higher-level interface to programmers.

\subsection{\Xten{} and \XWS}
The \Xten{} programming language \cite{x10} has
been designed to address the challenges of ``productivity with
performance'' on these diverse architectures.  In this paper we
present the design, implementation and evaluation of a portion of the
\Xten{} runtime system for multicore and SMPs, \XWS{}. \XWS{}
implements fine-grained concurrency through an extension of Cilk Work
Stealing (CWS) \cite{BJKLRZ95}.  Work stealing is a powerful technique
organized around a collection of workers (=threads) that each
maintains a double-ended queue (deque) of {\em frames} (or tasks). A
worker pops and pushes frames from the bottom of the deque. When its
deque is empty, it randomly selects another worker and attempts to
steal a frame from the top of its deque. CWS is carefully organized to
streamline parallel overhead so that execution of the code with a
single worker incurs a small constant factor overhead over execution
of the sequential code. The overhead associated with stealing is
deferred to the worker performing the steal (the {\em thief}) as
opposed to the worker being mugged (the {\em victim}). The CWS
algorithm is known to have nice properties in theory, and can be
efficiently implemented in practice.

\XWS{} extends \CWS{} to better support the programming of
applications with irregular concurrency. It removes the link between
recursion and concurrency introduced by Cilk. Crucial to this removal
is a method in \XWS{} for detecting termination of a computation
without counting all the frames created during the computation.
Further, \XWS{} integrates {\em barriers} -- essential for phased
computations such as breadth-first search -- with
work-stealing. Finally, \XWS{} support the implementation of {\em
adaptive batching} schemes by the programmer. Batching is a technique
for increasing the granularity of parallel tasks by batching together
several small tasks. Thieves steal a batch at a time. Depending on the
algorithm, the batching size may have a dramatic impact on the
performance of work-stealing -- for instance we have observed that on
the 32-way Niagara the best performance for batches of size $1$
(i.e.{} without batching) is approximately $20$ Million edges per
second (MEPS), whereas with batching it goes up to nearly $240$ MEPS.
\XWS{} permits the programmer to sense key metrics of the current
execution and use these to adjust batching size dynamically.

\XWS{} may be illustrated by the following sample programs (fragments
of running programs):

\begin{example}[Psuedo-DFS] \label{example:dfs}
The parallel exploration of a graph may be implemented quite simply by the following program:
{\footnotesize
\begin{verbatim}
class V  extends VertexFrame {
   V [] neighbors;
   V parent;
   V(int i){super(i);}
   boolean tryColor() {
     return parentU.compareAndSet(this,0,1);
   }
   void compute(Worker w) throws StealAbort {
     w.popAndReturnFrame();
     for (V e : neighbors) 
       if (e.tryColor()) {
         e.parent = this;
         w.pushFrame(e);
       }}}}
\end{verbatim}}
The class {\tt V} represents a vertex with an array used to represent
edges. {\tt V} extends {\tt Frame} and hence can be scheduled by the
work-stealing scheduler. On being scheduled its {\tt compute} method
is run, with the worker executing the vertex being passed as the
argument. The code for {\tt compute} may schedule parallel work by
invoking {\tt w.pushFrame}. 

Note that a parallel frame corresponds to a single vertex; this code
does not implement batching. (A batched version is discussed later.)
\end{example}

\begin{example}[BFS] \label{example:bfs}
The breadth-first parallel exploration of a graph may be implemented
as follows:
{\footnotesize
\begin{verbatim}
class V  extends VertexFrame {
   V [] neighbors;
   V parent;
   V(int i){super(i);}
   boolean tryColor() {
     return parentU.compareAndSet(this,0,1);
   }
   void compute(Worker w) throws StealAbort {
     w.popAndReturnFrame();
     for (V e : neighbors) 
       if (e.tryColor()) {
         e.parent = this;
         w.pushFrameNext(e);
       }}}}
\end{verbatim}}

Here the code utilizes the implementation of a global clock (barrier)
by \XWS. Each worker maintains two deques, the {\em now} and the {\tt
next} deque. Always the now deque is active, but execution of a frame
may cause frames to be added to the next deque.  When all the work in
the current phase has terminated (that is, all now deques across all
workers are empty), and at least one worker has added a frame to the
next deque, computation moves to the next phase and causes each worker
to swap their next and now deques.
\end{example}

\subsection{Rest of this paper}

The rest of this paper is as follows. In Section~\ref{sec:XWS}, we
present the details of the design of \XWS. In Section~\ref{sec:Graph}
we present comparable programs written using an application-specific
framework (Simple, \cite{BC04a}), as well as Cilk, and compare
performance on three different kinds of loads.  Our graph generators
include several employed in previous experimental studies of parallel
graph algorithms for related problems. For instance, we include the
torus topologies, random graphs and geometric graphs, used 
by \cite{Gre94}, Hsu \emph{et al.}  \cite{HRD97}, and others.
%Krishnamurthy \emph{et al.} \cite{KLC97}, and others.
Specifically, we present results for 2D Torus (vertices are placed on a 2D mesh, with each vertex connected to its four neighbors), Random graphs (random selection of a fixed number of edges for the given vertices), Geometric graphs ($k$-regular graphs with each vertex connected to its $k$ neighbors.



We present performance data on two machines. Altair (Opteron) is an
8-way Sun Fire V40Z server, running four dual-core AMD Opteron
processors at 2.4GHz (64KB instruction cache/core, 64KB data
cache/core, 16GB physical memory). Moxie (Niagara) is a 32-way Sun
Fire T200 Server running UltraSPARC T1 processor at 1.2 GHz (16KB
instruction cache/core, 8KB data cache/processor, 2MB integrated L2
cache, 32GB physical memory). (We are in the process of benchmarking
these programs on a 64-way Power5 SMP as well.)

%%% SUMMARIZE RESULTS.

We show that the performance of these programs in \XWS{} can be
substantially improved with batching. We present schemes for
adaptively determining the size of the batch based upon an estimate of
the current stealing pressure.

Finally we conclude with a section on Related work.

