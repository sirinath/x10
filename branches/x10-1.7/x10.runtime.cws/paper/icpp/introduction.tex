
\section{Introduction}
\label{s:intr}\label{sec:intro}

Obtaining practical efficient implementations for large, irregular
graph problems is challenging. Current software systems and commodity
multiprocessors do not support fine-grained, irregular parallelism
well. Implementing a custom framework for fine-grained parallelism for
each new graph algorithm is impractical.

We present \XWS{}, the \Xten{} Work Stealing framework.
\XWS{} is intended as an open-source runtime for the programming
language \Xten{}, a partitioned global address space language
supporting dynamic fine-grained concurrency.  \XWS{} is also intended
as a library to be used directly by application writers. \XWS{}
extends the Cilk work-stealing framework with several features
necessary to efficiently implement graph algorithms, viz., support for
improperly nested procedures, worker-specific data-structures, global
termination detection, and phased computation.

We present simple elegant programs using \XWS{} for different spanning
tree algorithms using (pseudo-)depth-first search and breadth-first
search.  We evaluate these programs on a 32-way Niagara (moxie), and
an 8-way Opteron server (altair) and on three different bounded-degree
graphs: (i) graphs with randomly selected edges and (a) no degree
restrictions (b) fixed degree, and (ii) planar torus graphs.

We show the performance of BFS and pseudo-DFS search depends crucially
on the granularity of parallel tasks. We show that the granularity
natural to the algorithms -- the examination of a single edge ---
leads to poor performance at scale. Instead, sets of of vertices must
be grouped into {\em batches}. We show that a fixed-size batching
scheme does not perform well. For instance, batches of size $1$ yield
a peak performance of 20 MEPS (Million Edges Per Second) on Niagara.
Instead we develop an adaptive batching scheme in which the batch
size is sensitive to the instantaneous size of the work queue.  With
this scheme, pseudo-DFS shows linear scaling on altair and moxie,
achieving peak performance of over 220 MEPS on moxie and substantially
outperforming C and Cilk implementations.

% In the following discussion, nodes and vertices of a graph are used interchangeably.

\subsection{Challenges in solving large, irregular graph problems}
%% Rewrite
In the last few years we have seen an explosion of mainstream architectural
innovation --- multi-cores, symmetric multiprocessors, clusters, and
accelerators (such as the Cell processor and GPGPUs) --- that now
requires application programmers to confront varied concurrency and
distribution issues. This raises the fundamental question: what
programming model can application programmers use to productively utilize
such diverse machines and systems?

Consider for instance the problem faced by designers of graph
algorithms.  Graph problems arise in traditional and
emerging scientific disciplines such as VLSI design, optimization,
databases, computational biology, social network analysis, and
transportation networks.

Large-scale graph problems are challenging to solve in parallel --
even on shared memory symmetric multiprocessor (SMP) or on a multicore
system -- because of their irregular and combinatorial nature.
Irregular graphs arise in many important real world settings. For
random and ``scale-free'' graphs \cite{CZF04} no known efficient
static partitioning techniques exist, and hence the load must be
balanced dynamically.  

Consider the spanning tree problem. Finding a spanning tree of a graph
is an important building block for many graph algorithms such as those for
biconnected components, ear decomposition \cite{MR86} and graph planarity testing \cite{KR88}.  Spanning tree represents
a wide range of graph problems that have fast theoretic parallel
algorithms but no known efficient parallel implementations that
achieve speedup without serious restrictive assumptions about the
inputs.

Bader and Cong \cite{BC04a} presented the first fast parallel spanning
tree algorithm that achieved good speedups on SMPs. Their algorithm is
based on a graph traversal approach, and is similar to DFS or BFS.
There are two steps in the algorithm. First a small stub tree of size
$O(p^2)$ is generated by one of the $p$ worker through a random walk of the
graph. The vertices of this tree are then evenly distributed to each
worker.  Each worker then traverses the graph in a manner similar to
sequential DFS or BFS, using efficient atomic operations (e.g.{}
Compare-and-Swap) to update the state of each node (e.g.{} update the
{\tt parent} pointer). The set of nodes being worked on is kept in a
local queue.  When a worker is finished with its portion (its queue is
empty), it checks randomly for any other worker with a non-empty
queue, and ``steals'' a portion of the victim's work for itself.

For efficient execution, it is very important that the queue be
managed carefully. For instance, the operation of adding work (a node)
to the local queue should be efficient (i.e.{} should not require
locking) since it will be performed frequently. Stealing is however
relatively infrequent and it is preferable to shift the cost of
stealing from the victim to the thief since the thief has no work to
do (the ``work first'' principle). The graph algorithm designer now
faces a choice. The designer may note \cite{BC04a} that correctness is
not compromised by permitting a thief to {\em copy} the set of nodes
that the victim is working on. Here the victim is permitted to write
to the queue without acquiring a lock. Now the price to be paid is
that the thief and the victim may end up working on the same node
(possibly at the same time).  While work may thus be duplicated,
correctness is not affected since the second worker to visit a node
will detect that the node has been visited (e.g.{} because its atomic
operation Compare-and-Swap will fail) and do nothing. Alternatively, the designer may
use a modified version of the Dekker protocol \cite{BJKLRZ95} to resolve the race condition. 
%, by ensuring that the thief and victim each writes to a volatile variable and read the variable written by the other. 
This guarantees that no work will be duplicated, but the mechanism used is very easy to get wrong, leading to subtle concurrency errors.

The above illustrates that the design of such high-performance
concurrent data-structures is difficult and error-prone. 
%Those concerns that are of interest to the graph algorithm designer (e.g.{} expressing breadth-first vs depth-first search) are mixed in with the concerns for efficient parallel representation.  
This suggests packaging the required components in a library or a framework and
exposing a higher-level interface to programmers.

\subsection{\Xten{}}
The \Xten{} programming language \cite{x10} has been designed to
address the challenges of ``productivity with performance'' on those
diverse architectures existing or emerging.  \Xten{} augments a core
sequential modern object-oriented language (very similar to Java$^{\sc
TM}$ or Scala) with constructs for distribution ({\em places}) and
concurrency ({\em asyncs}, {\em finish}, {\em atomic} and {\em
clocks}).\footnote{Discussion of the distribution constructs of
\Xten{} and advanced concurrency constructs ({\tt when}) is out of
scope of this paper.}  The statement {\tt async S} spawns a new task
or activity to execute the statement {\tt S}. The statement {\tt
finish S} executes {\tt S} and waits for all activities spawned during
its execution to terminate. The statement {\tt atomic S} causes the
statement {\tt S} to be executed as if in a single indivisible step
(with no other activity executing during this step). An \Xten{} clock
is a data-structure that represents a dynamic barrier. The activity
creating a clock is said to be registered on that clock. An activity
that is registered on a clock {\tt c} may create new activities
registered on {\tt c} by executing {\tt async clocked(c) S}. An
activity registered on a clock may execute a {\tt next;} operation to
suspend until such time as all activities registered on the clock have
executed a {\tt next} operation (barrier behavior).

\Xten{} may be used to implement spanning tree computations in a very straighforward way:
\begin{example}[Psuedo-DFS] \label{example:dfs}
The parallel exploration of a graph may be implemented thus:
{\footnotesize
\begin{verbatim}
class V  {
   V [] neighbors;
   V parent;
   V(int i){super(i);}
   boolean tryColor(V n) {
     atomic 
       if (parent==null) 
         parent = n;
     return parent==n;
   }
   void compute() {
     for (V e : neighbors) 
       if (e.tryColor()) 
             async e.compute();
   }
   void dfs() {
     finish compute();
   }
}
\end{verbatim}}
Computation is initiated by invoking {\tt r.dfs()} on 
the root vertex {\tt r}. This visits {\tt r} within the
scope of a {\tt finish}. When a vertex is visited, each of its
outgoing edges is examined in sequence. If the vertex reached from
the edge does not have its parent set, its parent is set atomically, 
and the vertex is (recursively) visited asynchronously. Thus an
activity is spawned for each vertex in the connected component
containing {\tt r}. (This code does not implement
batching, a batched version is discussed later.)
\end{example}

\begin{example}[BFS] \label{example:bfs}
The breadth-first parallel exploration of a graph may be implemented
as follows:
{\footnotesize
\begin{verbatim}
class V  extends VertexFrame {
   V [] neighbors;
   V parent;
   V(int i){super(i);}
   boolean tryColor(V n) { ... }
   void compute(clock c) {
     for (V e : neighbors) 
       if (e.tryColor()) 
         async clocked(c) { 
            next; 
            e.compute(c);
         }
   }
   void bfs() {
     finish async {
      clock c = new clock();
      compute(c);
     }
   }
}
\end{verbatim}}
Computation is initiated by invoking {\tt r.bfs()} on the root vertex
{\tt r}. This causes an activity to be launched which creates a new
clock {\tt c}, and visits {\tt r} (with the clock {\tt c}).  When a
vertex is visited, each of its outgoing edges is examined in sequence.
If the vertex reached from the edge does not have its parent set, its
parent is set atomically, and the vertex is visited (recursively) by a
new async (clocked on {\tt c}) which waits first for {\tt c} to
advance to the next phase.  Thus a vertex is visited in phase $n$ of
the clock only when all vertices reachable in one step from the
vertices visited in phase $n-1$ of the clock have been marked. This
ensures that all vertices are visited in breadth-first order.
\end{example}
\subsection{\XWS}
A central challenge in implementing \Xten{} is to efficiently
load-balance multiple activities, while respecting dependencies
introduced by clocks and {\tt finish}. This paper presents the design,
implementation and evaluation of a portion of the \Xten{} runtime
system for multicore and SMPs, \XWS{}. \XWS{} implements fine-grained
concurrency through an extension of Cilk Work Stealing (CWS)
\cite{BJKLRZ95}.  Work stealing is a powerful technique organized
around a collection of workers (threads) that each maintains a
double-ended queue (deque) of {\em frames} (or tasks). A worker pops
and pushes frames from the bottom of the deque. When its deque is
empty, it randomly selects another worker and attempts to steal a
frame from the top of its deque. CWS is carefully organized to
streamline parallel overhead so that execution of the code with a
single worker incurs a small constant factor overhead over execution
of the sequential code. The overhead associated with stealing is
deferred to the worker performing the steal (the {\em thief}) as
opposed to the worker being mugged (the {\em victim}). The CWS
algorithm is known to have nice properties in theory, and can be
efficiently implemented in practice.

\XWS{} extends \CWS{} to better support the programming of
applications with irregular concurrency. It removes the link between
recursion and concurrency introduced by Cilk. Crucial to this removal
is a method in \XWS{} for detecting termination of a computation
without counting all the frames created during the computation.
Further, \XWS{} integrates {\em barriers} -- essential for phased
computations such as breadth-first search -- with
work-stealing. Finally, \XWS{} support the implementation of {\em
adaptive batching} schemes by the programmer. Batching is a technique
for increasing the granularity of parallel tasks by batching together
several small tasks. Thieves steal a batch at a time. Depending on the
algorithm, the batching size may have a dramatic impact on the
performance of work-stealing -- for instance we have observed that on
the 32-way Niagara the best performance for batches of size $1$
(i.e.{} without batching) is approximately $20$ MEPS, whereas with
batching it goes up to nearly $240$ MEPS.

\XWS{} permits the programmer to sense key metrics of the current
execution and use these to adjust batching size dynamically.


\subsection{Performance experiments}

The rest of this paper is as follows. In Section~\ref{sec:XWS}, we
present the details of the design of \XWS. In Section~\ref{sec:Graph}
we examine comparable programs written using an application-specific
framework (Simple, \cite{BC04a}), as well as Cilk, and compare
performance on three different graph inputs.  Our graph generators
include several employed in previous experimental studies of parallel
graph algorithms for related problems. For instance, we include the
torus topologies, random graphs and geometric graphs, used in \cite{Gre94}, \cite{HRD97} and others.
%Krishnamurthy \emph{et al.} \cite{KLC97}, and others.

\begin{itemize}
\itemsep0pt
\item \textbf{2D Torus} The vertices of the graph are placed on a 2D
  mesh, with each vertex connected to its four neighbors.  

\item \textbf{Random Graph} We create a random graph of $n$ vertices
  and $m$ edges by randomly adding $m$ unique edges to the vertex
  set. Several software packages generate random graphs this way.
%  including LEDA \cite{MN99}.
  
\item \textbf{Geometric Graph} In these $k$-regular graphs,
  %$n$ points are chosen uniformly and at random in a unit square in
  %the Cartesian plane, and 
  each vertex is connected to its $k$ %nearest
  neighbors.  
%Moret and Shapiro \cite{MS94} use these
%  in their empirical study of sequential MST algorithms. \textbf{AD3}
%  is a geometric graph with $k=3$.  
\end{itemize}

We present performance data on two machines. Altair (Opteron) is an
8-way Sun Fire V40Z server, running four dual-core AMD Opteron
processors at 2.4GHz (64KB instruction cache/core, 64KB data
cache/core, 16GB physical memory). Moxie (Niagara) is a 32-way Sun
Fire T200 Server running UltraSPARC T1 processor at 1.2 GHz (16KB
instruction cache/core, 8KB data cache/processor, 2MB integrated L2
cache, 32GB physical memory). (We are in the process of benchmarking
these programs on a 64-way Power5 SMP as well.)

%%% SUMMARIZE RESULTS.

We show that the performance of these programs in \XWS{} can be
substantially improved with batching. We present schemes for
adaptively determining the size of the batch based upon an estimate of
the current stealing pressure.

Finally we conclude with a section on related work and acknowledgements. Due to the page limit, we can not provide more examples on how to use the \XWS{} framework. Readers please refer to the Java implementation of \XWS{} for details, available at {\tt http://x10.sf.net}.  